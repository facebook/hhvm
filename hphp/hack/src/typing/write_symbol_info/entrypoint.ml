(*
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the "hack" directory of this source tree.
 *
 *)

open Hh_prelude
open Hh_json
open Option.Monad_infix
module Fact_acc = Predicate.Fact_acc

module JobReturn = struct
  type t = {
    elapsed: float;
    hashes: Md5.Set.t;
    reindexed: SSet.t;
    referenced: SSet.t; (* set of files referenced by the indexed files *)
  }

  let neutral =
    {
      elapsed = 0.0;
      hashes = Md5.Set.empty;
      reindexed = SSet.empty;
      referenced = SSet.empty;
    }

  let merge t1 t2 =
    {
      elapsed = t1.elapsed +. t2.elapsed;
      hashes = Set.union t1.hashes t2.hashes;
      reindexed = SSet.union t1.reindexed t2.reindexed;
      referenced = SSet.union t1.referenced t2.referenced;
    }
end

let log_elapsed s elapsed =
  let { Unix.tm_min; tm_sec; _ } = Unix.gmtime elapsed in
  Hh_logger.log "%s %dm%ds" s tm_min tm_sec

let write_file referenced output_file =
  let open Out_channel in
  let oc = create output_file in
  SSet.iter
    (fun str ->
      output_string oc str;
      newline oc)
    referenced;
  close oc

let write_facts_file out_dir ?(global = false) files json_chunks =
  let (_out_file, channel) =
    Filename_unix.open_temp_file
      ~in_dir:out_dir
      "glean_symbol_info_chunk_"
      ".json"
  in
  let json_string = json_to_string (JSON_Array json_chunks) in
  let json_length = String.length json_string in
  Out_channel.output_string channel json_string;
  Out_channel.close channel;
  Hh_logger.log
    "Wrote %s of %sJSON facts for %s"
    (Byte_units.to_string_hum (Byte_units.of_bytes_int json_length))
    (if global then
      "global "
    else
      "")
    (match files with
    | `Count n -> Printf.sprintf "%d files" n
    | `Path path -> path)

(* these facts aren't generated by the workers, but once for the whole
   indexing run. There are
   - symbol hash facts for incrementality (empty if gen_sym_hash isn't set)
   - the namespace aliases defined in .hhconfig *)
let gen_global_facts ns ~ownership ~shard_name all_hashes =
  let fa = Fact_acc.init ~ownership in
  let list_hashes = Set.to_list all_hashes in
  if ownership then Fact_acc.set_ownership_unit fa (Some ".hhconfig");
  List.fold ns ~init:fa ~f:(fun fa (from, to_) ->
      Add_fact.global_namespace_alias fa ~from ~to_ |> snd)
  |> Add_fact.indexerInputsHash shard_name list_hashes
  |> snd
  |> Fact_acc.to_json

let write_json
    (ctx : Provider_context.t)
    (ownership : bool)
    (out_dir : string)
    (files_info : File_info.t list)
    (start_time : float) : JobReturn.t =
  (* Large file may lead to large json files which timeout when sent
     to the server. Not an issue currently, but if it is, we can index
     xrefs/decls separately, or split in batches according to files size *)
  (List.iter files_info ~f:(fun File_info.{ tast; path; _ } ->
       if List.length tast > 2000 then Hh_logger.log "Large file: %s" path);
   try
     let json_chunks = Index_batch.build_json ctx files_info ~ownership in
     let files =
       let file_count = List.length files_info in
       if Int.equal file_count 1 then
         `Path (List.hd_exn files_info).File_info.path
       else
         `Count file_count
     in
     write_facts_file out_dir files json_chunks
   with
   | WorkerCancel.Worker_should_exit as exn ->
     (* Cancellation requests must be re-raised *)
     let e = Exception.wrap exn in
     Exception.reraise e
   | e ->
     Printf.eprintf "WARNING: symbol write failure: \n%s\n" (Exn.to_string e));
  let elapsed = Unix.gettimeofday () -. start_time in
  log_elapsed "Processed batch in" elapsed;
  let hashes =
    List.filter_map files_info ~f:(fun file_info ->
        file_info.File_info.sym_hash)
    |> Md5.Set.of_list
  in
  JobReturn.{ elapsed; hashes; reindexed = SSet.empty; referenced = SSet.empty }

let references_from_files_info ctx files_info =
  List.map files_info ~f:(File_info.referenced ctx)
  |> List.reduce ~f:SSet.union
  |> Option.value ~default:SSet.empty

let recheck_job
    (ctx : Provider_context.t)
    (opts : Indexer_options.t)
    (_ : JobReturn.t)
    (fa : Indexable.t list) : JobReturn.t =
  let open Indexer_options in
  let start_time = Unix.gettimeofday () in

  (* file hashes must be computed for incremental indexing, bases or increments *)
  let gen_sym_hash = opts.gen_sym_hash || Option.is_some opts.incremental in

  (* in order to compute referenced files, we need to compute the path of referenced
     symbols *)
  let gen_references = Option.is_some opts.referenced_file in
  let files_info =
    List.filter_map
      fa
      ~f:
        (File_info.create
           ctx
           ~root_path:opts.root_path
           ~hhi_path:opts.hhi_path
           ~gen_sym_hash)
  in
  let reindex f =
    match (f.File_info.fanout, opts.incremental, f.File_info.sym_hash) with
    | (true, Some table, Some hash) when Sym_hash.mem table hash -> false
    | _ -> true
  in
  let to_reindex = List.filter ~f:reindex files_info in
  let res = write_json ctx opts.ownership opts.out_dir to_reindex start_time in
  let fanout_reindexed =
    let f File_info.{ path; fanout; _ } =
      if fanout then
        Some path
      else
        None
    in
    List.filter_map to_reindex ~f |> SSet.of_list
  in
  let referenced =
    if gen_references then
      references_from_files_info ctx files_info
    else
      SSet.empty
  in
  JobReturn.{ res with reindexed = fanout_reindexed; referenced }

let sym_hashes ctx ~files =
  let f file =
    let fi =
      File_info.create
        ctx
        ~root_path:"www"
        ~hhi_path:"hhi"
        ~gen_sym_hash:true
        (Indexable.from_file file)
    in
    let sym_hash =
      match fi >>= fun fi -> fi.File_info.sym_hash with
      | None ->
        failwith "Internal error" (* can't happen since gen_sym_hash = true *)
      | Some sh -> sh
    in
    (Relative_path.to_absolute file, sym_hash)
  in
  List.map files ~f

let index_files ctx ~out_dir ~files =
  let idx = List.map files ~f:Indexable.from_file in
  let opts = Indexer_options.default ~out_dir in
  recheck_job ctx opts JobReturn.neutral idx |> ignore

let go
    (workers : MultiWorker.worker list option)
    (ctx : Provider_context.t)
    (opts : Indexer_options.t)
    ~(namespace_map : (string * string) list)
    ~(files : Indexable.t list) : unit =
  Indexer_options.log opts;
  let num_workers =
    match workers with
    | Some w -> List.length w
    | None -> 1
  in
  let start_time = Unix.gettimeofday () in
  let jobs =
    MultiWorker.call
      workers
      ~job:(recheck_job ctx opts)
      ~merge:JobReturn.merge
      ~next:(Bucket.make ~num_workers ~max_size:115 files)
      ~neutral:JobReturn.neutral
  in
  (* just a uid *)
  let shard_name =
    Md5.digest_string (Float.to_string start_time) |> Md5.to_hex
  in
  let global_facts =
    gen_global_facts
      namespace_map
      ~ownership:opts.Indexer_options.ownership
      ~shard_name
      jobs.JobReturn.hashes
  in
  write_facts_file
    opts.Indexer_options.out_dir
    ~global:true
    (`Count (List.length files))
    global_facts;
  (* TODO remove this log once the workflows use the reindexed_file option *)
  SSet.iter (Hh_logger.log "Reindexed: %s") jobs.JobReturn.reindexed;
  Option.iter
    opts.Indexer_options.referenced_file
    ~f:(write_file jobs.JobReturn.referenced);
  Option.iter
    opts.Indexer_options.reindexed_file
    ~f:(write_file jobs.JobReturn.reindexed);
  let cumulated_elapsed = jobs.JobReturn.elapsed in
  log_elapsed "Processed all batches (cumulated time) in " cumulated_elapsed;
  let elapsed = Unix.gettimeofday () -. start_time in
  log_elapsed "Processed all batches in " elapsed
