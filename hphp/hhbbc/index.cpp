/*
   +----------------------------------------------------------------------+
   | HipHop for PHP                                                       |
   +----------------------------------------------------------------------+
   | Copyright (c) 2010-present Facebook, Inc. (http://www.facebook.com)  |
   +----------------------------------------------------------------------+
   | This source file is subject to version 3.01 of the PHP license,      |
   | that is bundled with this package in the file LICENSE, and is        |
   | available through the world-wide-web at the following url:           |
   | http://www.php.net/license/3_01.txt                                  |
   | If you did not receive a copy of the PHP license and are unable to   |
   | obtain it through the world-wide-web, please send a note to          |
   | license@php.net so we can mail you a copy immediately.               |
   +----------------------------------------------------------------------+
*/
#include "hphp/hhbbc/index.h"

#include <algorithm>
#include <cstdio>
#include <cstdlib>
#include <fstream>
#include <iterator>
#include <map>
#include <memory>
#include <mutex>
#include <stack>
#include <unordered_map>
#include <utility>
#include <vector>

#include <boost/dynamic_bitset.hpp>
#include <boost/filesystem.hpp>

#include <tbb/concurrent_hash_map.h>

#include <folly/AtomicLinkedList.h>
#include <folly/Format.h>
#include <folly/Lazy.h>
#include <folly/MapUtil.h>
#include <folly/Memory.h>
#include <folly/Range.h>
#include <folly/SharedMutex.h>
#include <folly/String.h>
#include <folly/concurrency/ConcurrentHashMap.h>

#include "hphp/runtime/base/array-iterator.h"
#include "hphp/runtime/base/tv-comparisons.h"
#include "hphp/runtime/base/type-structure-helpers-defs.h"

#include "hphp/runtime/vm/native.h"
#include "hphp/runtime/vm/preclass-emitter.h"
#include "hphp/runtime/vm/runtime.h"
#include "hphp/runtime/vm/trait-method-import-data.h"
#include "hphp/runtime/vm/unit-util.h"

#include "hphp/hhbbc/analyze.h"
#include "hphp/hhbbc/class-util.h"
#include "hphp/hhbbc/context.h"
#include "hphp/hhbbc/func-util.h"
#include "hphp/hhbbc/optimize.h"
#include "hphp/hhbbc/options.h"
#include "hphp/hhbbc/options-util.h"
#include "hphp/hhbbc/parallel.h"
#include "hphp/hhbbc/representation.h"
#include "hphp/hhbbc/type-structure.h"
#include "hphp/hhbbc/type-system.h"
#include "hphp/hhbbc/unit-util.h"
#include "hphp/hhbbc/wide-func.h"

#include "hphp/util/approximate-nearest-neighbor.h"
#include "hphp/util/assertions.h"
#include "hphp/util/bitset-utils.h"
#include "hphp/util/check-size.h"
#include "hphp/util/configs/eval.h"
#include "hphp/util/hash-set.h"
#include "hphp/util/lock-free-lazy.h"
#include "hphp/util/match.h"
#include "hphp/util/sparse-bitset.h"

#include "hphp/zend/zend-string.h"

namespace HPHP {
namespace HHBBC {

TRACE_SET_MOD(hhbbc_index)

//////////////////////////////////////////////////////////////////////

using namespace extern_worker;
namespace coro = folly::coro;

//////////////////////////////////////////////////////////////////////

struct ClassInfo;
struct ClassInfo2;

struct ClassGraphHasher;

struct AuxClassGraphs;

struct ClassBundle;

//////////////////////////////////////////////////////////////////////

namespace {

//////////////////////////////////////////////////////////////////////

const StaticString s_construct("__construct");
const StaticString s_toBoolean("__toBoolean");
const StaticString s_invoke("__invoke");
const StaticString s_Closure("Closure");
const StaticString s_AsyncGenerator("HH\\AsyncGenerator");
const StaticString s_Generator("Generator");
const StaticString s_Awaitable("HH\\Awaitable");
const StaticString s_TrivialHHVMBuiltinWrapper("TrivialHHVMBuiltinWrapper");
const StaticString s_Traversable("HH\\Traversable");

//////////////////////////////////////////////////////////////////////

// HHBBC consumes a LOT of memory, so we keep representation types small.
static_assert(CheckSize<php::Block, 24>(), "");
static_assert(CheckSize<php::Local, use_lowptr ? 12 : 16>(), "");
static_assert(CheckSize<php::Param, use_lowptr ? 56: 88>(), "");
static_assert(CheckSize<php::Func, use_lowptr ? 176: 224>(), "");

// Likewise, we also keep the bytecode and immediate types small.
static_assert(CheckSize<Bytecode, use_lowptr ? 32 : 40>(), "");
static_assert(CheckSize<MKey, 16>(), "");
static_assert(CheckSize<IterArgs, 8>(), "");
static_assert(CheckSize<FCallArgs, 8>(), "");
static_assert(CheckSize<RepoAuthType, 8>(), "");

//////////////////////////////////////////////////////////////////////

Dep operator|(Dep a, Dep b) {
  return static_cast<Dep>(
    static_cast<uintptr_t>(a) | static_cast<uintptr_t>(b)
  );
}

bool has_dep(Dep m, Dep t) {
  return static_cast<uintptr_t>(m) & static_cast<uintptr_t>(t);
}

/*
 * Maps functions to contexts that depend on information about that
 * function, with information about the type of dependency.
 */
using DepMap =
  tbb::concurrent_hash_map<
    DependencyContext,
    hphp_fast_map<
      DependencyContext,
      Dep,
      DependencyContextHash,
      DependencyContextEquals
    >,
    DependencyContextHashCompare
  >;

//////////////////////////////////////////////////////////////////////

/*
 * Each ClassInfo has a table of public static properties with these entries.
 */
struct PublicSPropEntry {
  Type inferredType{TInitCell};
  uint32_t refinements{0};
  bool everModified{true};
};

//////////////////////////////////////////////////////////////////////
/*
 * Entries in the ClassInfo method table need to track some additional
 * information.
 *
 * The reason for this is that we need to record attributes of the
 * class hierarchy.
 *
 * We store a lot of these, so we go to some effort to keep it as
 * small as possible.
 */
struct MethTabEntry {
  MethTabEntry()
    : MethTabEntry{MethRef{}, Attr{}} {}
  explicit MethTabEntry(const php::Func& f)
    : MethTabEntry{MethRef{f}, f.attrs} {}
  MethTabEntry(const php::Func& f, Attr a)
    : MethTabEntry{MethRef{f}, a} {}
  MethTabEntry(MethRef meth, Attr a)
    : cls{TopLevel, meth.cls}
    , clsIdx{meth.idx}
    , attrs{a} {}

  MethRef meth() const { return MethRef{cls.ptr(), clsIdx}; }
  void setMeth(MethRef m) { cls.set(cls.tag(), m.cls); clsIdx = m.idx; }

  // There's a private method further up the class hierarchy with the
  // same name.
  bool hasPrivateAncestor() const { return cls.tag() & HasPrivateAncestor; }
  // This method came from the ClassInfo that owns the MethTabEntry,
  // or one of its used traits.
  bool topLevel() const { return cls.tag() & TopLevel; }
  // This method isn't overridden by methods in any regular classes.
  bool noOverrideRegular() const { return cls.tag() & NoOverrideRegular; }
  // First appearance of a method with this name in the hierarchy.
  bool firstName() const { return cls.tag() & FirstName; }

  void setHasPrivateAncestor() {
    cls.set(Bits(cls.tag() | HasPrivateAncestor), cls.ptr());
  }
  void setTopLevel() {
    cls.set(Bits(cls.tag() | TopLevel), cls.ptr());
  }
  void setNoOverrideRegular() {
    cls.set(Bits(cls.tag() | NoOverrideRegular), cls.ptr());
  }
  void setFirstName() {
    cls.set(Bits(cls.tag() | FirstName), cls.ptr());
  }

  void clearHasPrivateAncestor() {
    cls.set(Bits(cls.tag() & ~HasPrivateAncestor), cls.ptr());
  }
  void clearTopLevel() {
    cls.set(Bits(cls.tag() & ~TopLevel), cls.ptr());
  }
  void clearNoOverrideRegular() {
    cls.set(Bits(cls.tag() & ~NoOverrideRegular), cls.ptr());
  }
  void clearFirstName() {
    cls.set(Bits(cls.tag() & ~FirstName), cls.ptr());
  }

private:
  // Logically, a MethTabEntry stores a MethRef. However doing so
  // makes the MethTabEntry larger, due to alignment. So instead we
  // store the MethRef fields (which lets the clsIdx and attrs share
  // the same 64-bits). Moreover, we can store the special bits in the
  // cls name pointer.
  enum Bits : uint8_t {
    HasPrivateAncestor = (1u << 0),
    TopLevel = (1u << 1),
    NoOverrideRegular = (1u << 2),
    FirstName = (1u << 3)
  };
  CompactTaggedPtr<const StringData, Bits> cls;
  uint32_t clsIdx;

public:
  // A method could be imported from a trait, and its attributes
  // changed.
  Attr attrs;

  template <typename SerDe> void serde(SerDe& sd) {
    if constexpr (SerDe::deserializing) {
      SString clsname;
      Bits bits;
      sd(clsname)(clsIdx)(attrs)(bits);
      cls.set(bits, clsname);
    } else {
      sd(cls.ptr())(clsIdx)(attrs)(cls.tag());
    }
  }
};

// Don't indeliberably make this larger
static_assert(CheckSize<MethTabEntry, 16>(), "");

//////////////////////////////////////////////////////////////////////

using ContextRetTyMap = tbb::concurrent_hash_map<
  CallContext,
  Index::ReturnType,
  CallContextHashCompare
>;

//////////////////////////////////////////////////////////////////////

template<typename Filter>
PropState make_unknown_propstate(const Index& index,
                                 const php::Class& cls,
                                 Filter filter) {
  auto ret = PropState{};
  for (auto& prop : cls.properties) {
    if (filter(prop)) {
      auto& elem = ret[prop.name];
      elem.ty = adjust_type_for_prop(
        IndexAdaptor { index },
        cls,
        &prop.typeConstraints,
        TCell
      );
      if (prop.attrs & AttrSystemInitialValue) {
        auto initial = loosen_all(from_cell(prop.val));
        if (!initial.subtypeOf(BUninit)) elem.ty |= initial;
      }
      elem.tc = &prop.typeConstraints;
      elem.attrs = prop.attrs;
      elem.everModified = true;
    }
  }

  return ret;
}

//////////////////////////////////////////////////////////////////////

bool func_supports_AER(const php::Func* func) {
  // Async functions always support async eager return, and no other
  // functions support it yet.
  return func->isAsync && !func->isGenerator;
}

uint32_t func_num_inout(const php::Func* func) {
  if (!func->hasInOutArgs) return 0;
  uint32_t count = 0;
  for (auto& p : func->params) count += p.inout;
  return count;
}

PrepKind func_param_prep(const php::Func* f, uint32_t paramId) {
  auto const sz = f->params.size();
  if (paramId >= sz) return PrepKind{TriBool::No, TriBool::No};
  PrepKind kind;
  kind.inOut = yesOrNo(f->params[paramId].inout);
  kind.readonly = yesOrNo(f->params[paramId].readonly);
  return kind;
}

uint32_t numNVArgs(const php::Func& f) {
  uint32_t cnt = f.params.size();
  return cnt && f.params[cnt - 1].isVariadic ? cnt - 1 : cnt;
}

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

std::string show(const ConstIndex& idx, const IIndex& index) {
  if (auto const cls = index.lookup_class(idx.cls)) {
    assertx(idx.idx < cls->constants.size());
    return folly::sformat(
      "{}::{} ({})",
      idx.cls,
      cls->constants[idx.idx].name,
      idx.idx
    );
  }
  return show(idx);
}

//////////////////////////////////////////////////////////////////////

/*
 * Inferred information about a function, computed by whole-program analysis.
 *
 * This structure contains the results of analysis passes that refine our
 * knowledge about a function's behavior. These values are computed iteratively
 * and can only become more specific (never less specific) across analysis
 * rounds until reaching a fixed point.
 */
struct InferredFuncInfo {
  /*
   * The best-known return type of the function, if we have any
   * information.  May be TBottom if the function is known to never
   * return (e.g. always throws).
   */
  Type returnTy = TInitCell;

  /*
   * If the function always returns the same parameter, this will be
   * set to its id; otherwise it will be NoLocalId.
   */
  LocalId retParam{NoLocalId};

  /*
   * The number of times we've refined returnTy.
   */
  uint32_t returnRefinements{0};

  /*
   * Whether the function is free of side-effects.
   */
  bool effectFree{false};

  /*
   * Bitset representing which parameters definitely don't affect the
   * result of the function, assuming it produces one. Note that the
   * parameter type verification does not count as a use in this
   * context.
   */
  std::bitset<64> unusedParams;

  template <typename SerDe> void serde(SerDe&);
};

//////////////////////////////////////////////////////////////////////

/*
 * Inferred information about a class, computed by whole-program analysis.
 *
 * This structure contains the results of analysis passes that refine our
 * knowledge about a class's properties and constants. Like InferredFuncInfo,
 * these values are computed iteratively and can only become more specific
 * (never less specific) across analysis rounds.
 */
struct InferredClassInfo {
  /*
   * Inferred information about a class constant declared on this
   * class (not flattened).
   */
  SStringToOneT<ClsConstInfo> clsConstantInfo;

  // If this is a closure, the best known types of its use-vars.
  CompactVector<Type> useVars;

  // Inferred information about this class' private properties and
  // statics.
  PropState privateProps;
  PropState privateStatics;

  template <typename SerDe> void serde(SerDe&);
};

//////////////////////////////////////////////////////////////////////

/*
 * Monotonicity requires that when you use some analysis results from
 * a func or class, that information must never become more general
 * (i.e., it can only shrink). Violating monotonicity can cause
 * analysis to oscillate indefinitely or produce incorrect
 * optimization decisions. Distributed analysis, however, can break
 * monotonicity. A given class or function might be analyzed on
 * multiple jobs simultaneously (i.e., if it's part of another class
 * or func's dependency trace). Since each job will generally have a
 * different set of available entities, it's quite possible for the
 * analysis of the same entity to produce different results in
 * different jobs.  This isn't a problem by itself, since the only
 * analysis that truly "counts" is the one which is authoritative
 * (i.e., the one job which actually reports the analysis results back
 * to the main process).
 *
 * Suppose we have entities A, B, and C. Entity A is analyzed and
 * reported on job #1, and entity B is analyzed and reported on job
 * #2. Because B depends on A, entity A is also present on job #2 (but
 * not reported from there). Entity C is actually a dep of entity A,
 * but this isn't known yet, so it's not present on job #1 (but it
 * happens to be on job #2 by chance).
 *
 * Round 1: In one round, we analyze everything. Entity A is analyzed
 * in job #1 and reports its results. We did not know that entity C is
 * a dep, so it is not present and the analysis cannot make use of
 * it. However, after analysis, we now know entity C is a dep for next
 * round. Likewise, entity B is processed and reported from job
 * #2. Since A is a known dep of B, A is also present on job #2 and
 * will be analyzed (but not reported). In the process of analyzing A,
 * we learn the dependency on C and since C happens to be present on
 * the job, we take it into account while analyzing A. This means the
 * analysis of A on job #2 might actually infer better types than on
 * job #1. This better analysis is ephemeral because it is not
 * reported back (that is job #1's job). When B is analyzed on job #2,
 * it uses the analysis of A. The analysis of B is reported back to
 * the main process.
 *
 * Round 2: So far nothing is amiss. When we perform the next round,
 * we know that C is a dependency of A, so job #1 includes both A and
 * C. We analyze A, which now has C's information. This analysis
 * should come to the same result as previously calculated in job #2
 * (or perhaps better), so it (eventually) will produce the most
 * specific types. B is analyzed on job #2 again, and A is present
 * there as well. A has the results from the first round analysis in
 * job #1. This analysis is less specific than we had computed in
 * round 1 job #2 (which then fed into B's analysis). We analyze B
 * again, using the less specific A information, and boom, we get a
 * monotonicity violation.
 *
 * There are a variety of ways to solve this. At first I attempted to
 * solve this by enforcing that jobs don't take advantage of
 * information that isn't present on all other connected jobs. However
 * this is clumsy, causes more rounds unnecessarily, and doesn't
 * always work with long chains of dependencies.
 *
 * Instead solve it with the notion of retained info. Whenever an
 * entity uses some information from another entity, it "retains" that
 * info inside itself. On subsequent rounds, if the information
 * obtained from another entity is less specific than in the retained
 * info, the retained info is used instead. This ensures a "ratchet"
 * where information doesn't ever get less specific. RetainedInfo is
 * double buffered. It contains the retained info from the last round,
 * while also recording the retained info for next round. These are
 * separate because the two can be very different (if we optimize away
 * a dependency we don't need its retained info anymore). Also we
 * avoid recording retained info for cases where it's unnecessary
 * (i.e., we know we have the authoritative information for an
 * entity).
 */
struct RetainedInfo {
  // Obtain the information about a given func or class, returning
  // nullptr if there is none.
  const InferredFuncInfo* get(const FuncInfo2&) const;
  const InferredClassInfo* get(const ClassInfo2&) const;

  // Record information about the given func or class.
  InferredFuncInfo& retain(const FuncInfo2&);
  InferredClassInfo& retain(const ClassInfo2&);

  // Drop the old retained info, and make the info from retain() calls
  // be the current state. This must be called before the information
  // is serialized.
  void flip();

  bool empty() const { return !state; }

  template <typename SerDe> void serde(SerDe&);
private:
  struct FKey {
    SString cls;
    SString func;

    bool operator<(const FKey& o) const {
      assertx(func);
      assertx(o.func);
      if (!cls) {
        return o.cls || string_data_lt_func{}(func, o.func);
      } else if (!o.cls) {
        return false;
      } else if (string_data_lt_type{}(cls, o.cls)) {
        return true;
      } else if (string_data_lt_type{}(o.cls, cls)) {
        return false;
      } else {
        return string_data_lt_func{}(func, o.func);
      }
    }

    template <typename SerDe> void serde(SerDe& sd) { sd(cls)(func); }
  };

  struct Hasher {
    size_t operator()(const FKey& k) const {
      return folly::hash::hash_combine(
        k.cls ? k.cls->hash() : 0,
        pointer_hash<StringData>{}(k.func)
      );
    }
  };

  struct Equals {
    bool operator()(const FKey& k1, const FKey& k2) const {
      assertx(k1.func);
      assertx(k2.func);
      if (!k1.cls) {
        return !k2.cls && (k1.func == k2.func);
      } else if (!k2.cls) {
        return false;
      } else if (!k1.cls->tsame(k2.cls)) {
        return false;
      } else {
        return k1.func == k2.func;
      }
    }
  };

  using FMap = hphp_fast_map<FKey, InferredFuncInfo, Hasher, Equals>;
  using CMap = TSStringToOneT<InferredClassInfo>;

  static FKey makeKey(const FuncInfo2&);

  struct State {
    FMap funcCurrent;
    FMap funcNext;
    CMap clsCurrent;
    CMap clsNext;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(funcCurrent, std::less<>{})
        (clsCurrent, std::less<>{})
        ; // Next intentionally not serialized
    }
  };
  std::unique_ptr<State> state;
};

//////////////////////////////////////////////////////////////////////

/*
 * Currently inferred information about a PHP function.
 *
 * Nothing in this structure can ever be untrue.  The way the
 * algorithm works, whatever is in here must be factual (even if it is
 * not complete information), because we may deduce other facts based
 * on it.
 */
struct res::Func::FuncInfo {
  const php::Func* func = nullptr;
  /*
   * The best-known return type of the function, if we have any
   * information.  May be TBottom if the function is known to never
   * return (e.g. always throws).
   */
  Type returnTy = TInitCell;

  /*
   * If the function always returns the same parameter, this will be
   * set to its id; otherwise it will be NoLocalId.
   */
  LocalId retParam{NoLocalId};

  /*
   * The number of times we've refined returnTy.
   */
  uint32_t returnRefinements{0};

  /*
   * Whether the function is effectFree.
   */
  bool effectFree{false};

  /*
   * Bitset representing which parameters definitely don't affect the
   * result of the function, assuming it produces one. Note that
   * the parameter type verification does not count as a use in this context.
   */
  std::bitset<64> unusedParams;

  /*
   * List of all func families this function belongs to.
   */
  CompactVector<FuncFamily*> families;
};

/*
 * Currently inferred information about a Hack function.
 *
 * Nothing in this structure can ever be untrue. The way the algorithm
 * works, whatever is in here must be factual (even if it is not
 * complete information), because we may deduce other facts based on
 * it.
 *
 * This class mirrors the FuncInfo struct, but is produced and used by
 * remote workers. As needed, this struct will gain more and more of
 * FuncInfo's fields (but stored in a more remote worker friendly
 * way). Local calculations will continue to use FuncInfo. Once
 * everything is converted to use remote workers, this struct will
 * subsume FuncInfo entirely (and be renamed).
 */
struct FuncInfo2 {
  /*
   * Name of this function. If this is a top level function, this will
   * be an unique identifier. For methods, it will just be the method
   * name.
   */
  SString name;

  /*
   * The php::Func representing this function. This field is not
   * serialized. If you wish to make use of it, it must be fixed up
   * manually after deserialization.
   */
  const php::Func* func = nullptr;

  /*
   * Inferred information about this func
   */
  InferredFuncInfo inferred;

  /*
   * Retained information about other entities which have contributed
   * to this func's inferred information.
   */
  RetainedInfo retained;

  /*
   * If we utilize a ClassGraph while resolving types, we store it
   * here. This ensures that that ClassGraph will always be available
   * again. This is only used for top-level functions. If this
   * function is a method, it will instead be stored in the
   * ClassInfo2.
   *
   * This is wrapped in a std::unique_ptr because needing this for a
   * function is very rare and we want to avoid making FuncInfo2 any
   * larger than necessary. It also helps solve a circular dependency
   * between the types (since std::unique_ptr does not require a
   * complete type at declaration).
   */
  std::unique_ptr<AuxClassGraphs> auxClassGraphs;

  template <typename SerDe> void serde(SerDe&);
};

//////////////////////////////////////////////////////////////////////

/*
 * FuncInfos2 for the methods of a class which does not have a
 * ClassInfo2 (because it's uninstantiable). Even if a class doesn't
 * have a ClassInfo2, we might still need FuncInfo2 for it's
 * methods. These are stored in here.
 */
struct MethodsWithoutCInfo {
  SString cls;
  // Same order as the methods vector on the associated php::Class.
  CompactVector<std::unique_ptr<FuncInfo2>> finfos;
  // __invoke methods on any closures declared in this class. Same
  // order as closures vector on the associated php::Class.
  CompactVector<std::unique_ptr<FuncInfo2>> closureInvokes;
  template <typename SerDe> void serde(SerDe& sd) {
    sd(cls)
      (finfos)
      (closureInvokes)
      ;
  }
};

//////////////////////////////////////////////////////////////////////

template <typename SerDe> void InferredFuncInfo::serde(SerDe& sd) {
  ScopedStringDataIndexer _;
  sd(returnTy)
    (retParam)
    (returnRefinements)
    (effectFree)
    (unusedParams)
    ;
}

//////////////////////////////////////////////////////////////////////

template <typename SerDe> void InferredClassInfo::serde(SerDe& sd) {
  ScopedStringDataIndexer _;
  sd(clsConstantInfo, string_data_lt{})
    (useVars)
    (privateProps)
    (privateStatics)
    ;
}

//////////////////////////////////////////////////////////////////////

using FuncFamily       = res::Func::FuncFamily;
using FuncInfo         = res::Func::FuncInfo;

//////////////////////////////////////////////////////////////////////

/*
 * Sometimes function resolution can't determine which function
 * something will call, but can restrict it to a family of functions.
 *
 * For example, if you want to call an abstract function on a base
 * class with all unique derived classes, we will resolve the function
 * to a FuncFamily that contains references to all the possible
 * overriding-functions.
 *
 * In general, a FuncFamily can contain functions which are used by a
 * regular class or not. In some contexts, we only care about the
 * subset which are used by a regular class, and in some contexts we
 * care about them all. To save memory, we use a single FuncFamily for
 * both cases. The users of the FuncFamily must skip over which funcs
 * it does not care about.
 *
 * Since we cache information related to the func list, if the "all"
 * case and the "regular-only" case are potentially different, we
 * allocated space for both possibilities. If we determine they'll
 * always be the same, we do not. For example, if the possible func
 * list only contains methods on regular classes, the distinction is
 * irrelevant.
 */
struct res::Func::FuncFamily {
  // A PossibleFunc is a php::Func* with an additional bit that
  // indicates whether that func is present on a regular class or
  // not. This lets us skip over that func if we only care about the
  // regular subset of the list.
  struct PossibleFunc {
    PossibleFunc(const php::Func* f, bool r) : m_func{r, f} {}
    const php::Func* ptr() const { return m_func.ptr(); }
    bool inRegular() const { return (bool)m_func.tag(); }
    bool operator==(const PossibleFunc& o) const { return m_func == o.m_func; }
  private:
    CompactTaggedPtr<const php::Func, uint8_t> m_func;
  };
  using PFuncVec = CompactVector<PossibleFunc>;

  // We have a lot of FuncFamilies, and most of them have the same
  // "static" information (doesn't change as a result of
  // analysis). So, we store unique groups of static info separately
  // and FuncFamilies point to the same ones.
  struct StaticInfo {
    Optional<uint32_t> m_numInOut;
    Optional<RuntimeCoeffects> m_requiredCoeffects;
    Optional<CompactVector<CoeffectRule>> m_coeffectRules;
    PrepKindVec m_paramPreps;
    uint32_t m_minNonVariadicParams;
    uint32_t m_maxNonVariadicParams;
    TriBool m_isReadonlyReturn;
    TriBool m_isReadonlyThis;
    TriBool m_supportsAER;
    bool m_maybeReified : 1;
    bool m_maybeCaresAboutDynCalls : 1;
    bool m_maybeBuiltin : 1;

    bool operator==(const StaticInfo& o) const;
    size_t hash() const;
  };

  // State in the FuncFamily which might vary depending on whether
  // we're considering the regular subset or not.
  struct Info {
    LockFreeLazy<Index::ReturnType> m_returnTy;
    const StaticInfo* m_static{nullptr};
  };

  FuncFamily(PFuncVec&& v, bool add) : m_v{std::move(v)}
  { if (add) m_regular = std::make_unique<Info>(); }

  FuncFamily(FuncFamily&&) = delete;
  FuncFamily(const FuncFamily&) = delete;
  FuncFamily& operator=(FuncFamily&&) = delete;
  FuncFamily& operator=(const FuncFamily&) = delete;

  const PFuncVec& possibleFuncs() const {
    return m_v;
  }

  Info& infoFor(bool regularOnly) {
    if (regularOnly && m_regular) return *m_regular;
    return m_all;
  }
  const Info& infoFor(bool regularOnly) const {
    if (regularOnly && m_regular) return *m_regular;
    return m_all;
  }

  Info m_all;
  // Only allocated if we determined the distinction is relevant. If
  // this is nullptr, m_all can be used for both cases.
  std::unique_ptr<Info> m_regular;
  PFuncVec m_v;
};

bool FuncFamily::StaticInfo::operator==(const FuncFamily::StaticInfo& o) const {
  return
    std::tie(m_numInOut, m_requiredCoeffects, m_coeffectRules,
             m_paramPreps, m_minNonVariadicParams,
             m_maxNonVariadicParams,
             m_isReadonlyReturn, m_isReadonlyThis, m_supportsAER,
             m_maybeReified, m_maybeCaresAboutDynCalls,
             m_maybeBuiltin) ==
    std::tie(o.m_numInOut, o.m_requiredCoeffects, o.m_coeffectRules,
             o.m_paramPreps, o.m_minNonVariadicParams,
             o.m_maxNonVariadicParams,
             o.m_isReadonlyReturn, o.m_isReadonlyThis, o.m_supportsAER,
             o.m_maybeReified, o.m_maybeCaresAboutDynCalls,
             o.m_maybeBuiltin);
}

size_t FuncFamily::StaticInfo::hash() const {
  auto hash = folly::hash::hash_combine(
    m_numInOut,
    m_requiredCoeffects,
    m_minNonVariadicParams,
    m_maxNonVariadicParams,
    m_isReadonlyReturn,
    m_isReadonlyThis,
    m_supportsAER,
    m_maybeReified,
    m_maybeCaresAboutDynCalls,
    m_maybeBuiltin
  );
  hash = folly::hash::hash_range(
    m_paramPreps.begin(),
    m_paramPreps.end(),
    hash
  );
  if (m_coeffectRules) {
    hash = folly::hash::hash_range(
      m_coeffectRules->begin(),
      m_coeffectRules->end(),
      hash
    );
  }
  return hash;
}

//////////////////////////////////////////////////////////////////////

namespace {

struct PFuncVecHasher {
  size_t operator()(const FuncFamily::PFuncVec& v) const {
    return folly::hash::hash_range(
      v.begin(),
      v.end(),
      0,
      [] (FuncFamily::PossibleFunc pf) {
        return hash_int64_pair(
          pointer_hash<const php::Func>{}(pf.ptr()),
          pf.inRegular()
        );
      }
    );
  }
};
struct FuncFamilyPtrHasher {
  using is_transparent = void;
  size_t operator()(const std::unique_ptr<FuncFamily>& ff) const {
    return PFuncVecHasher{}(ff->possibleFuncs());
  }
  size_t operator()(const FuncFamily::PFuncVec& pf) const {
    return PFuncVecHasher{}(pf);
  }
};
struct FuncFamilyPtrEquals {
  using is_transparent = void;
  bool operator()(const std::unique_ptr<FuncFamily>& a,
                  const std::unique_ptr<FuncFamily>& b) const {
    return a->possibleFuncs() == b->possibleFuncs();
  }
  bool operator()(const FuncFamily::PFuncVec& pf,
                  const std::unique_ptr<FuncFamily>& ff) const {
    return pf == ff->possibleFuncs();
  }
};

struct FFStaticInfoPtrHasher {
  using is_transparent = void;
  size_t operator()(const std::unique_ptr<FuncFamily::StaticInfo>& i) const {
    return i->hash();
  }
  size_t operator()(const FuncFamily::StaticInfo& i) const {
    return i.hash();
  }
};
struct FFStaticInfoPtrEquals {
  using is_transparent = void;
  bool operator()(const std::unique_ptr<FuncFamily::StaticInfo>& a,
                  const std::unique_ptr<FuncFamily::StaticInfo>& b) const {
    return *a == *b;
  }
  bool operator()(const FuncFamily::StaticInfo& a,
                  const std::unique_ptr<FuncFamily::StaticInfo>& b) const {
    return a == *b;
  }
};

//////////////////////////////////////////////////////////////////////

}

/*
 * Sometimes function resolution can't determine which exact function
 * something will call, but can restrict it to a family of functions.
 *
 * For example, if you want to call a function on a base class, we
 * will resolve the function to a func family that contains references
 * to all the possible overriding-functions.
 *
 * In general, a func family can contain functions which are used by a
 * regular class or not. In some contexts, we only care about the
 * subset which are used by a regular class, and in some contexts we
 * care about them all. To save memory, we use a single func family
 * for both cases. The users of the func family should only consult
 * the subset they care about.
 *
 * Besides the possible functions themselves, information in common
 * about the functions is cached. For example, return type. This
 * avoids having to iterate over potentially very large sets of
 * functions.
 *
 * This class mirrors the FuncFamily struct but is produced and used
 * by remote workers. Once everything is converted to use remote
 * workers, this struct will replace FuncFamily entirely (and be
 * renamed).
 */
struct FuncFamily2 {
  // Func families don't have any inherent name, but it's convenient
  // to have an unique id to refer to each one. We produce a SHA1 hash
  // of all of the methods in the func family.
  using Id = SHA1;

  Id m_id;
  // All methods in a func family should have the same name. However,
  // multiple func families may have the same name (so this is not an
  // unique identifier).
  SString m_name;
  // Methods used by a regular classes
  std::vector<MethRef> m_regular;
  // Methods used exclusively by non-regular classes, but as a private
  // method. In some situations, these are treated as if it was on
  // m_regular.
  std::vector<MethRef> m_nonRegularPrivate;
  // Methods used exclusively by non-regular classes
  std::vector<MethRef> m_nonRegular;

  // Information about the group of methods relevant to analysis which
  // doesn't change (hence "static").
  struct StaticInfo {
    Optional<uint32_t> m_numInOut;
    Optional<RuntimeCoeffects> m_requiredCoeffects;
    Optional<CompactVector<CoeffectRule>> m_coeffectRules;
    PrepKindVec m_paramPreps;
    uint32_t m_minNonVariadicParams;
    uint32_t m_maxNonVariadicParams;
    TriBool m_isReadonlyReturn;
    TriBool m_isReadonlyThis;
    TriBool m_supportsAER;
    bool m_maybeReified;
    bool m_maybeCaresAboutDynCalls;
    bool m_maybeBuiltin;

    StaticInfo& operator|=(const StaticInfo& o) {
      if (m_numInOut != o.m_numInOut) {
        m_numInOut.reset();
      }
      if (m_requiredCoeffects != o.m_requiredCoeffects) {
        m_requiredCoeffects.reset();
      }
      if (m_coeffectRules != o.m_coeffectRules) {
        m_coeffectRules.reset();
      }

      if (o.m_paramPreps.size() > m_paramPreps.size()) {
        m_paramPreps.resize(
          o.m_paramPreps.size(),
          PrepKind{TriBool::No, TriBool::No}
        );
      }
      for (size_t i = 0; i < o.m_paramPreps.size(); ++i) {
        m_paramPreps[i].inOut |= o.m_paramPreps[i].inOut;
        m_paramPreps[i].readonly |= o.m_paramPreps[i].readonly;
      }
      for (size_t i = o.m_paramPreps.size(); i < m_paramPreps.size(); ++i) {
        m_paramPreps[i].inOut |= TriBool::No;
        m_paramPreps[i].readonly |= TriBool::No;
      }

      m_minNonVariadicParams =
        std::min(m_minNonVariadicParams, o.m_minNonVariadicParams);
      m_maxNonVariadicParams =
        std::max(m_maxNonVariadicParams, o.m_maxNonVariadicParams);
      m_isReadonlyReturn |= o.m_isReadonlyReturn;
      m_isReadonlyThis |= o.m_isReadonlyThis;
      m_supportsAER |= o.m_supportsAER;
      m_maybeReified |= o.m_maybeReified;
      m_maybeCaresAboutDynCalls |= o.m_maybeCaresAboutDynCalls;
      m_maybeBuiltin |= o.m_maybeBuiltin;

      return *this;
    }

    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_numInOut)
        (m_requiredCoeffects)
        (m_coeffectRules)
        (m_paramPreps)
        (m_minNonVariadicParams)
        (m_maxNonVariadicParams)
        (m_isReadonlyReturn)
        (m_isReadonlyThis)
        (m_supportsAER)
        (m_maybeReified)
        (m_maybeCaresAboutDynCalls)
        (m_maybeBuiltin)
        ;
    }
  };
  Optional<StaticInfo> m_allStatic;
  Optional<StaticInfo> m_regularStatic;

  const StaticInfo& infoFor(bool regular) const {
    if (regular) {
      assertx(m_regularStatic.has_value());
      return *m_regularStatic;
    }
    return *m_allStatic;
  }

  template <typename SerDe> void serde(SerDe& sd) {
    sd(m_id)
      (m_name)
      (m_regular)
      (m_nonRegularPrivate)
      (m_nonRegular)
      (m_allStatic)
      (m_regularStatic)
      ;
  }
};

namespace {

// Func families are (usually) very small, but we have a lot of
// them. To reduce remote worker overhead, we bundle func families
// together into one blob.
struct FuncFamilyGroup {
  std::vector<std::unique_ptr<FuncFamily2>> m_ffs;
  template <typename SerDe> void serde(SerDe& sd) {
    // Multiple func families may reuse the same class name, so we
    // want to de-dup strings.
    ScopedStringDataIndexer _;
    sd(m_ffs);
  }
};

//////////////////////////////////////////////////////////////////////

/*
 * A method family table entry in a compact format. Can represent a
 * FuncFamily, a single php::Func, or emptiness. This represents the
 * possible resolutions of a call to a method with same name. It also
 * stores whether the entry is "complete" or "incomplete". An
 * incomplete entry means the possible resolutions includes the
 * possibility of the method not existing. A complete entry guarantees
 * it has to be one of the methods. This is (right now) irrelevant for
 * FuncFamily, but matters for php::Func, as it determines whether you
 * can fold away the call (if it's incomplete, the call might fatal).
 *
 * We create a lot of these, so we use some trickery to keep it
 * pointer sized.
 */
struct FuncFamilyOrSingle {
  FuncFamilyOrSingle() : m_ptr{Type::Empty, nullptr} {}
  explicit FuncFamilyOrSingle(FuncFamily* ff, bool incomplete)
    : m_ptr{incomplete ? Type::FuncFamilyIncomplete : Type::FuncFamily, ff} {}
  FuncFamilyOrSingle(const php::Func* f, bool incomplete)
    : m_ptr{incomplete ? Type::SingleIncomplete : Type::Single, (void*)f} {}

  // If this represents a FuncFamily, return it (or nullptr
  // otherwise).
  FuncFamily* funcFamily() const {
    return
      (m_ptr.tag() == Type::FuncFamily ||
       m_ptr.tag() == Type::FuncFamilyIncomplete)
        ? (FuncFamily*)m_ptr.ptr()
        : nullptr;
  }

  // If this represents a single php::Func, return it (or nullptr
  // otherwise).
  const php::Func* func() const {
    return
      (m_ptr.tag() == Type::Single || m_ptr.tag() == Type::SingleIncomplete)
        ? (const php::Func*)m_ptr.ptr()
        : nullptr;
  }

  // Return true if this entry represents nothing at all (for example,
  // if the method is guaranteed to not exist).
  bool isEmpty() const { return m_ptr.tag() == Type::Empty; }

  // NB: empty entries are neither incomplete nor complete. Check
  // isEmpty() first if that matters.

  // Return true if this resolution includes the possibility of no
  // method.
  bool isIncomplete() const {
    return
      m_ptr.tag() == Type::FuncFamilyIncomplete ||
      m_ptr.tag() == Type::SingleIncomplete;
  }
  // Return true if the method would resolve to exactly one of the
  // possibilities.
  bool isComplete() const {
    return
      m_ptr.tag() == Type::FuncFamily ||
      m_ptr.tag() == Type::Single;
  }

private:
  enum class Type : uint8_t {
    Empty,
    FuncFamily,
    FuncFamilyIncomplete,
    Single,
    SingleIncomplete
  };
  CompactTaggedPtr<void, Type> m_ptr;
};

std::string show(const FuncFamilyOrSingle& fam) {
  if (auto const ff = fam.funcFamily()) {
    auto const f = ff->possibleFuncs().front().ptr();
    return folly::sformat(
      "func-family {}::{}{}",
      f->cls->name, f->name,
      fam.isIncomplete() ? " (incomplete)" : ""
    );
  }
  if (auto const f = fam.func()) {
    return folly::sformat(
      "func {}::{}{}",
      f->cls->name, f->name,
      fam.isIncomplete() ? " (incomplete)" : ""
    );
  }
  return "empty";
}

/*
 * A method family table entry. Each entry encodes the possible
 * resolutions of a method call on a particular class. The reason why
 * this isn't just a func family is because we don't want to create a
 * func family when there's only one possible method involved (this is
 * common and if we did we'd create way more func
 * families). Furthermore, we really want information for two
 * different resolutions. One resolution is when we're only
 * considering regular classes, and the other is when considering all
 * classes. One of these resolutions can correspond to a func family
 * and the other may not. This struct encodes all the possible cases
 * that can occur.
 */
struct FuncFamilyEntry {
  // The equivalent of FuncFamily::StaticInfo, but only relevant for a
  // single method (so doesn't have a FuncFamily where the StaticInfo
  // can live). This can be derived from the method directly, but by
  // storing it here, we don't need to send the actual methods to the
  // workers.
  struct MethMetadata {
    MethMetadata() : m_requiredCoeffects{RuntimeCoeffects::none()} {}

    PrepKindVec m_prepKinds;
    CompactVector<CoeffectRule> m_coeffectRules;
    uint32_t m_numInOut;
    uint32_t m_nonVariadicParams;
    RuntimeCoeffects m_requiredCoeffects;
    bool m_isReadonlyReturn : 1;
    bool m_isReadonlyThis : 1;
    bool m_supportsAER : 1;
    bool m_isReified : 1;
    bool m_caresAboutDyncalls : 1;
    bool m_builtin : 1;

    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_prepKinds)
        (m_coeffectRules)
        (m_numInOut)
        (m_nonVariadicParams)
        (m_requiredCoeffects)
        ;
      SERDE_BITFIELD(m_isReadonlyReturn, sd);
      SERDE_BITFIELD(m_isReadonlyThis, sd);
      SERDE_BITFIELD(m_supportsAER, sd);
      SERDE_BITFIELD(m_isReified, sd);
      SERDE_BITFIELD(m_caresAboutDyncalls, sd);
      SERDE_BITFIELD(m_builtin, sd);
    }
  };

  // Both "regular" and "all" resolutions map to a func family. This
  // must always be the same func family because the func family
  // stores the information necessary for both cases.
  struct BothFF {
    FuncFamily2::Id m_ff;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_ff);
    }
  };
  // The "all" resolution maps to a func family but the "regular"
  // resolution maps to a single method.
  struct FFAndSingle {
    FuncFamily2::Id m_ff;
    MethRef m_regular;
    // If true, m_regular is actually non-regular, but a private
    // method (which is sometimes treated as regular).
    bool m_nonRegularPrivate;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_ff)(m_regular)(m_nonRegularPrivate);
    }
  };
  // The "all" resolution maps to a func family but the "regular"
  // resolution maps to nothing (for example, there's no regular
  // classes with that method).
  struct FFAndNone {
    FuncFamily2::Id m_ff;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_ff);
    }
  };
  // Both the "all" and "regular" resolutions map to (the same) single
  // method.
  struct BothSingle {
    MethRef m_all;
    MethMetadata m_meta;
    // If true, m_all is actually non-regular, but a private method
    // (which is sometimes treated as regular).
    bool m_nonRegularPrivate;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_all)(m_meta)(m_nonRegularPrivate);
    }
  };
  // The "all" resolution maps to a single method but the "regular"
  // resolution maps to nothing.
  struct SingleAndNone {
    MethRef m_all;
    MethMetadata m_meta;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(m_all)(m_meta);
    }
  };
  // No resolutions at all.
  struct None {
    template <typename SerDe> void serde(SerDe&) {}
  };

  std::variant<
    BothFF, FFAndSingle, FFAndNone, BothSingle, SingleAndNone, None
  > m_meths{None{}};
  // A resolution is "incomplete" if there's a subclass which does not
  // contain any method with that name (not even inheriting it). If a
  // resolution is incomplete, it means besides the set of resolved
  // methods, the call might also error due to missing method. This
  // distinction is only important in a few limited circumstances.
  bool m_allIncomplete{true};
  bool m_regularIncomplete{true};
  // Whether any method in the resolution overrides a private
  // method. This is only of interest when building func families.
  bool m_privateAncestor{false};

  template <typename SerDe> void serde(SerDe& sd) {
    if constexpr (SerDe::deserializing) {
      m_meths = [&] () -> decltype(m_meths) {
        uint8_t tag;
        sd(tag);
        switch (tag) {
          case 0: return sd.template make<BothFF>();
          case 1: return sd.template make<FFAndSingle>();
          case 2: return sd.template make<FFAndNone>();
          case 3: return sd.template make<BothSingle>();
          case 4: return sd.template make<SingleAndNone>();
          case 5: return sd.template make<None>();
          default: always_assert(false);
        }
      }();
    } else {
      match(
        m_meths,
        [&] (const BothFF& e)        { sd(uint8_t(0))(e); },
        [&] (const FFAndSingle& e)   { sd(uint8_t(1))(e); },
        [&] (const FFAndNone& e)     { sd(uint8_t(2))(e); },
        [&] (const BothSingle& e)    { sd(uint8_t(3))(e); },
        [&] (const SingleAndNone& e) { sd(uint8_t(4))(e); },
        [&] (const None& e)          { sd(uint8_t(5))(e); }
      );
    }

    sd(m_allIncomplete)
       (m_regularIncomplete)
       (m_privateAncestor)
      ;
  }
};

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

/*
 * ClassGraph is an abstraction for representing a subset of the
 * complete class hierarchy in a program. Every instance of ClassGraph
 * "points" into the hierarchy at a specific class. From an instance,
 * all (transitive) parents are known, and all (transitive) children
 * may be known.
 *
 * Using a ClassGraph, one can obtain relevant information about the
 * class hierarchy, or perform operations between two classes (union
 * or intersection, for example), without having the ClassInfo
 * available.
 *
 * One advantage of ClassGraph is that it is self
 * contained. Serializing an instance of ClassGraph will serialize all
 * information needed about the hierarchy. This means that an
 * extern-worker job does not need to retrieve additional metadata
 * before using it.
 */
struct ClassGraph {
  // Default constructed instances are not usable for anything (needed
  // for serialization). Use the below factory functions to actually
  // create them.
  ClassGraph() = default;

  // A ClassGraph is falsey if it has been default-constructed.
  explicit operator bool() const { return this_; }

  // Retrieve the name of this class.
  SString name() const;

  // Retrieve an optional ClassInfo/ClassInfo2 associated with this
  // class. A ClassGraph is not guaranteed to have a
  // ClassInfo/ClassInfo2, but if it does, this can be used to save a
  // name -> info lookup.
  ClassInfo* cinfo() const;
  ClassInfo2* cinfo2() const;

  // Return a class equivalent to this one without considering
  // non-regular classes. This might be this class, or a subclass.
  ClassGraph withoutNonRegular() const;

  // Whether the class might be a regular or non-regular class. This
  // check is precise if isMissing() is false.
  bool mightBeRegular() const;
  bool mightBeNonRegular() const;

  // Whether this class might have any regular or non-regular
  // subclasses (not including this class itself). This check is
  // precise if hasCompleteChildren() or isConservative() is true.
  bool mightHaveRegularSubclass() const;
  bool mightHaveNonRegularSubclass() const;

  // A "missing" ClassGraph is a special variant which represents a
  // class about which nothing is known. The class might not even
  // exist. The only valid thing to do with such a class is query its
  // name.
  bool isMissing() const;

  // A class might or might not have complete knowledge about its
  // children. If this returns true, you can perform any of the below
  // queries on it. If not, you can only perform queries related to
  // the parents (non-missing classes always have complete parent
  // information).
  bool hasCompleteChildren() const;

  // A conservative class is one which will never have complete
  // children. This generally means that the class has too many
  // subclasses to efficiently represent. We treat such classes
  // conservatively, except in a few cases.
  bool isConservative() const;

  // Whether this class is an interface, a trait, an enum, or an
  // abstract class. It is invalid to check these if isMissing() is
  // true.
  bool isInterface() const;
  bool isTrait() const;
  bool isEnum() const;
  bool isAbstract() const;

  // Retrieve the immediate base class of this class. If this class
  // doesn't have an immediate base, or if isMissing() is true, a
  // falsey ClassGraph is returned.
  ClassGraph base() const;

  // Retrieve the "topmost" base class of this class. This is the base
  // class which does not have a base class.
  ClassGraph topBase() const;

  // Retrieve all base classes of this class, including this
  // class. The ordering is from top-most class to this one (so this
  // one will be the last element). The returned classes may not have
  // complete child info, even if this class does.
  std::vector<ClassGraph> bases() const;

  // Retrieve all interfaces implemented by this class, either
  // directly, or by transitive parents. This includes this class if
  // it is an interface. The ordering is alphabetical. Like bases(),
  // the returned classes may not have complete child info, even if
  // this class does.
  std::vector<ClassGraph> interfaces() const;

  // Walk over all parents of this class, calling the supplied
  // callable with each parent. If the callable returns true, that
  // parent's parents will then be visited. If false, then they will
  // be skipped.
  template <typename F> void walkParents(const F&) const;

  // Retrieve all children of this class (including this class
  // itself). This is only valid to call if hasCompleteChildren() is
  // true. NB: Being on a class' children list does not necessarily
  // mean that it has a "is-a" relationship. Namely, classes on a
  // trait's children list are not instances of the trait itself.
  std::vector<ClassGraph> children(bool regOnly = false) const;

  // Retrieve the interfaces implemented by this class directly.
  std::vector<ClassGraph> declInterfaces() const {
    return directParents(FlagInterface);
  }
  // Retrieve the set of non-flattened traits used by this class
  // directly. Any traits flattened into this class will not appear.
  std::vector<ClassGraph> usedTraits() const {
    return directParents(FlagTrait);
  }

  // Retrieve the direct parents of this class (all of the classes for
  // which this class is a direct child).
  std::vector<ClassGraph> directParents() const;

  // Returns true if this class is a child of the other class.
  bool isChildOf(ClassGraph) const;

  // Returns a list of the parents that *all* regular subclasses of
  // this class have in common.
  std::vector<ClassGraph> commonParentsOfRegSubs() const;

  // Retrieve the set of classes which *might* be equivalent to this
  // class when ignoring non-regular classes. This does not include
  // subclasses of this class.
  std::vector<ClassGraph> candidateRegOnlyEquivs() const;

  // Subtype checks
  bool exactSubtypeOfExact(ClassGraph, bool nonRegL, bool nonRegR) const;
  bool exactSubtypeOf(ClassGraph, bool nonRegL, bool nonRegR) const;
  bool subSubtypeOf(ClassGraph, bool nonRegL, bool nonRegR) const;

  // Could-be checks
  bool exactCouldBeExact(ClassGraph, bool nonRegL, bool nonRegR) const;
  bool exactCouldBe(ClassGraph, bool nonRegL, bool nonRegR) const;
  bool subCouldBe(ClassGraph, bool nonRegL, bool nonRegR) const;

  // "Ensures" that this ClassGraph will be present (with the
  // requested information) for subsequent analysis rounds.
  void ensure() const;
  void ensureWithChildren(bool regOnly = false) const;
  void ensureCInfo() const;

  // Used when building ClassGraphs initially.
  void setClosureBase();
  void setComplete();
  void setBase(ClassGraph);
  void addParent(ClassGraph);
  void flattenTraitInto(ClassGraph);
  void setCInfo(ClassInfo&);
  void setRegOnlyEquivs() const;
  void finalizeParents();
  void reset();

  // ClassGraphs are ordered by their name alphabetically.
  bool operator==(ClassGraph h) const { return this_ == h.this_; }
  bool operator!=(ClassGraph h) const { return this_ != h.this_; }
  bool operator<(ClassGraph h) const;

  // Create a new ClassGraph corresponding to the given php::Class.
  // This function will assert if a ClassGraph with the php::Class'
  // name already exists.
  static ClassGraph create(const php::Class&);
  // Retrieve a ClassGraph with the given name, asserting if one
  // doesn't exist.
  static ClassGraph get(SString);
  // Retrieve a ClassGraph with the given name. If one doesn't already
  // existing, one will be created as if by calling getMissing().
  static ClassGraph getOrCreate(SString);
  // Retrieve a "missing" ClassGraph with the given name, asserting if
  // a non-missing one already exists. This is mainly used for tests.
  static ClassGraph getMissing(SString);

  // Before using ClassGraph, it must be initialized (particularly
  // before deserializing any). initConcurrent() must be used if any
  // deserialization with be performed in multiple threads
  // concurrently.
  static void init();
  static void initConcurrent();
  // If initConcurrent() was used, this must be used when
  // deserialization is done and perform querying any ClassGraphs.
  static void stopConcurrent();
  // Called to clean up memory used by ClassGraph framework.
  static void destroy();

  // Set/clear an AnalysisIndex to use for calls to ensure() (and
  // family).
  static void setAnalysisIndex(AnalysisIndex::IndexData&);
  static void clearAnalysisIndex();

  template <typename SerDe, typename T> void serde(SerDe&, T, bool = false);

  // When serializing multiple ClassGraphs, this can be declared
  // before serializing any of them, to allow for the serialization to
  // share common state and take up less space.
  struct ScopedSerdeState;

  friend struct ClassGraphHasher;

private:
  struct Node;
  struct Table;

  struct TLNodeIdxSet;

  using NodeSet = hphp_fast_set<Node*, pointer_hash<Node>>;
  template <typename T>
  using NodeMap = hphp_fast_map<Node*, T, pointer_hash<Node>>;
  using NodeVec = TinyVector<Node*, 4>;

  struct SmallBitset;
  struct LargeBitset;
  template <typename> struct ParentTracker;

  enum Flags : std::uint16_t {
    FlagNone         = 0,
    FlagInterface    = (1 << 0),
    FlagTrait        = (1 << 1),
    FlagAbstract     = (1 << 2),
    FlagEnum         = (1 << 3),
    FlagCInfo2       = (1 << 4),
    FlagRegSub       = (1 << 5),
    FlagNonRegSub    = (1 << 6),
    FlagWait         = (1 << 7),
    FlagChildren     = (1 << 8),
    FlagConservative = (1 << 9),
    FlagMissing      = (1 << 10)
  };

  // These are only set at runtime, so shouldn't be serialized in a
  // Node.
  static constexpr Flags kSerializable =
    (Flags)~(FlagCInfo2 | FlagRegSub | FlagNonRegSub |
             FlagWait | FlagChildren | FlagConservative);

  // Node on the graph:
  struct Node {
    // The flags are stored along with any ClassInfo to save memory.
    using CIAndFlags = CompactTaggedPtr<void, Flags>;

    SString name{nullptr};
    // Atomic because they might be manipulated from multiple threads
    // during deserialization.
    std::atomic<CIAndFlags::Opaque> ci{CIAndFlags{}.getOpaque()};
    // Direct (not transitive) parents and children of this node.
    CompactVector<Node*> parents;
    CompactVector<Node*> children;

    // This information is lazily cached (and is not serialized).
    struct NonRegularInfo {
      NodeSet subclassOf;
      Node* regOnlyEquiv{nullptr};
    };
    LockFreeLazyPtr<NonRegularInfo> nonRegInfo;
    LockFreeLazyPtr<NonRegularInfo> nonRegInfoDisallow;

    LockFreeLazy<NodeSet> commonParentsOfRegSubs;

    // Unique sequential id assigned to every node. Used for NodeIdxSet.
    using Idx = uint32_t;
    Idx idx{0};

    Flags flags() const { return CIAndFlags{ci.load()}.tag(); }
    ClassInfo* cinfo() const {
      CIAndFlags cif{ci.load()};
      if (cif.tag() & FlagCInfo2) return nullptr;
      return (ClassInfo*)cif.ptr();
    }
    ClassInfo2* cinfo2() const {
      CIAndFlags cif{ci.load()};
      if (!(cif.tag() & FlagCInfo2)) return nullptr;
      return (ClassInfo2*)cif.ptr();
    }
    void* rawPtr() const {
      CIAndFlags cif{ci.load()};
      return cif.ptr();
    }

    bool isBase() const { return !(flags() & (FlagInterface | FlagTrait)); }
    bool isRegular() const {
      return !(flags() & (FlagInterface | FlagTrait | FlagAbstract | FlagEnum));
    }
    bool isTrait() const { return flags() & FlagTrait; }
    bool isInterface() const { return flags() & FlagInterface; }
    bool isEnum() const { return flags() & FlagEnum; }
    bool isAbstract() const { return flags() & FlagAbstract; }
    bool isMissing() const { return flags() & FlagMissing; }
    bool hasCompleteChildren() const { return flags() & FlagChildren; }
    bool isConservative() const { return flags() & FlagConservative; }

    bool hasRegularSubclass() const { return flags() & FlagRegSub; }
    bool hasNonRegularSubclass() const { return flags() & FlagNonRegSub; }

    const NonRegularInfo& nonRegularInfo();

    // NB: These aren't thread-safe, so don't use them during concurrent
    // deserialization.
    void setFlags(Flags f, Flags r = FlagNone) {
      CIAndFlags old{ci.load()};
      old.set((Flags)((old.tag() | f) & ~r), old.ptr());
      ci.store(old.getOpaque());
    }
    void setCInfo(ClassInfo& c) {
      CIAndFlags old{ci.load()};
      old.set((Flags)(old.tag() & ~FlagCInfo2), &c);
      ci.store(old.getOpaque());
    }
    void setCInfo(ClassInfo2& c) {
      CIAndFlags old{ci.load()};
      old.set((Flags)(old.tag() | FlagCInfo2), &c);
      ci.store(old.getOpaque());
    }

    struct Compare {
      bool operator()(const Node* a, const Node* b) const {
        assertx(a->name);
        assertx(b->name);
        return string_data_lt_type{}(a->name, b->name);
      }
    };
  };

  struct NodeIdxSet {
    NodeIdxSet();

    bool operator[](const Node& n) const {
      return n.idx < versions.size() && versions[n.idx] == version;
    }
    bool add(Node& n);
    void erase(Node& n);
    void clear() { ++version; }
  private:
    std::vector<uint64_t> versions;
    uint64_t version;
  };

  // Iterating through parents or children can result in one of three
  // different outcomes:
  enum class Action {
    Continue, // Keep iterating into any children/parents
    Stop, // Stop iteration entirely
    Skip // Continue iteration, but skip over any children/parents of
         // this class
  };

  // Class graph-internal state for serialization. Since a common access
  // pattern is to create/destroy many SerdeState's while serializing the
  // output of analysis jobs, the state may be used via the activate/deactivate
  // APIs, which violate RAII and explicitly leak the containers on
  // deactivation.
  struct SerdeState {
    SerdeState() : active{false} {}

    bool isActive() const {
      return active;
    }

    void activate() {
      assertx(!active);
      active = true;
    }

    void deactivate() {
      assertx(active);
      active = false;

      if (upward) upward->clear();
      if (downward) downward->clear();

      strings.clear();
      newStrings.clear();
      strToIdx.clear();
    }

    Optional<NodeIdxSet> upward;
    Optional<NodeIdxSet> downward;

    std::vector<SString> strings;
    std::vector<SString> newStrings;
    SStringToOneT<size_t> strToIdx;

  private:
    bool active;
  };

  std::vector<ClassGraph> directParents(Flags) const;

  bool storeAuxs(AnalysisIndex::IndexData&, bool) const;
  bool onAuxs(AnalysisIndex::IndexData&, bool, bool) const;

  static Table& table();

  static NodeVec combine(const NodeVec&, const NodeVec&,
                         bool, bool, bool, bool);
  static NodeVec intersect(const NodeVec&, const NodeVec&,
                           bool, bool, bool&);
  static NodeVec removeNonReg(const NodeVec&);
  static bool couldBeIsect(const NodeVec&, const NodeVec&, bool, bool);
  static NodeVec canonicalize(const NodeSet&, bool);

  template <typename F>
  static void enumerateIsectMembers(const NodeVec&, bool,
                                    const F&, bool = false);

  static std::pair<NodeSet, NodeSet> calcSubclassOfSplit(Node&, bool);
  static NodeSet calcSubclassOf(Node&, bool);
  static Node* calcRegOnlyEquiv(Node&, const NodeSet&);

  static bool betterNode(Node*, Node*);

  template <typename F>
  static Action forEachParent(Node&, const F&, NodeIdxSet&);
  template <typename F>
  static Action forEachParent(Node&, const F&);
  template <typename F>
  static Action forEachParentImpl(Node&, const F&, NodeIdxSet*, bool);

  template <typename F>
  static Action forEachChild(Node&, const F&, NodeIdxSet&, bool = true);
  template <typename F>
  static Action forEachChild(Node&, const F&, bool = true);
  template <typename F>
  static Action forEachChildImpl(Node&, const F&, NodeIdxSet*, bool);

  template <typename F, typename F2, typename T>
  static T foldParents(Node& n, const F& f, const F2& f2, NodeMap<T>& m) {
    return foldParentsImpl(n, f, f2, m, true);
  }
  template <typename F, typename F2, typename T>
  static T foldParentsImpl(Node&, const F&, const F2&, NodeMap<T>&, bool);

  static bool findParent(Node&, Node&, NodeIdxSet&);
  static bool findParent(Node&, Node&);

  static NodeSet allParents(Node&);

  struct LockedSerdeImpl;
  struct UnlockedSerdeImpl;

  template <typename SerDe> static void encodeName(SerDe&, SString);
  template <typename SerDe> static SString decodeName(SerDe&);

  template <typename SerDe, typename Impl, typename T>
  void serdeImpl(SerDe&, const Impl&, T, bool);

  template <typename SerDe, typename Impl>
  static void deserBlock(SerDe&, const Impl&);
  template <typename SerDe> static size_t serDownward(SerDe&, Node&);
  template <typename SerDe> static bool serUpward(SerDe&, Node&);

  template <typename Impl>
  static std::pair<Flags, Optional<size_t>> setCompleteImpl(const Impl&, Node&);

  template <typename Impl>
  static void setConservative(const Impl&, Node&, bool, bool);

  static std::unique_ptr<Table> g_table;
  static thread_local SerdeState tl_serde_state;

  friend struct res::Class;

  explicit ClassGraph(Node* n) : this_{n} {}

  Node* this_{nullptr};
};

std::unique_ptr<ClassGraph::Table> ClassGraph::g_table{nullptr};
// tl_serde_state is meant to only be accessed via `ScopedSerdeState`.
// ScopedSerdeState intentionally leaks the internal container memory
// of the thread-local state. This speeds up worker processes
// that serialize many class graphs and would otherwise cause
// memory pressure.
thread_local ClassGraph::SerdeState ClassGraph::tl_serde_state{};

struct ClassGraphHasher {
  using folly_is_avalanching = std::true_type;
  size_t operator()(ClassGraph g) const {
    return pointer_hash<ClassGraph::Node>{}(g.this_);
  }
};

bool ClassGraph::NodeIdxSet::add(Node& n) {
  if (n.idx >= versions.size()) {
    versions.resize(folly::nextPowTwo(n.idx+1), 0);
  }
  if (versions[n.idx] == version) return false;
  versions[n.idx] = version;
  return true;
}

void ClassGraph::NodeIdxSet::erase(Node& n) {
  if (n.idx < versions.size()) versions[n.idx] = 0;
}

// Thread-local NodeIdxSet which automatically clears itself
// afterwards (thus avoids memory allocation).
struct ClassGraph::TLNodeIdxSet {
  TLNodeIdxSet() : set{get()} {}

  ~TLNodeIdxSet() {
    set.clear();
    sets().emplace_back(std::move(set));
  }

  NodeIdxSet& operator*() { return set; }
  NodeIdxSet* operator->() { return &set; }
private:
  NodeIdxSet set;

  static NodeIdxSet get() {
    auto& s = sets();
    if (s.empty()) return {};
    auto set = std::move(s.back());
    s.pop_back();
    return set;
  }

  static std::vector<NodeIdxSet>& sets() {
    static thread_local std::vector<NodeIdxSet> s;
    return s;
  }
};

struct ClassGraph::Table {
  // Node map to ensure pointer stability.
  TSStringToOneNodeT<Node> nodes;
  // Mapping of one node equivalent to another when only considering
  // regular subclasses. No entry if the mapping is an identity or to
  // a subclass (which is common). Stored separately to save memory
  // since it's rare.
  hphp_fast_map<Node*, Node*> regOnlyEquivs;
  hphp_fast_map<Node*, size_t> completeSizeCache;
  AnalysisIndex::IndexData* index{nullptr};
  struct Locking {
    mutable folly::SharedMutex table;
    std::array<std::mutex, 2048> nodes;
    mutable folly::SharedMutex equivs;
    mutable folly::SharedMutex sizes;
  };
  // If present, we're doing concurrent deserialization.
  Optional<Locking> locking;
};

ClassGraph::NodeIdxSet::NodeIdxSet()
  : versions{folly::nextPowTwo((Node::Idx)(table().nodes.size() + 1)), 0}
  , version{1}
{
  assertx(!table().locking);
}

struct ClassGraph::ScopedSerdeState {
  ScopedSerdeState() {
    // If there's no SerdeState active, make one active, otherwise do
    // nothing.
    if (tl_serde_state.isActive()) return;
    tl_serde_state.activate();
    activated = true;
  }

  ~ScopedSerdeState() {
    assertx(tl_serde_state.isActive());
    if (!activated) return;
    tl_serde_state.deactivate();
  }

  ScopedSerdeState(const ScopedSerdeState&) = delete;
  ScopedSerdeState(ScopedSerdeState&&) = delete;
  ScopedSerdeState& operator=(const ScopedSerdeState&) = delete;
  ScopedSerdeState& operator=(ScopedSerdeState&&) = delete;

private:
  bool activated{false};
};

/*
 * When operating over ClassGraph nodes, we often need compact sets of
 * them. This is easy to do with bitsets, but we don't have a fixed
 * upper-size of the sets. We could always use dynamic_bitset, but
 * this can be inefficient. Instead we templatize the algorithm over
 * the bitset, and use a fixed size std::bitset for the common case
 * and dynamic_bitset for the (rare) exceptions.
 */
struct ClassGraph::SmallBitset {
  explicit SmallBitset(size_t limit) {
    assertx(limit <= bits.size());
  }
  SmallBitset(size_t limit, size_t idx) {
    assertx(limit <= bits.size());
    assertx(idx < limit);
    bits[idx] = true;
  }
  SmallBitset& operator|=(const SmallBitset& o) {
    bits |= o.bits;
    return *this;
  }
  SmallBitset& operator&=(const SmallBitset& o) {
    bits &= o.bits;
    return *this;
  }
  SmallBitset& operator-=(size_t idx) {
    assertx(idx < bits.size());
    bits[idx] = false;
    return *this;
  }
  SmallBitset& flip(size_t limit) {
    assertx(limit <= bits.size());
    bits.flip();
    auto const offset = bits.size() - limit;
    bits <<= offset;
    bits >>= offset;
    return *this;
  }
  bool test(size_t idx) const {
    assertx(idx < bits.size());
    return bits[idx];
  }
  bool any() const { return bits.any(); }
  bool none() const { return bits.none(); }
  bool all(size_t limit) const {
    auto s = *this;
    s.flip(limit);
    return s.none();
  }
  size_t first() const { return bitset_find_first(bits); }
  size_t next(size_t prev) const { return bitset_find_next(bits, prev); }
  bool operator==(const SmallBitset& o) const { return bits == o.bits; }

  static constexpr size_t kMaxSize = 64;
  using B = std::bitset<kMaxSize>;
  B bits;

  struct Hasher {
    size_t operator()(const SmallBitset& b) const {
      return std::hash<B>{}(b.bits);
    }
  };
};

struct ClassGraph::LargeBitset {
  explicit LargeBitset(size_t limit): bits{limit} {}
  LargeBitset(size_t limit, size_t idx): bits{limit} {
    assertx(idx < limit);
    bits[idx] = true;
  }
  LargeBitset& operator|=(const LargeBitset& o) {
    bits |= o.bits;
    return *this;
  }
  LargeBitset& operator&=(const LargeBitset& o) {
    bits &= o.bits;
    return *this;
  }
  LargeBitset& operator-=(size_t idx) {
    assertx(idx < bits.size());
    bits[idx] = false;
    return *this;
  }
  LargeBitset& flip(size_t limit) {
    assertx(limit == bits.size());
    bits.flip();
    return *this;
  }
  bool test(size_t idx) const {
    assertx(idx < bits.size());
    return bits[idx];
  }
  bool any() const { return bits.any(); }
  bool none() const { return bits.none(); }
  bool all(size_t) const { return bits.all(); }
  size_t first() const { return bits.find_first(); }
  size_t next(size_t prev) const { return bits.find_next(prev); }
  bool operator==(const LargeBitset& o) const { return bits == o.bits; }

  boost::dynamic_bitset<> bits;

  struct Hasher {
    size_t operator()(const LargeBitset& b) const {
      return std::hash<boost::dynamic_bitset<>>{}(b.bits);
    }
  };
};

// Helper class (parameterized over bitset type) to track Nodes and
// their parents.
template <typename Set>
struct ClassGraph::ParentTracker {
  // Heads is the universe of nodes which are tracked.
  template <typename T>
  explicit ParentTracker(const T& heads, const NodeSet* ignore = nullptr)
    : count{heads.size()}
    , valid{count}
    , ignore{ignore}
  {
    toNode.insert(heads.begin(), heads.end());
    indices.reserve(count);
    for (size_t i = 0; i < count; ++i) indices[toNode[i]] = i;
    valid.flip(count);
  }

  // Intersect this node and it's parents with the current valid ones
  // and return true if any remain.
  bool operator()(Node& n) {
    valid &= set(n);
    return valid.any();
  }
  // Return true if this node and it's parents contain all nodes being
  // tracked.
  bool all(Node& n) { return set(n).all(count); }

  // Return all nodes left in the valid set.
  NodeSet nodes() const {
    NodeSet s;
    for (auto i = valid.first(); i < count; i = valid.next(i)) {
      s.emplace(toNode[i]);
    }
    return s;
  }

private:
  struct Wrapper {
    explicit Wrapper(size_t limit): s{limit} {}
    Wrapper(size_t limit, size_t b): s{limit, b} {}
    explicit operator bool() const { return stop; }
    Wrapper& operator|=(const Wrapper& o) {
      s |= o.s;
      stop &= o.stop;
      return *this;
    }
    Set s;
    bool stop{false};
  };

  Set set(Node& n) {
    if (n.isMissing()) {
      auto const idx = folly::get_ptr(indices, &n);
      if (!idx) return Set{count};
      assertx(*idx < count);
      return Set{count, *idx};
    }

    auto wrapper = foldParents(
      n,
      [&] (Node& p) {
        if (ignore && ignore->contains(&p)) {
          Wrapper w{count};
          w.stop = true;
          return w;
        }
        auto const idx = folly::get_ptr(indices, &p);
        if (!idx) return Wrapper{count};
        assertx(*idx < count);
        return Wrapper{count, *idx};
      },
      [&] { return Wrapper{count}; },
      nodeToParents
    );
    return wrapper.s;
  }

  size_t count;
  Set valid;
  NodeMap<size_t> indices;
  NodeMap<Wrapper> nodeToParents;
  NodeVec toNode;
  const NodeSet* ignore;
};

// Abstractions for whether we're deserializing concurrently or
// not. This separates out the locking logic and let's us avoid any
// runtime checks (except one) during deserializing to see if we need
// to lock.

// Non-concurrent implementation. This just modifies the fields
// without any locking.
struct ClassGraph::UnlockedSerdeImpl {
  std::pair<Node*, bool> create(SString name) const {
    auto& t = table();
    auto& n = t.nodes[name];
    if (n.name) {
      assertx(n.name->tsame(name));
      return std::make_pair(&n, false);
    }
    n.name = name;
    assertx(t.nodes.size() < std::numeric_limits<Node::Idx>::max());
    n.idx = t.nodes.size();
    assertx(n.idx > 0);
    return std::make_pair(&n, true);
  }
  Node& get(SString name) const {
    auto n = folly::get_ptr(table().nodes, name);
    always_assert_flog(
      n,
      "Attempting to retrieve missing ClassGraph node '{}'",
      name
    );
    assertx(n->name->tsame(name));
    return *n;
  }
  void setEquiv(Node& n, Node& e) const {
    auto const [it, s] = table().regOnlyEquivs.emplace(&n, &e);
    always_assert(s || it->second == &e);
  }
  size_t getCompleteSize(Node& n) const {
    auto const size = folly::get_default(table().completeSizeCache, &n);
    always_assert(size > 1);
    return size;
  }
  void setCompleteSize(Node& n, size_t size) const {
    assertx(size > 1);
    auto const [it, s] = table().completeSizeCache.emplace(&n, size);
    always_assert(s || it->second == size);
  }
  template <typename F> void lock(Node&, const F& f) const { f(); }
  template <typename F> void forEachChild(Node& n, const F& f) const {
    for (auto const c : n.children) f(*c);
  }
  void signal(Node& n, Flags f) const {
    assertx(n.flags() == FlagNone);
    assertx(!(f & FlagWait));
    n.setFlags(f);
  }
  void setCInfo(Node& n, ClassInfo& cinfo) const { n.setCInfo(cinfo); }
  void setCInfo(Node& n, ClassInfo2& cinfo) const { n.setCInfo(cinfo); }
  void updateFlags(Node& n, Flags add, Flags remove = FlagNone) const {
    if (add == FlagNone && remove == FlagNone) return;
    auto const oldFlags = n.flags();
    auto const newFlags = (Flags)((oldFlags | add) & ~remove);
    if (newFlags == oldFlags) return;
    n.ci.store(Node::CIAndFlags{newFlags, n.rawPtr()}.getOpaque());
  }
};

// Concurrent implementation. The node table is guarded by a RWLock
// and the individual nodes are guarded by an array of locks. The hash
// of the node's address determines the lock to use. In addition,
// FlagWait is used to tell threads that the node is being created and
// one must wait for the flag to be reset.
struct ClassGraph::LockedSerdeImpl {
  std::pair<Node*, bool> create(SString name) const {
    // Called when we find an existing node. We cannot safely return
    // this node until FlagWait is cleared.
    auto const wait = [&] (Node& n) {
      assertx(n.name->tsame(name));
      while (true) {
        Node::CIAndFlags f{n.ci.load()};
        if (!(f.tag() & FlagWait)) return std::make_pair(&n, false);
        // Still set, wait for it to change and then check again.
        n.ci.wait(f.getOpaque());
      }
    };

    auto& t = table();

    // First access the table with a read lock and see if the node
    // already exists.
    {
      auto const n = [&] {
        std::shared_lock _{t.locking->table};
        return folly::get_ptr(t.nodes, name);
      }();
      // It already exists, wait on FlagWait and then return.
      if (n) return wait(*n);
    }

    // It didn't, we need to create it:
    std::unique_lock _{t.locking->table};
    // We now have exlusive access to the table. Check for the node
    // again, as someone else might have created it in the meantime.
    if (auto const n = folly::get_ptr(t.nodes, name)) {
      // If someone did, wait on FlagWait and then return. Drop the
      // write lock before waiting to avoid deadlock.
      _.unlock();
      return wait(*n);
    }

    // Node still isn't present. Create one now.
    auto [it, emplaced] = t.nodes.try_emplace(name);
    always_assert(emplaced);
    auto& n = it->second;
    assertx(!n.name);
    assertx(!(n.flags() & FlagWait));
    n.name = name;
    assertx(t.nodes.size() < std::numeric_limits<Node::Idx>::max());
    n.idx = t.nodes.size();
    assertx(n.idx > 0);
    // Set FlagWait, this will ensure that any other thread who
    // retrieves this node (after we drop the write lock) will block
    // until we're done deserializing it and it's children.
    n.setFlags(FlagWait);
    return std::make_pair(&n, true);
  }
  Node& get(SString name) const {
    auto& t = table();
    std::shared_lock _{t.locking->table};
    auto n = folly::get_ptr(t.nodes, name);
    always_assert_flog(
      n,
      "Attempting to retrieve missing ClassGraph node '{}'",
      name
    );
    assertx(n->name->tsame(name));
    // FlagWait shouldn't be set here because we shouldn't call get()
    // until a node and all of it's dependents are created.
    assertx(!(n->flags() & FlagWait));
    return *n;
  }
  void setEquiv(Node& n, Node& e) const {
    auto& t = table();
    {
      std::shared_lock _{t.locking->equivs};
      if (auto const DEBUG_ONLY old = folly::get_default(t.regOnlyEquivs, &n)) {
        assertx(old == &e);
        return;
      }
    }
    std::unique_lock _{t.locking->equivs};
    auto const [it, s] = t.regOnlyEquivs.emplace(&n, &e);
    always_assert(s || it->second == &e);
  }
  size_t getCompleteSize(Node& n) const {
    auto& t = table();
    std::shared_lock _{t.locking->sizes};
    auto const size = folly::get_default(t.completeSizeCache, &n);
    always_assert(size > 1);
    return size;
  }
  void setCompleteSize(Node& n, size_t size) const {
    assertx(size > 1);
    auto& t = table();
    std::unique_lock _{t.locking->sizes};
    auto const [it, s] = t.completeSizeCache.emplace(&n, size);
    always_assert(s || it->second == size);
  }
  // Lock a node by using the lock it hashes to and execute f() while
  // holding the lock.
  template <typename F> void lock(Node& n, const F& f) const {
    auto& t = table();
    auto& lock = t.locking->nodes[
      pointer_hash<Node>{}(&n) % t.locking->nodes.size()
    ];
    lock.lock();
    SCOPE_EXIT { lock.unlock(); };
    f();
  }
  template <typename F> void forEachChild(Node& n, const F& f) const {
    CompactVector<Node*> children;
    lock(n, [&] { children = n.children; });
    for (auto const c : children) f(*c);
  }
  // Signal that a node (and all of it's dependents) is done being
  // deserialized. This clears FlagWait and wakes up any threads
  // waiting on the flag. In addition, it sets the node's flags to the
  // provided flags.
  void signal(Node& n, Flags other) const {
    assertx(!(other & FlagWait));
    while (true) {
      auto old = n.ci.load();
      Node::CIAndFlags f{old};
      assertx(f.tag() == FlagWait);
      // Use CAS to set the flag
      f.set(other, f.ptr());
      if (n.ci.compare_exchange_strong(old, f.getOpaque())) break;
    }
    // Wake up any other threads.
    n.ci.notify_all();
  }
  // Set a ClassInfo2 on this node, using CAS to detect concurrent
  // modifications.
  void setCInfo(Node& n, ClassInfo& c) const {
    while (true) {
      auto old = n.ci.load();
      Node::CIAndFlags f{old};
      f.set((Flags)(f.tag() & ~FlagCInfo2), &c);
      if (n.ci.compare_exchange_strong(old, f.getOpaque())) break;
    }
  }
  void setCInfo(Node& n, ClassInfo2& c) const {
    while (true) {
      auto old = n.ci.load();
      Node::CIAndFlags f{old};
      f.set((Flags)(f.tag() | FlagCInfo2), &c);
      if (n.ci.compare_exchange_strong(old, f.getOpaque())) break;
    }
  }
  void updateFlags(Node& n, Flags add, Flags remove = FlagNone) const {
    if (add == FlagNone && remove == FlagNone) return;
    while (true) {
      auto old = n.ci.load();
      Node::CIAndFlags f{old};
      auto const oldFlags = (Flags)f.tag();
      auto const newFlags = (Flags)((oldFlags | add) & ~remove);
      if (newFlags == oldFlags) break;
      f.set(newFlags, f.ptr());
      if (n.ci.compare_exchange_strong(old, f.getOpaque())) break;
    }
  }
};

SString ClassGraph::name() const {
  assertx(this_);
  return this_->name;
}

ClassInfo* ClassGraph::cinfo() const {
  assertx(this_);
  return this_->cinfo();
}

ClassInfo2* ClassGraph::cinfo2() const {
  assertx(this_);
  return this_->cinfo2();
}

bool ClassGraph::mightBeRegular() const {
  assertx(this_);
  return this_->isRegular() || isMissing();
}

bool ClassGraph::mightBeNonRegular() const {
  assertx(this_);
  return !this_->isRegular() || isMissing();
}

bool ClassGraph::mightHaveRegularSubclass() const {
  assertx(this_);
  if (this_->hasCompleteChildren() || this_->isConservative()) {
    return this_->hasRegularSubclass();
  }
  return true;
}

bool ClassGraph::mightHaveNonRegularSubclass() const {
  assertx(this_);
  if (this_->hasCompleteChildren() || this_->isConservative()) {
    return this_->hasNonRegularSubclass();
  }
  return true;
}

bool ClassGraph::isMissing() const {
  assertx(this_);
  return this_->isMissing();
}

bool ClassGraph::hasCompleteChildren() const {
  assertx(this_);
  return !this_->isMissing() && this_->hasCompleteChildren();
}

bool ClassGraph::isConservative() const {
  assertx(this_);
  return !this_->isMissing() && this_->isConservative();
}

bool ClassGraph::isInterface() const {
  assertx(this_);
  assertx(!isMissing());
  return this_->isInterface();
}

bool ClassGraph::isTrait() const {
  assertx(this_);
  assertx(!isMissing());
  return this_->isTrait();
}

bool ClassGraph::isEnum() const {
  assertx(this_);
  assertx(!isMissing());
  return this_->isEnum();
}

bool ClassGraph::isAbstract() const {
  assertx(this_);
  assertx(!isMissing());
  return this_->isAbstract();
}

void ClassGraph::setComplete() {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  assertx(!this_->hasCompleteChildren());
  assertx(!this_->isConservative());
  setCompleteImpl(UnlockedSerdeImpl{}, *this_);
}

void ClassGraph::setClosureBase() {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  assertx(!this_->hasCompleteChildren());
  assertx(!this_->isConservative());
  assertx(is_closure_base(this_->name));
  this_->children.clear();
  this_->setFlags((Flags)(FlagConservative | FlagRegSub));
}

void ClassGraph::setBase(ClassGraph b) {
  assertx(!table().locking);
  assertx(this_);
  assertx(b.this_);
  assertx(!isMissing());
  assertx(!b.isMissing());
  assertx(b.this_->isBase());
  this_->parents.emplace_back(b.this_);
  if (!b.this_->isConservative()) {
    b.this_->children.emplace_back(this_);
  }
}

void ClassGraph::addParent(ClassGraph p) {
  assertx(!table().locking);
  assertx(this_);
  assertx(p.this_);
  assertx(!isMissing());
  assertx(!p.isMissing());
  assertx(!p.this_->isBase());
  this_->parents.emplace_back(p.this_);
  if (!p.this_->isConservative()) {
    p.this_->children.emplace_back(this_);
  }
}

void ClassGraph::flattenTraitInto(ClassGraph t) {
  assertx(!table().locking);
  assertx(this_);
  assertx(t.this_);
  assertx(t.this_->isTrait());
  assertx(!isMissing());
  assertx(!t.isMissing());
  assertx(!t.isConservative());
  assertx(!t.hasCompleteChildren());

  // Remove this trait as a parent, and move all of it's parents into
  // this class.
  always_assert(this_->parents.erase(t.this_));
  always_assert(t.this_->children.erase(this_));
  for (auto const p : t.this_->parents) {
    this_->parents.emplace_back(p);
    p->children.emplace_back(this_);
  }
}

// Indicates that all possible parents has been added to this
// node. Puts the parents list in canonical order.
void ClassGraph::finalizeParents() {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  assertx(!this_->isConservative());
  assertx(!this_->hasCompleteChildren());
  std::sort(begin(this_->parents), end(this_->parents), Node::Compare{});
  this_->parents.erase(
    std::unique(begin(this_->parents), end(this_->parents)),
    end(this_->parents)
  );
}

void ClassGraph::reset() {
  assertx(!table().locking);
  assertx(this_);
  assertx(this_->children.empty());
  assertx(!this_->isMissing());
  assertx(!this_->isConservative());
  assertx(!this_->hasCompleteChildren());
  for (auto const parent : this_->parents) {
    if (parent->isConservative()) continue;
    always_assert(parent->children.erase(this_));
  }
  this_->parents.clear();
  this_->ci.store(Node::CIAndFlags::Opaque{});
  this_ = nullptr;
}

void ClassGraph::setCInfo(ClassInfo& ci) {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  assertx(this_->hasCompleteChildren() || this_->isConservative());
  this_->setCInfo(ci);
}

bool ClassGraph::operator<(ClassGraph h) const {
  return string_data_lt_type{}(this_->name, h.this_->name);
}

ClassGraph ClassGraph::withoutNonRegular() const {
  assertx(!table().locking);
  assertx(this_);
  ensure();
  if (this_->isMissing() || this_->isRegular()) return *this;
  return ClassGraph { this_->nonRegularInfo().regOnlyEquiv };
}

ClassGraph ClassGraph::base() const {
  assertx(!table().locking);
  assertx(this_);
  ensure();
  if (!this_->isMissing()) {
    for (auto const p : this_->parents) {
      if (p->isBase()) return ClassGraph { p };
    }
  }
  return ClassGraph { nullptr };
}

ClassGraph ClassGraph::topBase() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();

  auto current = this_;
  auto last = this_;
  do {
    last = this_;
    auto const& parents = current->parents;
    current = nullptr;
    // There should be at most one base on the parent list. Verify
    // this in debug builds.
    for (auto const p : parents) {
      if (p->isBase()) {
        assertx(!current);
        current = p;
        if (!debug) break;
      }
    }
  } while (current);

  return ClassGraph { last };
}

std::vector<ClassGraph> ClassGraph::bases() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();

  std::vector<ClassGraph> out;
  auto current = this_;
  do {
    out.emplace_back(ClassGraph{ current });
    auto const& parents = current->parents;
    current = nullptr;
    // There should be at most one base on the parent list. Verify
    // this in debug builds.
    for (auto const p : parents) {
      if (p->isBase()) {
        assertx(!current);
        current = p;
        if (!debug) break;
      }
    }
  } while (current);

  std::reverse(begin(out), end(out));
  return out;
}

std::vector<ClassGraph> ClassGraph::interfaces() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();
  std::vector<ClassGraph> out;
  forEachParent(
    *this_,
    [&] (Node& p) {
      if (p.isInterface()) out.emplace_back(ClassGraph{ &p });
      return Action::Continue;
    }
  );
  std::sort(begin(out), end(out));
  return out;
}

template <typename F>
void ClassGraph::walkParents(const F& f) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();
  forEachParent(
    *this_,
    [&] (Node& p) {
      return !f(ClassGraph{ &p })
        ? Action::Skip
        : Action::Continue;
    }
  );
}

std::vector<ClassGraph> ClassGraph::children(bool regOnly) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());
  assertx(hasCompleteChildren());

  ensureWithChildren(regOnly);

  std::vector<ClassGraph> out;
  // If this_ is a trait, then forEachChild won't walk the list. Use
  // forEachChildImpl with the right params to prevent this.
  TLNodeIdxSet visited;
  forEachChildImpl(
    *this_,
    [&] (Node& c) {
      if (!regOnly || c.isRegular()) {
        out.emplace_back(ClassGraph{ &c });
      }
      return Action::Continue;
    },
    &*visited,
    false
  );
  std::sort(begin(out), end(out));
  return out;
}

std::vector<ClassGraph> ClassGraph::directParents(Flags flags) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();
  std::vector<ClassGraph> out;
  out.reserve(this_->parents.size());
  for (auto const p : this_->parents) {
    if (!(p->flags() & flags)) continue;
    out.emplace_back(ClassGraph { p });
  }
  return out;
}

std::vector<ClassGraph> ClassGraph::directParents() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!isMissing());

  ensure();
  std::vector<ClassGraph> out;
  out.reserve(this_->parents.size());
  for (auto const p : this_->parents) out.emplace_back(ClassGraph { p });
  return out;
}

bool ClassGraph::isChildOf(ClassGraph o) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);
  assertx(!isMissing());
  assertx(!o.isMissing());

  ensure();
  if (this_ == o.this_) return true;
  // Nothing is a child of a trait except itself and we know they're
  // not equal.
  if (o.this_->isTrait()) return false;
  return findParent(*this_, *o.this_);
}

std::vector<ClassGraph> ClassGraph::commonParentsOfRegSubs() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  assertx(!this_->isRegular());
  assertx(hasCompleteChildren());

  auto const& nodes = this_->commonParentsOfRegSubs.get(
    [&] { return calcSubclassOf(*this_, true); }
  );

  using namespace folly::gen;
  return from(nodes)
    | map([] (Node* n) { return ClassGraph{ n }; })
    | as<std::vector>();
}

std::vector<ClassGraph> ClassGraph::candidateRegOnlyEquivs() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());

  if (this_->isRegular() || this_->isConservative()) return {};
  assertx(this_->hasCompleteChildren());
  auto const nonParents = calcSubclassOfSplit(*this_, false).first;
  if (nonParents.empty()) return {};
  if (nonParents.size() == 1) return { ClassGraph { *nonParents.begin() } };

  auto heads = nonParents;

  // Remove any nodes which are reachable from another node. Such
  // nodes are redundant.
  {
    TLNodeIdxSet visited;
    for (auto const n : nonParents) {
      if (!heads.contains(n)) continue;
      forEachParent(
        *n,
        [&] (Node& p) {
          if (&p == n) return Action::Continue;
          if (!nonParents.contains(&p)) return Action::Continue;
          if (!heads.contains(&p)) return Action::Skip;
          heads.erase(&p);
          return Action::Continue;
        },
        *visited
      );
      visited->erase(*n);
    }
  }

  // Remove any nodes which have a (regular) child which does not have
  // this_ a parent. Such a node can't be an equivalent because it
  // contains more regular nodes than this_. Note that we might not
  // have complete children information for all nodes, so this check
  // is conservative. We only remove nodes which are definitely not
  // candidates.
  folly::erase_if(
    heads,
    [&] (Node* n) {
      auto const action = forEachChild(
        *n,
        [&] (Node& c) {
          if (!c.isRegular()) return Action::Continue;
          if (!findParent(c, *this_)) return Action::Stop;
          return Action::Skip;
        }
      );
      return action == Action::Stop;
    }
  );

  std::vector<ClassGraph> out;
  out.reserve(heads.size());
  for (auto const n : heads) out.emplace_back(ClassGraph { n });
  std::sort(begin(out), end(out));
  return out;
}

void ClassGraph::setRegOnlyEquivs() const {
  assertx(!table().locking);
  assertx(this_);
  assertx(!this_->isMissing());
  if (this_->isRegular() || this_->isConservative()) return;
  assertx(this_->hasCompleteChildren());
  auto const equiv = calcRegOnlyEquiv(*this_, calcSubclassOf(*this_, false));
  if (!equiv || equiv == this_ || findParent(*equiv, *this_)) return;
  auto const [it, s] = table().regOnlyEquivs.emplace(this_, equiv);
  always_assert(s || it->second == equiv);
}

const ClassGraph::Node::NonRegularInfo&
ClassGraph::Node::nonRegularInfo() {
  assertx(!table().locking);
  assertx(!isMissing());
  assertx(!isRegular());
  ClassGraph{this}.ensureWithChildren(true);
  return nonRegInfo.get(
    [this] {
      auto info = std::make_unique<NonRegularInfo>();
      info->subclassOf = calcSubclassOf(*this, false);
      info->regOnlyEquiv = calcRegOnlyEquiv(*this, info->subclassOf);
      return info.release();
    }
  );
}

bool ClassGraph::exactSubtypeOfExact(ClassGraph o,
                                     bool nonRegL,
                                     bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();
  o.ensure();
  auto const missing1 = this_->isMissing();
  auto const missing2 = o.this_->isMissing();

  // Two exact classes are only subtypes of another if they're the
  // same. One additional complication is if the class isn't regular
  // and we're not considering non-regular classes. In that case, the
  // class is actually Bottom, and we need to apply the rules of
  // subtyping to Bottom (Bottom is a subtype of everything, but
  // nothing is a subtype of it).
  if (missing1) {
    // Missing classes are only definitely a subtype if it's the same
    // node and the lhs can become bottom or the rhs cannot.
    return (this_ == o.this_) && (!nonRegL || nonRegR);
  } else if (!nonRegL && !this_->isRegular()) {
    // Lhs is a bottom, so a subtype of everything.
    return true;
  } else if (missing2 || (!nonRegR && !o.this_->isRegular())) {
    // Lhs is not a bottom, check if the rhs is.
    return false;
  } else {
    // Neither is bottom, check for equality.
    return this_ == o.this_;
  }
}

bool ClassGraph::exactSubtypeOf(ClassGraph o,
                                bool nonRegL,
                                bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();
  o.ensure();
  auto const missing1 = this_->isMissing();
  auto const missing2 = o.this_->isMissing();

  // If we want to exclude non-regular classes on either side, and the
  // lhs is not regular, there's no subtype relation. If nonRegL is
  // false, then lhs is just a bottom (and bottom is a subtype of
  // everything), and if nonRegularR is false, then the rhs does not
  // contain any non-regular classes, so lhs is guaranteed to not be
  // part of it.
  if (missing1) {
    // If the lhs side is missing, it's identical to
    // exactSubtypeOfExact.
    return (this_ == o.this_) && (!nonRegL || nonRegR);
  } else if ((!nonRegL || !nonRegR) && !this_->isRegular()) {
    return !nonRegL;
  } else if (this_ == o.this_) {
    return true;
  } else if (missing2 || o.this_->isTrait()) {
    // The lhs is not missing, so cannot be a subtype of a missing
    // class. Nothing is a subtype of a trait.
    return false;
  } else {
    // Otherwise try to find the rhs node among the parents of the lhs
    // side.
    return findParent(*this_, *o.this_);
  }
}

bool ClassGraph::subSubtypeOf(ClassGraph o, bool nonRegL, bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();
  o.ensure();
  auto const missing1 = this_->isMissing();
  auto const missing2 = o.this_->isMissing();

  if (nonRegL && !nonRegR) {
    if (missing1 || !this_->isRegular()) return false;
    if (this_->hasCompleteChildren() && this_->hasNonRegularSubclass()) {
      return false;
    }
  }

  // If this_ must be part of the lhs, it's equivalent to
  // exactSubtypeOf. Otherwise if exactSubtypeOf returns true for the
  // conservative case, then it must always be true, so we don't need
  // to look at children.
  if (nonRegL || missing1 || this_->isRegular()) {
    return exactSubtypeOf(o, nonRegL, nonRegR);
  }
  if (exactSubtypeOf(o, true, true)) return true;

  // this_ is not regular and will not be part of the lhs. We need to
  // look at the regular children of this_ and check whether they're
  // all subtypes of the rhs.

  // Traits have no children for the purposes of this test.
  if (this_->isTrait() || missing2 || o.this_->isTrait()) {
    return false;
  }
  return this_->nonRegularInfo().subclassOf.contains(o.this_);
}

bool ClassGraph::exactCouldBeExact(ClassGraph o,
                                   bool nonRegL,
                                   bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();

  // Two exact classes can only be each other if they're the same
  // class. The only complication is if the class isn't regular and
  // we're not considering non-regular classes. In that case, the
  // class is actually Bottom, a Bottom can never could-be anything
  // (not even itself).
  if (this_ != o.this_) return false;
  if (this_->isMissing() || this_->isRegular()) return true;
  return nonRegL && nonRegR;
}

bool ClassGraph::exactCouldBe(ClassGraph o, bool nonRegL, bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();
  o.ensure();
  auto const missing1 = this_->isMissing();
  auto const missing2 = o.this_->isMissing();

  // exactCouldBe is almost identical to exactSubtypeOf, except the
  // case of the lhs being bottom is treated differently (bottom in
  // exactSubtypeOf implies true, but here it implies false).
  if (missing1) {
    if (missing2) return true;
    if ((!nonRegL || !nonRegR) &&
        !o.this_->isRegular() &&
        !o.this_->hasRegularSubclass()) {
      return false;
    }
    return !o.this_->hasCompleteChildren();
  } else if ((!nonRegL || !nonRegR) && !this_->isRegular()) {
    return false;
  } else if (this_ == o.this_) {
    return true;
  } else if (missing2 || o.this_->isTrait()) {
    return false;
  } else {
    return findParent(*this_, *o.this_);
  }
}

bool ClassGraph::subCouldBe(ClassGraph o, bool nonRegL, bool nonRegR) const {
  assertx(!table().locking);
  assertx(this_);
  assertx(o.this_);

  ensure();
  o.ensure();
  auto const missing1 = this_->isMissing();
  auto const missing2 = o.this_->isMissing();

  auto const regOnly = !nonRegL || !nonRegR;

  if (exactCouldBe(o, nonRegL, nonRegR) ||
      o.exactCouldBe(*this, nonRegR, nonRegL)) {
    return true;
  } else if (missing1 && missing2) {
    return true;
  } else if ((!missing1 && this_->isTrait()) ||
             (!missing2 && o.this_->isTrait())) {
    return false;
  } else if (regOnly) {
    if (!missing1 && !this_->isRegular() && !mightHaveRegularSubclass()) {
      return false;
    }
    if (!missing2 && !o.this_->isRegular() && !o.mightHaveRegularSubclass()) {
      return false;
    }
  }

  auto left = this_;
  auto right = o.this_;
  if (betterNode(right, left)) std::swap(left, right);
  ClassGraph lg{left};
  ClassGraph rg{right};

  lg.ensureWithChildren(regOnly);
  if (!lg.hasCompleteChildren()) return true;

  TLNodeIdxSet visited;
  auto const action = forEachChild(
    *left,
    [&] (Node& c) {
      if (regOnly && !c.isRegular()) return Action::Continue;
      return findParent(c, *right, *visited)
        ? Action::Stop : Action::Continue;
    }
  );
  return action == Action::Stop;
}

// Calculate all the classes which this class could be a subclass
// of. This is only necessary if the class is non-regular (for regular
// classes the subtype check is trivial). This not only speeds up
// subtypeOf checks, but it is used when calculating reg-only
// equivalent classes. The results are split into the nodes which
// aren't parents of this class, and the ones which are not.
std::pair<ClassGraph::NodeSet, ClassGraph::NodeSet>
ClassGraph::calcSubclassOfSplit(Node& n, bool ignoreTrait) {
  assertx(!table().locking);
  assertx(!n.isMissing());
  assertx(!n.isRegular());

  // Traits cannot be a subclass of anything but itself, and if we
  // don't know all of the children, we have to be pessimistic and
  // report only the parents.
  if (!ignoreTrait && n.isTrait()) return std::make_pair(NodeSet{}, NodeSet{});
  if (!n.hasCompleteChildren()) {
    return std::make_pair(NodeSet{}, allParents(n));
  }

  // Find the first regular child of this non-regular class.
  Node* first = nullptr;
  forEachChild(
    n,
    [&] (Node& c) {
      if (!c.isRegular()) return Action::Continue;
      assertx(&c != &n);
      // n has complete children, so all children must as well.
      assertx(c.hasCompleteChildren());
      first = &c;
      return Action::Stop;
    },
    !ignoreTrait
  );
  // No regular child
  if (!first) return std::make_pair(NodeSet{}, NodeSet{});

  auto parents = allParents(n);

  // Given the regular class we found, gather all of it's
  // children. This is the initial set of candidates.
  NodeVec candidates;
  forEachParent(
    *first,
    [&] (Node& p) {
      // Ignore parents since we already know about this.
      if (parents.contains(&p)) return Action::Skip;
      candidates.emplace_back(&p);
      return Action::Continue;
    }
  );
  if (candidates.empty()) return std::make_pair(NodeSet{}, std::move(parents));

  // Then use ParentTracker to remove any nodes which are not parents
  // of the other children.
  auto const run = [&] (auto tracker) {
    forEachChild(
      n,
      [&] (Node& c) {
        if (!c.isRegular()) return Action::Continue;
        return tracker(c) ? Action::Skip : Action::Stop;
      },
      !ignoreTrait
    );
    auto nodes = tracker.nodes();
    return std::make_pair(std::move(nodes), std::move(parents));
  };

  if (candidates.size() <= SmallBitset::kMaxSize) {
    return run(ParentTracker<SmallBitset>{candidates, &parents});
  } else {
    return run(ParentTracker<LargeBitset>{candidates, &parents});
  }
}

ClassGraph::NodeSet ClassGraph::calcSubclassOf(Node& n, bool ignoreTrait) {
  assertx(!table().locking);
  assertx(!n.isMissing());
  assertx(!n.isRegular());
  auto [nonParents, parents] = calcSubclassOfSplit(n, ignoreTrait);
  nonParents.insert(begin(parents), end(parents));
  return nonParents;
}

// Calculate the "best" Node which is equivalent to this Node when
// considering only regular children. We canonicalize this Node to the
// equivalent Node where appropriate. Nullptr is returned if the
// "equivalent" Node is actually Bottom (because there are no regular
// children). subclassOf gives the set of "candidate" Nodes.
ClassGraph::Node* ClassGraph::calcRegOnlyEquiv(Node& base,
                                               const NodeSet& subclassOf) {
  assertx(!table().locking);
  assertx(!base.isMissing());
  assertx(!base.isRegular());

  if (subclassOf.empty()) return nullptr;
  if (base.isTrait()) return nullptr;
  if (base.hasCompleteChildren() || base.isConservative()) {
    if (!base.hasRegularSubclass()) return nullptr;
  }
  if (subclassOf.size() == 1) {
    assertx(subclassOf.contains(&base));
    return &base;
  }

  // If we recorded an equivalent when deserializing, just use that.
  if (auto const e = folly::get_default(table().regOnlyEquivs, &base)) {
    assertx(e->hasCompleteChildren());
    return e;
  }
  // Otherwise calculate it.

  auto heads = subclassOf;

  // Remove any nodes which are reachable from another node. Such
  // nodes are redundant.
  {
    TLNodeIdxSet visited;
    for (auto const n : subclassOf) {
      if (!heads.contains(n)) continue;
      forEachParent(
        *n,
        [&] (Node& p) {
          if (&p == n) return Action::Continue;
          if (!subclassOf.contains(&p)) return Action::Continue;
          if (!heads.contains(&p)) return Action::Skip;
          heads.erase(&p);
          return Action::Continue;
        },
        *visited
      );
      visited->erase(*n);
    }
  }
  if (heads.size() == 1) return *heads.begin();

  assertx(base.hasCompleteChildren());

  // Find the best node among the remaining candidates.
  auto const run = [&] (auto tracker) {
    // A node is only a valid candidate (and thus sufficient) if all
    // of it's children are subclasses of the "base" Node.
    auto const isSufficient = [&] (Node* n) {
      if (n == &base) return true;
      if (findParent(*n, base)) return true;
      if (!n->hasCompleteChildren()) return false;
      auto const action = forEachChild(
        *n,
        [&] (Node& c) {
          if (!c.isRegular()) return Action::Continue;
          return tracker.all(c) ? Action::Skip : Action::Stop;
        }
      );
      return action != Action::Stop;
    };

    Node* best = nullptr;
    for (auto const n : heads) {
      if (!isSufficient(n)) continue;
      if (!best || betterNode(n, best)) best = n;
    }
    assertx(best);
    return best;
  };

  if (heads.size() <= SmallBitset::kMaxSize) {
    return run(ParentTracker<SmallBitset>{heads});
  } else {
    return run(ParentTracker<LargeBitset>{heads});
  }
}

// Somewhat arbitrarily ranks two Nodes in a consistent manner.
bool ClassGraph::betterNode(Node* n1, Node* n2) {
  if (n1 == n2) return false;

  // Non-missing nodes are always better. Two missing nodes are ranked
  // according to name.
  auto const missing1 = n1->isMissing();
  auto const missing2 = n2->isMissing();
  if (missing1) {
    if (!missing2) return false;
    return string_data_lt_type{}(n1->name, n2->name);
  } else if (missing2) {
    return true;
  }

  auto const complete1 = n1->hasCompleteChildren();
  auto const complete2 = n2->hasCompleteChildren();

  // Nodes with complete children are better than those that don't.
  if (!complete1) {
    if (complete2) return false;
    return string_data_lt_type{}(n1->name, n2->name);
  } else if (!complete2) {
    return true;
  }

  // Rank them acccording to what kind of class it is.
  auto const weight = [] (const Node* n) {
    auto const f = n->flags();
    if (f & FlagAbstract)  return 1;
    if (f & FlagInterface) return 2;
    if (f & FlagTrait)     return 3;
    if (f & FlagEnum)      return 4;
    return 0;
  };
  auto const w1 = weight(n1);
  auto const w2 = weight(n2);
  if (w1 != w2) return w1 < w2;

  // Finally, choose the one with the least (immediate)
  // children. Calculating the full subclass list would be better, but
  // more expensive. Traits are always considered to have no children.
  auto const s1 = n1->isTrait() ? 0 : n1->children.size();
  auto const s2 = n2->isTrait() ? 0 : n2->children.size();
  if (s1 != s2) return s1 < s2;

  // At last resort, order by name.
  return string_data_lt_type{}(n1->name, n2->name);
}

// Union together two sets of intersected classes.
ClassGraph::NodeVec ClassGraph::combine(const NodeVec& lhs,
                                        const NodeVec& rhs,
                                        bool isSubL,
                                        bool isSubR,
                                        bool nonRegL,
                                        bool nonRegR) {
  assertx(!table().locking);
  assertx(!lhs.empty());
  assertx(!rhs.empty());
  assertx(IMPLIES(!isSubL, lhs.size() == 1));
  assertx(IMPLIES(!isSubR, rhs.size() == 1));

  // Combine two sets, keeping only the nodes that are in both
  // sets. An empty set is equivalent to Top (not Bottom), thus
  // everything in the other set goes in.
  NodeSet combined;
  auto const combine = [&] (const NodeSet& s1, const NodeSet& s2) {
    if (s1.empty()) {
      always_assert(!s2.empty());
      combined.insert(begin(s2), end(s2));
    } else if (s2.empty()) {
      combined.insert(begin(s1), end(s1));
    } else if (s1.size() < s2.size()) {
      for (auto const e : s1) {
        if (s2.contains(e)) combined.emplace(e);
      }
    } else {
      for (auto const e : s2) {
        if (s1.contains(e)) combined.emplace(e);
      }
    }
  };

  /*
   * (A&B) | (C&D) = (A|C) & (A|D) & (B|C) & (B|D)
   *
   * (this generalizes to arbitrary size lists). So to union together
   * two intersection sets, we need to calculate the union of
   * individual classes pair-wise, then intersect them together.
   */
  auto const process = [&] (Node* l, Node* r) {
    ClassGraph{l}.ensure();
    ClassGraph{r}.ensure();

    // Order the l and r to cut down on the number of cases to deal
    // with below.
    auto const flip = [&] {
      if (l->isMissing()) return false;
      if (r->isMissing()) return true;
      if (nonRegL || l->isRegular()) return false;
      if (nonRegR || r->isRegular()) return true;
      if (isSubL) return false;
      return isSubR;
    }();
    if (flip) std::swap(l, r);

    auto const flipSubL = flip ? isSubR : isSubL;
    auto const flipSubR = flip ? isSubL : isSubR;
    auto const flipNonRegL = flip ? nonRegR : nonRegL;
    auto const flipNonRegR = flip ? nonRegL : nonRegR;

    // This logic handles the unioning of two classes. If two classes
    // don't have a common parent, their union is Top, which is
    // dropped from the intersection list. For regular classes, we can
    // just get the parent list. For non-regular classes, we need to
    // use subclassOf.
    if (l->isMissing()) {
      if (l == r) combined.emplace(l);
    } else if (flipNonRegL || l->isRegular()) {
      if (flipNonRegR || r->isRegular()) {
        combine(allParents(*l), allParents(*r));
      } else if (flipSubR) {
        combine(allParents(*l), r->nonRegularInfo().subclassOf);
      } else {
        combine(allParents(*l), {});
      }
    } else if (flipSubL) {
      if (flipSubR) {
        combine(l->nonRegularInfo().subclassOf,
                r->nonRegularInfo().subclassOf);
      } else {
        combine(l->nonRegularInfo().subclassOf, {});
      }
    } else {
      always_assert(false);
    }
  };

  for (auto const l : lhs) {
    for (auto const r : rhs) process(l, r);
  }

  // The resultant list is not in canonical form, so canonicalize it.
  return canonicalize(combined, nonRegL || nonRegR);
}

// Intersect two sets of intersected classes together. This seems like
// you would just combine the lists, but the intersection of the two
// might have more specific classes in common.
ClassGraph::NodeVec ClassGraph::intersect(const NodeVec& lhs,
                                          const NodeVec& rhs,
                                          bool nonRegL,
                                          bool nonRegR,
                                          bool& nonRegOut) {
  assertx(!table().locking);
  assertx(!lhs.empty());
  assertx(!rhs.empty());

  // Combine the two lists together.
  NodeVec combined;
  combined.reserve(lhs.size() + rhs.size());
  std::set_union(
    lhs.begin(), lhs.end(),
    rhs.begin(), rhs.end(),
    std::back_inserter(combined),
    betterNode
  );

  // Build the "heads". These are the nodes which could potentially be
  // part of the new intersection list, but don't need to be
  // calculated below. We leave them out to reduce the amount of work.
  NodeSet heads;
  for (auto const n : combined) {
    ClassGraph g{ n };
    g.ensure();
    if (n->isMissing()) {
      heads.emplace(n);
    } else if ((nonRegL && nonRegR) || n->isRegular()) {
      auto const p = allParents(*n);
      heads.insert(p.begin(), p.end());
    } else if (!g.mightHaveRegularSubclass()) {
      nonRegOut = false;
      return {};
    } else {
      auto const& s = n->nonRegularInfo().subclassOf;
      heads.insert(s.begin(), s.end());
    }
  }

  // Then enumerate over all of the classes in the combined set and
  // track which parents they *all* have in common.

  Optional<ParentTracker<SmallBitset>> small;
  Optional<ParentTracker<LargeBitset>> large;
  nonRegOut = false;

  enumerateIsectMembers(
    combined,
    nonRegL && nonRegR,
    [&] (Node& n) {
      ClassGraph g{ &n };
      if (n.isMissing() || !n.isRegular() ||
          !(n.hasCompleteChildren() || n.isConservative())) {
        if (nonRegL && nonRegR) nonRegOut = true;
      } else if (!nonRegOut && nonRegL && nonRegR) {
        if (n.hasNonRegularSubclass()) nonRegOut = true;
      }

      // If we already created a tracker, use it.
      if (small) {
        return (*small)(n);
      } else if (large) {
        return (*large)(n);
      } else {
        // Otherwise this is the first time. Find the initial set of
        // candidates (all of the parents of this node minus the
        // heads) and set up the tracker.
        auto common = n.isMissing()
          ? NodeSet{&n}
          : allParents(n);
        folly::erase_if(common, [&] (Node* c) { return heads.contains(c); });
        if (common.size() <= SmallBitset::kMaxSize) {
          small.emplace(common, &heads);
        } else {
          large.emplace(common, &heads);
        }
        return !common.empty();
      }
    }
  );
  assertx(IMPLIES(!nonRegL || !nonRegR, !nonRegOut));

  // At this point the tracker only contains all of the parents which
  // are in common. These nodes, plus the heads, only need to be
  // canonicalized.
  if (small) {
    auto s = small->nodes();
    s.insert(heads.begin(), heads.end());
    return canonicalize(s, nonRegOut);
  } else if (large) {
    auto s = large->nodes();
    s.insert(heads.begin(), heads.end());
    return canonicalize(s, nonRegOut);
  } else {
    return NodeVec{};
  }
}

// Given a set of intersected nodes, return an equivalent set with all
// of the non-regular classes removed.
ClassGraph::NodeVec ClassGraph::removeNonReg(const NodeVec& v) {
  assertx(!table().locking);
  assertx(!v.empty());

  // We can just treat this as an intersection:
  NodeVec lhs;
  NodeVec rhs;
  for (auto const n : v) {
    ClassGraph{n}.ensure();
    if (n->isMissing() || n->isRegular()) {
      lhs.emplace_back(n);
    } else if (auto const e = n->nonRegularInfo().regOnlyEquiv) {
      lhs.emplace_back(e);
    } else {
      return NodeVec{};
    }
  }
  if (lhs.size() <= 1) return lhs;

  std::sort(lhs.begin(), lhs.end(), betterNode);
  rhs.emplace_back(lhs.back());
  lhs.pop_back();

  auto nonRegOut = false;
  SCOPE_EXIT { assertx(!nonRegOut); };
  return intersect(lhs, rhs, false, false, nonRegOut);
}

// Given two lists of intersected classes, return true if the
// intersection of the two lists might be non-empty.
bool ClassGraph::couldBeIsect(const NodeVec& lhs,
                              const NodeVec& rhs,
                              bool nonRegL,
                              bool nonRegR) {
  assertx(!table().locking);
  assertx(!lhs.empty());
  assertx(!rhs.empty());

  NodeVec combined;
  combined.reserve(lhs.size() + rhs.size());
  std::set_union(
    lhs.begin(), lhs.end(),
    rhs.begin(), rhs.end(),
    std::back_inserter(combined),
    betterNode
  );

  auto couldBe = false;
  enumerateIsectMembers(
    combined,
    nonRegL && nonRegR,
    [&] (Node&) {
      couldBe = true;
      return false;
    }
  );
  return couldBe;
}

// Call the provided callback for each Node in the given intersection
// list. If "all" is true, all Nodes are provided. Otherwise, only the
// "top most" children are provided (the children which meet the
// criteria and don't have any parents which do).
template <typename F>
void ClassGraph::enumerateIsectMembers(const NodeVec& nodes,
                                       bool nonReg,
                                       const F& f,
                                       bool all) {
  assertx(!table().locking);

  if (nodes.empty()) return;

  if (table().index) {
    for (auto const n : nodes) ClassGraph{n}.ensure();
  }

  // Find the "best" node to start with.
  auto head = *std::min_element(nodes.begin(), nodes.end(), betterNode);
  ClassGraph{head}.ensureWithChildren(!nonReg);
  if (head->isMissing() || !head->hasCompleteChildren()) {
    // If the best node is missing or doesn't have complete children,
    // they all don't, so just supply the input list.
    for (auto const n : nodes) {
      assertx(n->isMissing() || !n->hasCompleteChildren());
      if (!f(*n)) break;
    }
    return;
  }

  // Otherwise report Nodes which are children of all of the Nodes.
  auto const run = [&] (auto tracker) {
    forEachChild(
      *head,
      [&] (Node& c) {
        if (nonReg || c.isRegular()) {
          if (tracker.all(c)) {
            if (!f(c)) return Action::Stop;
            return all ? Action::Continue : Action::Skip;
          }
        }
        return Action::Continue;
      }
    );
  };

  if (nodes.size() <= SmallBitset::kMaxSize) {
    return run(ParentTracker<SmallBitset>{nodes});
  } else {
    return run(ParentTracker<LargeBitset>{nodes});
  }
}

// "Canonicalize" a set of intersected classes by removing redundant
// Nodes and putting them in a deterministic order.
ClassGraph::NodeVec ClassGraph::canonicalize(const NodeSet& nodes,
                                             bool nonReg) {
  NodeVec out;
  if (nodes.size() <= 1) {
    // Trivial cases
    out.insert(begin(nodes), end(nodes));
    return out;
  }

  // Remove redundant nodes. A node is redundant if it is reachable
  // via another node. In that case, this node is implied by the other
  // and can be removed.
  auto heads = nodes;
  {
    TLNodeIdxSet visited;
    for (auto const n : nodes) {
      if (!heads.contains(n)) continue;
      ClassGraph{n}.ensure();
      if (n->isMissing()) continue;
      forEachParent(
        *n,
        [&] (Node& p) {
          if (&p == n) return Action::Continue;
          if (!nodes.contains(&p)) return Action::Continue;
          if (!heads.contains(&p)) return Action::Skip;
          heads.erase(&p);
          return Action::Continue;
        },
        *visited
      );
      visited->erase(*n);
    }
  }

  // A node can be redundant with another even they aren't reachable
  // via each other. This can only happen if we're considering only
  // regular nodes. If a class is a super type of another, it's
  // redundant, as the other class implies this class. If the classes
  // are both super types of each other, they're equivalent, and we
  // keep the "best" one.
  if (!nonReg) {
    for (auto const n1 : heads) {
      auto const isSuperType = [&] {
        for (auto const n2 : heads) {
          if (n1 == n2) continue;
          if (!ClassGraph{n2}.subSubtypeOf(ClassGraph{n1}, false, false)) {
            continue;
          }
          if (!ClassGraph{n1}.subSubtypeOf(ClassGraph{n2}, false, false)) {
            return true;
          }
          if (betterNode(n2, n1)) return true;
        }
        return false;
      }();
      if (!isSuperType) out.emplace_back(n1);
    }
  } else {
    out.insert(begin(heads), end(heads));
  }

  // Finally sort them according to how good they are.
  std::sort(out.begin(), out.end(), betterNode);
  return out;
}

template <typename F>
ClassGraph::Action ClassGraph::forEachParent(Node& n, const F& f, NodeIdxSet& v) {
  return forEachParentImpl(n, f, &v, true);
}

template <typename F>
ClassGraph::Action ClassGraph::forEachParent(Node& n, const F& f) {
  TLNodeIdxSet v;
  return forEachParentImpl(n, f, &*v, true);
}

template <typename F>
ClassGraph::Action ClassGraph::forEachParentImpl(Node& n,
                                                 const F& f,
                                                 NodeIdxSet* v,
                                                 bool start) {
  assertx(!n.isMissing());
  if (v && !v->add(n)) return Action::Skip;
  if (start || !n.isTrait()) {
    auto const action = f(n);
    if (action != Action::Continue) return action;
  }
  for (auto const parent : n.parents) {
    if (forEachParentImpl(*parent, f, v, false) == Action::Stop) {
      return Action::Stop;
    }
  }
  return Action::Continue;
}

template <typename F, typename F2, typename T>
T ClassGraph::foldParentsImpl(Node& n,
                              const F& f,
                              const F2& f2,
                              NodeMap<T>& m,
                              bool start) {
  assertx(!n.isMissing());
  if (auto const t = folly::get_ptr(m, &n)) return *t;
  T t = (start || !n.isTrait()) ? f(n) : f2();
  for (auto const parent : n.parents) {
    if (t) break;
    t |= foldParentsImpl(*parent, f, f2, m, false);
  }
  m.insert_or_assign(&n, t);
  return t;
}

bool ClassGraph::findParent(Node& n1, Node& n2) {
  TLNodeIdxSet visited;
  return findParent(n1, n2, *visited);
}

bool ClassGraph::findParent(Node& start, Node& target, NodeIdxSet& visited) {
  assertx(!start.isMissing());

  static thread_local hphp_fast_map<
    std::pair<Node*, Node*>,
    bool,
    pointer_pair_hash<Node>
  > cache;
  if (auto const r = folly::get_ptr(cache, std::make_pair(&start, &target))) {
    return *r;
  }

  auto const action = forEachParent(
    start,
    [&] (Node& p) { return (&p == &target) ? Action::Stop : Action::Continue; },
    visited
  );

  cache.try_emplace(
    std::make_pair(&start, &target),
    action == Action::Stop
  );
  return action == Action::Stop;
}

ClassGraph::NodeSet ClassGraph::allParents(Node& n) {
  assertx(!n.isMissing());
  NodeSet s;
  forEachParent(
    n,
    [&] (Node& p) {
      s.emplace(&p);
      return Action::Continue;
    }
  );
  return s;
}

template <typename F>
ClassGraph::Action ClassGraph::forEachChild(Node& n, const F& f,
                                            NodeIdxSet& v, bool start) {
  return forEachChildImpl(n, f, &v, start);
}

template <typename F>
ClassGraph::Action ClassGraph::forEachChild(Node& n, const F& f, bool start) {
  TLNodeIdxSet v;
  return forEachChildImpl(n, f, &*v, start);
}

template <typename F>
ClassGraph::Action
ClassGraph::forEachChildImpl(Node& n, const F& f, NodeIdxSet* v, bool start) {
  assertx(!n.isMissing());
  if (v && !v->add(n)) return Action::Skip;
  auto const action = f(n);
  if (action != Action::Continue) return action;
  if (start && n.isTrait()) return action;
  for (auto const child : n.children) {
    if (forEachChildImpl(*child, f, v, false) == Action::Stop) {
      return Action::Stop;
    }
  }
  return Action::Continue;
}

ClassGraph ClassGraph::create(const php::Class& cls) {
  assertx(!table().locking);

  auto const& [n, emplaced] = table().nodes.try_emplace(cls.name);
  always_assert_flog(
    emplaced,
    "Attempting to create already existing ClassGraph node '{}'",
    cls.name
  );
  assertx(!n->second.name);
  assertx(n->second.flags() == FlagNone);
  assertx(!n->second.cinfo());
  assertx(!n->second.cinfo2());
  n->second.name = cls.name;
  n->second.idx = table().nodes.size();

  auto f = FlagNone;
  if (cls.attrs & AttrInterface)              f = Flags(f | FlagInterface);
  if (cls.attrs & AttrTrait)                  f = Flags(f | FlagTrait);
  if (cls.attrs & AttrAbstract)               f = Flags(f | FlagAbstract);
  if (cls.attrs & (AttrEnum | AttrEnumClass)) f = Flags(f | FlagEnum);
  n->second.setFlags(f);

  return ClassGraph{ &n->second };
}

ClassGraph ClassGraph::get(SString name) {
  assertx(!table().locking);
  auto n = folly::get_ptr(table().nodes, name);
  always_assert_flog(
    n,
    "Attempting to retrieve missing ClassGraph node '{}'",
    name
  );
  assertx(n->name->tsame(name));
  return ClassGraph{ n };
}

ClassGraph ClassGraph::getOrCreate(SString name) {
  assertx(!table().locking);

  auto const& [n, emplaced] = table().nodes.try_emplace(name);
  if (emplaced) {
    assertx(!n->second.name);
    assertx(n->second.flags() == FlagNone);
    assertx(!n->second.cinfo());
    assertx(!n->second.cinfo2());
    n->second.name = name;
    n->second.idx = table().nodes.size();
    n->second.setFlags(FlagMissing);
  } else {
    assertx(n->second.name->tsame(name));
  }
  return ClassGraph{ &n->second };
}

ClassGraph ClassGraph::getMissing(SString name) {
  assertx(!table().locking);

  auto const& [n, emplaced] = table().nodes.try_emplace(name);
  if (emplaced) {
    assertx(!n->second.name);
    assertx(n->second.flags() == FlagNone);
    assertx(!n->second.cinfo());
    assertx(!n->second.cinfo2());
    n->second.name = name;
    n->second.idx = table().nodes.size();
    n->second.setFlags(FlagMissing);
  } else {
    assertx(n->second.name->tsame(name));
    assertx(n->second.flags() == FlagMissing);
  }
  return ClassGraph{ &n->second };
}

void ClassGraph::init() {
  always_assert(!g_table);
  g_table = std::make_unique<Table>();
}

void ClassGraph::initConcurrent() {
  always_assert(!g_table);
  g_table = std::make_unique<Table>();
  g_table->locking.emplace();
}

void ClassGraph::stopConcurrent() {
  always_assert(g_table);
  g_table->locking.reset();
}

void ClassGraph::destroy() {
  assertx(IMPLIES(g_table, !g_table->index));
  g_table.reset();
}

ClassGraph::Table& ClassGraph::table() {
  always_assert_flog(
    g_table,
    "Attempting to access ClassGraph node table when one isn't active!"
  );
  return *g_table;
}

void ClassGraph::setAnalysisIndex(AnalysisIndex::IndexData& index) {
  assertx(!table().locking);
  assertx(!table().index);
  table().index = &index;
}

void ClassGraph::clearAnalysisIndex() {
  assertx(!table().locking);
  assertx(table().index);
  table().index = nullptr;
}

// Set the FlagComplete/FlagConservative and FlagRegSub/FlagNonRegSub
// flags in this class and it's children. Return the
// FlagRegSub/FlagNonRegSub flags from the children, along with the
// count of the subclass list from that child. If std::nullopt is
// given as the count, we've exceeded the limit we want to track.
template <typename Impl>
std::pair<ClassGraph::Flags, Optional<size_t>>
ClassGraph::setCompleteImpl(const Impl& impl, Node& n) {
  assertx(!n.isMissing());

  // Conservative nodes don't have children. However, they're
  // guaranteed to have their FlagRegSub and FlagNonRegSub flags set
  // properly, so we don't need to.
  if (n.hasCompleteChildren() || n.isConservative()) {
    auto f = FlagNone;
    if (n.isRegular() || n.hasRegularSubclass()) {
      f = (Flags)(f | FlagRegSub);
    }
    if (!n.isRegular() || n.hasNonRegularSubclass()) {
      f = (Flags)(f | FlagNonRegSub);
    }
    if (n.hasCompleteChildren()) {
      return std::make_pair(
        f,
        n.children.empty() ? 1 : impl.getCompleteSize(n)
      );
    }
    return std::make_pair(f, std::nullopt);
  }

  // Otherwise aggregate the flags and counts from the children.
  auto flags = FlagNone;
  Optional<size_t> count;
  count.emplace(1);

  if (!n.children.empty()) {
    impl.forEachChild(
      n,
      [&] (Node& child) {
        auto const [f, c] = setCompleteImpl(impl, child);
        flags = (Flags)(flags | f);
        if (count) {
          if (c) {
            *count += *c;
          } else {
            count.reset();
          }
        }
      }
    );
  }

  if (!count || *count > options.preciseSubclassLimit) {
    // The child is conservative, or we've exceeded the subclass list
    // limit. Mark this node as being conservative.
    setConservative(impl, n, flags & FlagRegSub, flags & FlagNonRegSub);
    count.reset();
  } else {
    // Didn't have complete children, but now does. Update the flags.
    if (!n.children.empty()) impl.setCompleteSize(n, *count);
    impl.lock(
      n,
      [&] {
        std::sort(begin(n.children), end(n.children), Node::Compare{});
        assertx(
          std::adjacent_find(begin(n.children), end(n.children)) ==
          end(n.children)
        );
      }
    );
    impl.updateFlags(n, (Flags)(flags | FlagChildren));
  }

  if (n.isRegular())  flags = (Flags)(flags | FlagRegSub);
  if (!n.isRegular()) flags = (Flags)(flags | FlagNonRegSub);
  return std::make_pair(flags, count);
}

// Make a node conservative (does not have any child information).
template <typename Impl>
void ClassGraph::setConservative(const Impl& impl,
                                 Node& n,
                                 bool regSub,
                                 bool nonRegSub) {
  assertx(!n.isMissing());
  assertx(!n.hasCompleteChildren());

  if (n.isConservative()) {
    assertx(n.hasRegularSubclass() == regSub);
    assertx(n.hasNonRegularSubclass() == nonRegSub);
    return;
  }

  assertx(!n.hasRegularSubclass());
  assertx(!n.hasNonRegularSubclass());

  auto f = FlagConservative;
  if (regSub)    f = (Flags)(f | FlagRegSub);
  if (nonRegSub) f = (Flags)(f | FlagNonRegSub);

  impl.updateFlags(n, f);
}

template <typename SerDe, typename T>
void ClassGraph::serde(SerDe& sd, T cinfo, bool ignoreChildren) {
  // Serialization/deserialization entry point. If we're operating
  // concurrently, use one Impl, otherwise, use the other.
  if (SerDe::deserializing && table().locking) {
    serdeImpl(sd, LockedSerdeImpl{}, cinfo, ignoreChildren);
  } else {
    serdeImpl(sd, UnlockedSerdeImpl{}, cinfo, ignoreChildren);
  }
}

template <typename SerDe, typename Impl, typename T>
void ClassGraph::serdeImpl(SerDe& sd,
                           const Impl& impl,
                           T cinfo,
                           bool ignoreChildren) {
  // Allocate SerdeState if someone else hasn't already.
  ScopedSerdeState _;

  sd.alternate(
    [&] {
      if constexpr (SerDe::deserializing) {
        // Deserializing:

        // First ensure that all nodes reachable by this node are
        // deserialized.
        sd.readWithLazyCount([&] { deserBlock(sd, impl); });

        // Then obtain a pointer to the node that ClassGraph points
        // to.
        if (auto const name = decodeName(sd)) {
          this_ = &impl.get(name);

          // If this node was marked as having complete children (and
          // we're not ignoring that), mark this node and all of it's
          // transitive children as also having complete children.
          Flags flags;
          sd(flags);
          if (flags & FlagChildren) {
            assertx(flags == FlagChildren);
            assertx(!this_->isConservative());
            if (!this_->hasCompleteChildren()) {
              setCompleteImpl(impl, *this_);
              assertx(this_->hasCompleteChildren());
            }
          } else if (flags & FlagConservative) {
            assertx(flags == (flags & (FlagConservative |
                                       FlagRegSub | FlagNonRegSub)));
            setConservative(
              impl,
              *this_,
              flags & FlagRegSub,
              flags & FlagNonRegSub
            );
          } else {
            assertx(flags == FlagNone);
          }

          // If this node isn't regular, and we've recorded an equivalent
          // node for it, make sure that the equivalent is
          // hasCompleteChildren(), as that's an invariant.
          if (auto const equiv = decodeName(sd)) {
            auto const equivNode = &impl.get(equiv);
            assertx(!equivNode->isRegular());
            if (!equivNode->hasCompleteChildren()) {
              assertx(!equivNode->isConservative());
              setCompleteImpl(impl, *equivNode);
              assertx(equivNode->hasCompleteChildren());
            }
            impl.setEquiv(*this_, *equivNode);
          }

          if constexpr (!std::is_null_pointer_v<T>) {
            if (cinfo) {
              assertx(!this_->isMissing());
              impl.setCInfo(*this_, *cinfo);
            }
          }
        } else {
          this_ = nullptr;
        }
      } else {
        // Serializing:

        if (!tl_serde_state.downward) tl_serde_state.downward.emplace();
        if (!tl_serde_state.upward) tl_serde_state.upward.emplace();

        // Serialize all of the nodes reachable by this node (parents,
        // children, and parents of children) and encode how many.
        sd.lazyCount(
          [&] () -> size_t {
            if (!this_) return 0;
            // Only encode children if requested.
            auto count = ignoreChildren
              ? serUpward(sd, *this_)
              : serDownward(sd, *this_);
            if (ignoreChildren) return count;
            if (auto const e =
                folly::get_default(table().regOnlyEquivs, this_)) {
              assertx(!this_->isRegular());
              assertx(e->hasCompleteChildren());
              count += serDownward(sd, *e);
            }
            return count;
          }
        );
        // Encode the "entry-point" into the graph represented by this
        // ClassGraph.
        if (this_) {
          encodeName(sd, this_->name);
          // Record whether this node has complete children, so we can
          // reconstruct that when deserializing.
          assertx(IMPLIES(hasCompleteChildren(), !isConservative()));
          assertx(IMPLIES(!hasCompleteChildren() && !isConservative(),
                          !this_->hasRegularSubclass() &&
                          !this_->hasNonRegularSubclass()));

          auto mask = (Flags)(FlagChildren | FlagConservative);
          if (this_->isConservative()) {
            mask = (Flags)(mask | FlagRegSub | FlagNonRegSub);
          }
          if (ignoreChildren) mask = (Flags)(mask & ~FlagChildren);
          sd((Flags)(this_->flags() & mask));

          // If this Node isn't regular and has an equivalent node, record
          // that here.
          if (!ignoreChildren && !this_->isMissing() && !this_->isRegular()) {
            if (auto const e =
                folly::get_default(table().regOnlyEquivs, this_)) {
              assertx(e->hasCompleteChildren());
              encodeName(sd, e->name);
            } else {
              encodeName(sd, nullptr);
            }
          } else {
            encodeName(sd, nullptr);
          }
        } else {
          encodeName(sd, nullptr);
        }
      }
    },
    [&] {
      // When serializing, we write this out last. When deserializing,
      // we read it first.
      assertx(tl_serde_state.isActive());
      sd(tl_serde_state.newStrings);
      tl_serde_state.strings.insert(
        end(tl_serde_state.strings),
        begin(tl_serde_state.newStrings),
        end(tl_serde_state.newStrings)
      );
      tl_serde_state.newStrings.clear();
    }
  );
}

// Serialize a string using the string table.
template <typename SerDe>
void ClassGraph::encodeName(SerDe& sd, SString s) {
  assertx(tl_serde_state.isActive());
  if (auto const idx = folly::get_ptr(tl_serde_state.strToIdx, s)) {
    sd(*idx);
    return;
  }
  auto const idx =
    tl_serde_state.strings.size() + tl_serde_state.newStrings.size();
  tl_serde_state.newStrings.emplace_back(s);
  tl_serde_state.strToIdx.emplace(s, idx);
  sd(idx);
}

// Deserialize a string using the string table.
template <typename SerDe>
SString ClassGraph::decodeName(SerDe& sd) {
  assertx(tl_serde_state.isActive());
  size_t idx;
  sd(idx);
  always_assert(idx < tl_serde_state.strings.size());
  return tl_serde_state.strings[idx];
}

// Deserialize a node, along with any other nodes it depends on.
template <typename SerDe, typename Impl>
void ClassGraph::deserBlock(SerDe& sd, const Impl& impl) {
  // First get the name for this node.
  auto const name = decodeName(sd);
  assertx(name);

  Flags flags;
  sd(flags);
  // These flags are never encoded and only exist at runtime.
  assertx((flags & kSerializable) == flags);
  assertx(IMPLIES(flags & FlagMissing, flags == FlagMissing));

  // Try to create it:
  auto const [node, created] = impl.create(name);
  if (created || node->isMissing()) {
    // If this is the first time we've seen this node, deserialize any
    // dependent nodes.
    sd.withSize(
      [&, node=node] {
        assertx(!node->hasCompleteChildren());
        assertx(!node->isConservative());

        // Either it already existed and we got an existing Node, or we
        // created it. Even if it already existed, we still need to process
        // it below as if it was new, because this might have additional
        // flags to add to the Node.

        // Deserialize dependent nodes.
        sd.readWithLazyCount([&] { deserBlock(sd, impl); });

        // At this point all dependent nodes are guaranteed to exist.

        // Read the parent links. The children links are not encoded as
        // they can be inferred from the parent links.
        CompactVector<Node*> parents;
        {
          size_t size;
          sd(size);
          parents.reserve(size);
          for (size_t i = 0; i < size; ++i) {
            // This should always succeed because all dependents
            // should exist.
            parents.emplace_back(&impl.get(decodeName(sd)));
          }
          parents.shrink_to_fit();
        }

        // If this is a "missing" node, it shouldn't have any links
        // (because we shouldn't know anything about it).
        assertx(IMPLIES(flags & FlagMissing, parents.empty()));

        // For each parent, register this node as a child. Lock the
        // appropriate node if we're concurrent deserializing.
        for (auto const parent : parents) {
          impl.lock(
            *parent,
            [&, node=node] {
              if (parent->hasCompleteChildren()) return;
              parent->children.emplace_back(node);
            }
          );
        }

        impl.lock(
          *node,
          [&, node=node] { node->parents = std::move(parents); }
        );
      }
    );

    if (created) {
      // If we created this node, we need to clear FlagWait and
      // simultaneously set the node's flags to what we decoded.
      impl.signal(*node, flags);
    } else if (!(flags & FlagMissing)) {
      impl.updateFlags(*node, flags, FlagMissing);
    }
  } else {
    // Otherwise skip over the dependent nodes.
    always_assert(flags == FlagMissing ||
                  flags == (node->flags() & kSerializable));
    sd.skipWithSize();
  }
}

// Walk downward through a node's children until we hit a leaf. At
// that point, we call serUpward on the leaf, which will serialize it
// and all of it's parents (which should include all nodes traversed
// here). Return the number of nodes serialized.
template <typename SerDe>
size_t ClassGraph::serDownward(SerDe& sd, Node& n) {
  assertx(!table().locking);
  assertx(tl_serde_state.isActive());

  if (!tl_serde_state.downward->add(n)) return 0;

  if (n.children.empty() || !n.hasCompleteChildren()) {
    return serUpward(sd, n);
  }
  assertx(!n.isConservative());

  size_t count = 0;
  for (auto const child : n.children) {
    count += serDownward(sd, *child);
  }
  return count;
}

// Serialize the given node, along with all of it's parents. Return
// true if anything was serialized.
template <typename SerDe>
bool ClassGraph::serUpward(SerDe& sd, Node& n) {
  assertx(!table().locking);
  assertx(tl_serde_state.isActive());
  // If we've already serialized this node, no need to serialize it
  // again.
  if (!tl_serde_state.upward->add(n)) return false;

  assertx(n.name);
  assertx(IMPLIES(n.isMissing(), n.parents.empty()));
  assertx(IMPLIES(n.isMissing(), n.children.empty()));
  assertx(IMPLIES(n.isMissing(), n.flags() == FlagMissing));
  assertx(IMPLIES(!n.hasCompleteChildren() && !n.isConservative(),
                  !n.hasRegularSubclass()));
  assertx(IMPLIES(!n.hasCompleteChildren() && !n.isConservative(),
                  !n.hasNonRegularSubclass()));

  encodeName(sd, n.name);
  // Shouldn't have any FlagWait when serializing.
  assertx(!(n.flags() & FlagWait));
  sd((Flags)(n.flags() & kSerializable));

  sd.withSize(
    [&] {
      // Recursively serialize all parents of this node. This ensures
      // that when deserializing, the parents will be available before
      // deserializing this node.
      sd.lazyCount(
        [&] {
          size_t count = 0;
          for (auto const parent : n.parents) {
            count += serUpward(sd, *parent);
          }
          return count;
        }
      );

      // Record the names of the parents, to restore the links when
      // deserializing.
      sd(n.parents.size());
      for (auto const p : n.parents) {
        assertx(p->name);
        encodeName(sd, p->name);
      }
    }
  );

  return true;
}

//////////////////////////////////////////////////////////////////////

// Storage for the auxiliary ClassGraphs a class or func may need.
struct AuxClassGraphs {
  // Nodes for which we don't need children.
  hphp_fast_set<ClassGraph, ClassGraphHasher> noChildren;
  // Nodes for which we have and need children.
  hphp_fast_set<ClassGraph, ClassGraphHasher> withChildren;

  // Equivalent to the above, but Nodes added within the current
  // worker. These are not serialized, but will replace noChildren and
  // withChildren when the worker is finishing.
  hphp_fast_set<ClassGraph, ClassGraphHasher> newNoChildren;
  hphp_fast_set<ClassGraph, ClassGraphHasher> newWithChildren;

  struct CacheKey {
    ClassGraph graph;
    bool children;
    bool regOnly;

    bool operator==(const CacheKey& o) const {
      return
        graph == o.graph &&
        children == o.children &&
        regOnly == o.regOnly;
    }
    struct Hasher {
      size_t operator()(const CacheKey& k) const {
        return folly::hash::hash_combine(
          ClassGraphHasher{}(k.graph),
          k.children,
          k.regOnly
        );
      }
    };
  };
  hphp_fast_map<CacheKey, bool, CacheKey::Hasher> cache;

  template <typename SerDe> void serde(SerDe& sd) {
    ClassGraph::ScopedSerdeState _;
    sd(noChildren, std::less<>{}, nullptr, true)
      (withChildren, std::less<>{}, nullptr, false)
      ;
    // The rest deliberately not serialized.
  }
};

//////////////////////////////////////////////////////////////////////

template <typename SerDe> void FuncInfo2::serde(SerDe& sd) {
  ScopedStringDataIndexer _;
  ClassGraph::ScopedSerdeState _2;
  sd(name)
    (inferred)
    (retained)
    (auxClassGraphs)
    ;
}

//////////////////////////////////////////////////////////////////////

// Stores information about declarations of properties. Using this, a
// class can find the parent class the property is inherited from,
// as well as any possible subclasses which have redeclarations.
struct PropDeclInfo {
  // The class that declares this prop (if any)
  SString decl{nullptr};
  // Subclasses that also declare/override this prop
  TSStringSet subDecls;

  template <typename SerDe> void serde(SerDe& sd) {
    sd(decl)
      (subDecls, string_data_lt_type{})
      ;
  }
};

/*
 * For a given class (non-type) constant, information about the
 * constant and any redeclarations in subclasses. ClassInfo2 will have
 * an entry for every constant declared on the class (or inherited
 * from parent). It may have entries for constants which are not known
 * to the class. This represents a constant declared solely in a
 * subclass. In this case, the presence of an entry is to indicate the
 * constant may exist (as opposed to definitely not existing).
 *
 * This exists to avoid having to visit all subclasses to resolve the
 * type (which can be expensive).
 */
struct ClsCnsSubInfo {
  // Union of possible ClsConstLookupResults (non-dynamic) which can
  // result from looking up this constant.
  ClsConstLookupResult result;
  // Subclasses which have a dynamic initialization of this constant.
  SStringSet dynamic;

  ClsCnsSubInfo& operator|=(const ClsCnsSubInfo& o) {
    result |= o.result;
    dynamic.insert(begin(o.dynamic), end(o.dynamic));
    if (result.ty.is(BInitCell) &&
        result.found == TriBool::Maybe &&
        result.mightThrow) {
      dynamic.clear();
    }
    return *this;
  }

  bool isMissing() const { return result.found == TriBool::No; }

  static ClsCnsSubInfo missing() {
    return ClsCnsSubInfo{
      ClsConstLookupResult{ TBottom, TriBool::No, false }
    };
  }

  static ClsCnsSubInfo conservative() {
    return ClsCnsSubInfo{
      ClsConstLookupResult{ TInitCell, TriBool::Maybe, true }
    };
  }

  static ClsCnsSubInfo fromCns(const php::Class& cls, const php::Const& cns) {
    using C = ClsCnsSubInfo;
    using R = ClsConstLookupResult;
    if (!cns.val.has_value() || cns.kind != ConstModifierFlags::Kind::Value) {
      return missing();
    }
    if (type(*cns.val) != KindOfUninit) {
      auto const mightThrow = bool(cls.attrs & AttrInternal);
      return C{ R{ from_cell(*cns.val), TriBool::Yes, mightThrow }};
    }
    C c{ R{ TBottom, TriBool::Yes, true }};
    c.dynamic.emplace(cls.name);
    return c;
  }

  template <typename SerDe> void serde(SerDe& sd) {
    sd(result)
      (dynamic, string_data_lt{})
      ;
  }
};

//////////////////////////////////////////////////////////////////////

/*
 * Known information about an instantiatiable class.
 */
struct ClassInfo {
  /*
   * A pointer to the underlying php::Class that we're storing
   * information about.
   */
  const php::Class* cls = nullptr;

  /*
   * The info for the parent of this Class.
   */
  ClassInfo* parent = nullptr;

  struct ConstIndex {
    const php::Const& operator*() const {
      return cls->constants[idx];
    }
    const php::Const* operator->() const {
      return get();
    }
    const php::Const* get() const {
      return &cls->constants[idx];
    }
    const php::Class* cls;
    uint32_t idx;
  };

  /*
   * A (case-sensitive) map from class constant name to the php::Class* and
   * index into the constants vector that it came from. This map is flattened
   * across the inheritance hierarchy. Use a vector_map for stable iteration.
   */
  hphp_vector_map<SString, ConstIndex> clsConstants;

  /*
   * Inferred class constant types for the constants declared on this
   * class. The order mirrors the order in the php::Class constants
   * vector. If the vector is smaller than a constant's index, that
   * constant's type is implicitly TInitCell.
   */
  CompactVector<ClsConstInfo> clsConstTypes;

  /*
   * The traits used by this class, which *haven't* been flattened
   * into it. If the class is AttrNoExpandTrait, this will always be
   * empty.
   */
  CompactVector<const ClassInfo*> usedTraits;

  /*
   * A list of extra properties supplied by this class's used traits.
   */
  CompactVector<php::Prop> traitProps;

  /*
   * A (case-sensitive) map from class method names to the php::Func
   * associated with it. This map is flattened across the inheritance
   * hierarchy. There's a lot of these, so we use a sorted_vector_map
   * to minimize wasted space.
   */
  folly::sorted_vector_map<SString, MethTabEntry> methods;

  /*
   * A (case-sensitive) map from class method names to associated
   * FuncFamilyOrSingle objects that represent the set of
   * possibly-overriding methods.
   *
   * In addition to the set of methods, a bit is also set indicating
   * whether the set of "complete" or not. A complete set means the
   * ultimate method will definitely be one in the set. An incomplete
   * set means that the ultimate method will either be one in the set,
   * or won't resolve to anything (a missing function).
   *
   * We do not encode "special" methods in these, as their semantics
   * are special and it's not useful.
   *
   * For every method present in this ClassInfo's method table, there
   * will be an entry in methodFamilies. For regular classes, this
   * suffices for looking up information for both all subclasses and
   * the regular subset. For non-regular classes, the results for
   * "all" and the regular subset may differ. In that case, there is a
   * separate "aux" table containing the results for the regular
   * subset. If there is no associated entry in the aux table, the
   * result is the same as the entry in the normal table (this is a
   * common case and saves on memory). For regular classes the aux
   * table is always empty.
   *
   * If a method is marked as AttrNoOverride, it will not have an
   * entry in these maps. If a method is marked as noOverrideRegular,
   * it will not have an entry in the aux map (if it would have
   * otherwise). In either case, the resolved method is assumed to be
   * the same method in this ClassInfo's method table.
   *
   * The above is true for all class types. For abstract classes and
   * interfaces, however, there may be more entries here than present
   * in the methods table. These correspond to methods implemented by
   * *all* regular subclasses of the abstract class/interface. For
   * that reason, they will only be present in the regular variant of
   * the map. This is needed to preserve monotonicity (see comment in
   * BuildSubclassListJob::process_roots).
   */
  folly::sorted_vector_map<SString, FuncFamilyOrSingle> methodFamilies;
  folly::sorted_vector_map<SString, FuncFamilyOrSingle> methodFamiliesAux;

  ClassGraph classGraph;

  /*
   * Property types for public static properties, declared on this exact class
   * (i.e. not flattened in the hierarchy).
   */
  SStringToOneT<PublicSPropEntry> publicStaticProps;

  /*
   * Flags to track if this class is mocked, or if any of its derived classes
   * are mocked.
   */
  bool isMocked{false};
  bool isSubMocked{false};

  /*
   * Track if this class has a property which might redeclare a property in a
   * parent class with an inequivalent type-hint.
   */
  bool hasBadRedeclareProp{true};

  /*
   * Track if this class has any properties with initial values that might
   * violate their type-hints.
   */
  bool hasBadInitialPropValues{true};

  /*
   * Track if this class has any const props (including inherited ones).
   */
  bool hasConstProp{false};

  /*
   * Track if any derived classes (including this one) have any const props.
   */
  bool subHasConstProp{false};

  /*
   * Track if this class has a reified parent.
   */
  bool hasReifiedParent{false};
};

/*
 * Known information about an instantiable class.
 *
 * This class mirrors the ClassInfo struct, but is produced and used
 * by remote workers. As needed, this struct will gain more and more
 * of ClassInfo's fields (but stored in a more remote worker friendly
 * way). Local calculations will continue to use ClassInfo. Once
 * everything is converted to use remote workers, this struct will
 * subsume ClassInfo entirely (and be renamed).
 */
struct ClassInfo2 {
  /*
   * The name of the underlying php::Class that this ClassInfo
   * represents.
   */
  LSString name{nullptr};

  /*
   * The name of the parent of this class (or nullptr if none).
   */
  LSString parent{nullptr};

  /*
   * php::Class associated with this ClassInfo. Must be set manually
   * when needed.
   */
  const php::Class* cls{nullptr};

  /*
   * A (case-sensitive) map from class constant name to the ConstIndex
   * representing the constant. This map is flattened across the
   * inheritance hierarchy.
   */
  struct ConstIndexAndKind {
    ConstIndex idx;
    // Store the kind here as well, so we don't need to potentially
    // find another class to determine it.
    ConstModifierFlags::Kind kind;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(idx)(kind);
    }
  };
  SStringToOneT<ConstIndexAndKind> clsConstants;

  // Metadata tracking where a property is declared across the class
  // hierarchy. For a given property, this allows one to identify all
  // relevant declaring classes without walking entire hierarchies,
  // which is expensive.
  //
  // If there's no entry in the map, the prop cannot be present. If
  // there's an entry, but decl and subDecls are empty, it means the
  // prop isn't declared on this class (or parents), but at least one
  // subclass does. We are pessimistic in that case. This saves space
  // in the table and such accesses shouldn't type-check anyways.
  SStringToOneT<PropDeclInfo> propDeclInfo;

  // For a class constant access like Foo::CNS, information about CNS
  // which can be used to infer its type (without visiting all
  // subclasses). The absence of an entry means CNS definitely doesn't
  // exist. If CNS is not declared on this class (or parents), but is
  // declared in a subclass, it will still have an entry here.
  SStringToOneT<ClsCnsSubInfo> cnsSubInfo;

  /*
   * Inferred information about this class
   */
  InferredClassInfo inferred;

  /*
   * A list of extra properties supplied by this class's used traits.
   */
  CompactVector<php::Prop> traitProps;

  /*
   * A (case-sensitive) map from class method names to the
   * MethTabEntry associated with it. This map is flattened across the
   * inheritance hierarchy. MethTabEntry represents the php::Func,
   * along with some metadata specific to the method on this specific
   * class.
   */
  SStringToOneT<MethTabEntry> methods;

  /*
   * In most situations, methods are inherited from a parent class, so
   * if a class declares a method, all of its subclasses are
   * guaranteed to possess it. An exception to this is with
   * interfaces, whose methods are not inherited.
   *
   * However, when building func-families and other method data, its
   * useful to know all of the methods declared by all parents of a
   * class. Most of those methods will be on the method table for this
   * ClassInfo, but if any aren't, they'll be added to this set.
   */
  SStringSet missingMethods;

  /*
   * ClassGraph representing this class. This ClassGraph is guaranteed
   * to have hasCompleteChildren() be true (except if this is the
   * Closure base class).
   */
  ClassGraph classGraph;

  /*
   * Set of "extra" methods for this class. These are methods which
   * aren't formally part of this class, but must be analyzed along
   * with the class' methods. Examples are unflattened trait methods,
   * and the invoke methods of their associated closures.
   */
  MethRefSet extraMethods;

  /*
   * A vector of the closures class-infos declared in this class. Such
   * closures are stored in their declaring class, not as a top-level
   * class.
   */
  CompactVector<std::unique_ptr<ClassInfo2>> closures;

  /*
   * A (case-sensitive) map from method names to associated
   * FuncFamilyEntry objects that represent the set of
   * possibly-overriding methods.
   *
   * We do not encode "special" methods in these, as their semantics
   * are special and it's not useful.
   *
   * For every method present in this ClassInfo's method table, there
   * will be an entry in methodFamilies (unless if AttrNoOverride, see
   * below). The FuncFamilyEntry will provide the set of
   * possibly-overriding methods, for both the regular class subset
   * and all classes.
   *
   * If a method is marked as AttrNoOverride, it will not have an
   * entry in this map. The resolved method is assumed to be the same
   * method in this ClassInfo's method table. If a method is marked as
   * noOverrideRegular, it will have an entry in this map, but can be
   * treated as AttrNoOverride if you're only considering regular
   * classes.
   *
   * There may be more entries in methodFamilies than in the
   * ClassInfo's method table. For classes which aren't abstract and
   * aren't interfaces, this is an artifact of how the table is
   * created and can be ignored. For abstract classes and interfaces,
   * however, these extra entries correspond to methods implemented by
   * *all* regular subclasses of the abstract class/interface. In this
   * situation, only the data in the FuncFamilyEntry corresponding to
   * the regular subset is meaningful. This is needed to preserve
   * monotonicity (see comment in
   * BuildSubclassListJob::process_roots).
   */
  SStringToOneT<FuncFamilyEntry> methodFamilies;

  /*
   * FuncInfo2s for the methods declared on this class (not
   * flattened). This is in the same order as the methods vector on
   * the associated php::Class.
   */
  CompactVector<std::unique_ptr<FuncInfo2>> funcInfos;

  /*
   * If we utilize a ClassGraph while resolving types, we store it
   * here. This ensures that that ClassGraph will always be available
   * again.
   */
  AuxClassGraphs auxClassGraphs;

  /*
   * Retained information about other entities which have contributed
   * to this class's inferred information.
   */
  RetainedInfo retained;

  /*
   * Track if this class has a property which might redeclare a property in a
   * parent class with an inequivalent type-hint.
   */
  bool hasBadRedeclareProp{true};

  /*
   * Track if this class has any properties with initial values that might
   * violate their type-hints.
   */
  bool hasBadInitialPropValues{true};

  /*
   * Track if this class has any const props (including inherited ones).
   */
  bool hasConstProp{false};

  /*
   * Track if any derived classes (including this one) have any const props.
   */
  bool subHasConstProp{false};

  /*
   * Track if this class has a reified parent.
   */
  bool hasReifiedParent{false};

  /*
   * Whether this class (or any derived classes) has a __Reified
   * attribute.
   */
  bool hasReifiedGeneric{false};
  bool subHasReifiedGeneric{false};

  /*
   * Initial AttrNoReifiedInit setting of attrs (which might be
   * modified).
   */
  bool initialNoReifiedInit{false};

  /*
   * Whether is_mock_class() is true for this class.
   */
  bool isMockClass{false};

  /*
   * Whether this class is mocked (has a direct subclass for which
   * is_mock_class() is true).
   */
  bool isMocked{false};

  /*
   * Whether isMocked is true for this class, or any of its
   * subclasses.
   */
  bool isSubMocked{false};

  /*
   * Whether is_regular_class() is true for this class.
   */
  bool isRegularClass{false};

  template <typename SerDe> void serde(SerDe& sd) {
    ScopedStringDataIndexer _;
    ClassGraph::ScopedSerdeState _2;
    sd(name)
      (parent)
      (clsConstants, string_data_lt{})
      (propDeclInfo, string_data_lt{})
      (cnsSubInfo, string_data_lt{})
      (inferred)
      (traitProps)
      (methods, string_data_lt{})
      (missingMethods, string_data_lt{})
      (classGraph, this)
      (extraMethods, std::less<MethRef>{})
      (closures)
      (methodFamilies, string_data_lt{})
      (funcInfos)
      (auxClassGraphs)
      (retained)
      (hasBadRedeclareProp)
      (hasBadInitialPropValues)
      (hasConstProp)
      (subHasConstProp)
      (hasReifiedParent)
      (hasReifiedGeneric)
      (subHasReifiedGeneric)
      (initialNoReifiedInit)
      (isMockClass)
      (isMocked)
      (isSubMocked)
      (isRegularClass)
      ;
  }
};

//////////////////////////////////////////////////////////////////////

const InferredFuncInfo* RetainedInfo::get(const FuncInfo2& finfo) const {
  if (!state) return nullptr;
  return folly::get_ptr(state->funcCurrent, makeKey(finfo));
}

const InferredClassInfo* RetainedInfo::get(const ClassInfo2& cinfo) const {
  if (!state) return nullptr;
  return folly::get_ptr(state->clsCurrent, cinfo.name);
}

InferredFuncInfo& RetainedInfo::retain(const FuncInfo2& finfo) {
  if (!state) state = std::make_unique<State>();
  return state->funcNext.try_emplace(makeKey(finfo)).first->second;
}

InferredClassInfo& RetainedInfo::retain(const ClassInfo2& cinfo) {
  if (!state) state = std::make_unique<State>();
  return state->clsNext.try_emplace(cinfo.name).first->second;
}

void RetainedInfo::flip() {
  if (!state) return;
  state->funcCurrent = std::move(state->funcNext);
  state->clsCurrent = std::move(state->clsNext);
  state->funcNext.clear();
  state->clsNext.clear();
}

RetainedInfo::FKey RetainedInfo::makeKey(const FuncInfo2& finfo) {
  assertx(finfo.func);
  return FKey{
    finfo.func->cls ? finfo.func->cls->name : nullptr,
    finfo.func->name
  };
}

template <typename SerDe> void RetainedInfo::serde(SerDe& sd) {
  ScopedStringDataIndexer _;
  sd(state);
}

//////////////////////////////////////////////////////////////////////

namespace res {

//////////////////////////////////////////////////////////////////////

Class::Class(ClassGraph g): opaque{g.this_} {
  assertx(g.this_);
}

ClassGraph Class::graph() const {
  assertx(opaque.left());
  return ClassGraph{ (ClassGraph::Node*)opaque.left() };
}

bool Class::isSerialized() const {
  return opaque.right();
}

ClassInfo* Class::cinfo() const {
  return graph().cinfo();
}

ClassInfo2* Class::cinfo2() const {
  return graph().cinfo2();
}

bool Class::same(const Class& o) const {
  if (opaque.left()) {
    if (!o.opaque.left()) return false;
    return graph() == o.graph();
  } else if (o.opaque.right()) {
    return opaque.right()->tsame(o.opaque.right());
  } else {
    return false;
  }
}

bool Class::exactSubtypeOfExact(const Class& o,
                                bool nonRegL,
                                bool nonRegR) const {
  return graph().exactSubtypeOfExact(o.graph(), nonRegL, nonRegR);
}

bool Class::exactSubtypeOf(const Class& o, bool nonRegL, bool nonRegR) const {
  return graph().exactSubtypeOf(o.graph(), nonRegL, nonRegR);
}

bool Class::subSubtypeOf(const Class& o, bool nonRegL, bool nonRegR) const {
  return graph().subSubtypeOf(o.graph(), nonRegL, nonRegR);
}

bool Class::exactCouldBeExact(const Class& o,
                              bool nonRegL,
                              bool nonRegR) const {
  return graph().exactCouldBeExact(o.graph(), nonRegL, nonRegR);
}

bool Class::exactCouldBe(const Class& o, bool nonRegL, bool nonRegR) const {
  return graph().exactCouldBe(o.graph(), nonRegL, nonRegR);
}

bool Class::subCouldBe(const Class& o, bool nonRegL, bool nonRegR) const {
  return graph().subCouldBe(o.graph(), nonRegL, nonRegR);
}

SString Class::name() const {
  if (opaque.left()) {
    return graph().name();
  } else {
    assertx(opaque.right());
    return opaque.right();
  }
}

Optional<res::Class> Class::withoutNonRegular() const {
  if (auto const g = graph().withoutNonRegular()) {
    return Class { g };
  } else {
    return std::nullopt;
  }
}

bool Class::mightBeRegular()    const { return graph().mightBeRegular(); }
bool Class::mightBeNonRegular() const { return graph().mightBeNonRegular(); }

bool Class::couldBeOverridden() const {
  if (!graph().isMissing() && graph().isTrait()) return false;
  return
    graph().mightHaveRegularSubclass() ||
    graph().mightHaveNonRegularSubclass();
}

bool Class::couldBeOverriddenByRegular() const {
  if (!graph().isMissing() && graph().isTrait()) return false;
  return graph().mightHaveRegularSubclass();
}

bool Class::mightContainNonRegular() const {
  return graph().mightBeNonRegular() || graph().mightHaveNonRegularSubclass();
}

bool Class::couldHaveMagicBool() const {
  auto const g = graph();
  g.ensure();
  if (g.isMissing()) return true;
  if (!g.isInterface()) return has_magic_bool_conversion(g.topBase().name());
  g.ensureWithChildren();
  if (!g.hasCompleteChildren()) return true;
  for (auto const c : g.children()) {
    if (c.isMissing()) return true;
    if (has_magic_bool_conversion(c.topBase().name())) return true;
  }
  return false;
}

bool Class::couldHaveMockedSubClass() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->isSubMocked;
  } else if (auto const ci = cinfo2()) {
    return ci->isSubMocked;
  } else {
    return true;
  }
}

bool Class::couldBeMocked() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->isMocked;
  } else if (auto const ci = cinfo2()) {
    return ci->isMocked;
  } else {
    return true;
  }
}

bool Class::couldHaveReifiedGenerics() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->cls->hasReifiedGenerics;
  } else if (auto const ci = cinfo2()) {
    return ci->hasReifiedGeneric;
  } else {
    return true;
  }
}

bool Class::mustHaveReifiedGenerics() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->cls->hasReifiedGenerics;
  } else if (auto const ci = cinfo2()) {
    return ci->hasReifiedGeneric;
  } else {
    return false;
  }
}

bool Class::couldHaveReifiedParent() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->hasReifiedParent;
  } else if (auto const ci = cinfo2()) {
    return ci->hasReifiedParent;
  } else {
    return true;
  }
}

bool Class::mustHaveReifiedParent() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->hasReifiedParent;
  } else if (auto const ci = cinfo2()) {
    return ci->hasReifiedParent;
  } else {
    return false;
  }
}

bool Class::mightCareAboutDynConstructs() const {
  if (!Cfg::Eval::ForbidDynamicConstructs) return false;
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return !(ci->cls->attrs & AttrDynamicallyConstructible);
  } else if (auto const ci = cinfo2()) {
    return !ci->cls || !(ci->cls->attrs & AttrDynamicallyConstructible);
  } else {
    return true;
  }
}

bool Class::mightCareAboutDynamicallyReferenced() const {
  if (Cfg::Eval::DynamicallyReferencedNoticeSampleRate == 0) return false;
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return !(ci->cls->attrs & AttrDynamicallyReferenced);
  } else if (auto const ci = cinfo2()) {
    return !ci->cls || !(ci->cls->attrs & AttrDynamicallyReferenced);
  } else {
    return true;
  }
}

bool Class::couldHaveConstProp() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->hasConstProp;
  } else if (auto const ci = cinfo2()) {
    return ci->hasConstProp;
  } else {
    return true;
  }
}

bool Class::subCouldHaveConstProp() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->subHasConstProp;
  } else if (auto const ci = cinfo2()) {
    return ci->subHasConstProp;
  } else {
    return true;
  }
}

Optional<res::Class> Class::parent() const {
  if (auto const p = graph().base()) {
    return Class { p };
  } else {
    return std::nullopt;
  }
}

const php::Class* Class::cls() const {
  graph().ensureCInfo();
  if (auto const ci = cinfo()) {
    return ci->cls;
  } else if (auto const ci = cinfo2()) {
    return ci->cls;
  } else {
    return nullptr;
  }
}

bool Class::hasCompleteChildren() const {
  return graph().hasCompleteChildren();
}

bool Class::isComplete() const {
  return
    !graph().isMissing() &&
    (graph().hasCompleteChildren() || graph().isConservative());
}

bool
Class::forEachSubclass(const std::function<void(SString, Attr)>& f) const {
  auto const g = graph();
  g.ensureWithChildren();
  if (g.isMissing() || !g.hasCompleteChildren()) return false;
  for (auto const c : g.children()) {
    auto const attrs = [&] {
      if (c.isInterface()) return AttrInterface;
      if (c.isTrait())     return AttrTrait;
      if (c.isEnum())      return AttrEnum;
      if (c.isAbstract())  return AttrAbstract;
      return AttrNone;
    }();
    f(c.name(), attrs);
  }
  return true;
}

std::string show(const Class& c) {
  if (auto const n = c.opaque.right()) {
    return folly::sformat("?\"{}\"", n);
  }

  auto const g = c.graph();
  if (g.isMissing()) return folly::sformat("\"{}\"", g.name());
  if (!g.hasCompleteChildren()) {
    if (g.isConservative()) {
      return folly::sformat(
        "{}*{}",
        (c.cinfo() || c.cinfo2()) ? "" : "-",
        g.name()
      );
    }
    return folly::sformat("!{}", g.name());
  }
  if (!c.cinfo() && !c.cinfo2()) return folly::sformat("-{}", g.name());
  return g.name()->toCppString();
}

// Call the given callable for every class which is a subclass of
// *all* the classes in the range. If the range includes nothing but
// unresolved classes, they will be passed, as-is, to the callable. If
// the range includes a mix of resolved and unresolved classes, the
// unresolved classes will be used to narrow the classes passed to the
// callable, but the unresolved classes themself will not be passed to
// the callable. If the callable returns false, iteration is
// stopped. If includeNonRegular is true, non-regular subclasses are
// visited (normally they are skipped).
template <typename F>
void Class::visitEverySub(folly::Range<const Class*> classes,
                          bool includeNonRegular,
                          const F& f) {
  if (classes.size() == 1) {
    auto const n = classes[0].graph().this_;
    ClassGraph{n}.ensureWithChildren(!includeNonRegular);
    if (n->isMissing() || !n->hasCompleteChildren()) {
      f(Class{ClassGraph{ n }});
      return;
    }
    ClassGraph::forEachChild(
      *n,
      [&] (ClassGraph::Node& c) {
        assertx(!c.isMissing());
        if (includeNonRegular || c.isRegular()) {
          if (!f(Class{ClassGraph{ &c }})) return ClassGraph::Action::Stop;
        }
        return ClassGraph::Action::Continue;
      }
    );
    return;
  }

  ClassGraph::NodeVec v;
  v.reserve(classes.size());
  for (auto const c : classes) v.emplace_back(c.graph().this_);

  ClassGraph::enumerateIsectMembers(
    v,
    includeNonRegular,
    [&] (ClassGraph::Node& c) { return f(Class{ClassGraph{ &c }}); },
    true
  );
}

Class::ClassVec Class::combine(folly::Range<const Class*> classes1,
                               folly::Range<const Class*> classes2,
                               bool isSub1,
                               bool isSub2,
                               bool nonRegular1,
                               bool nonRegular2) {
  ClassGraph::NodeVec v1;
  ClassGraph::NodeVec v2;
  v1.reserve(classes1.size());
  v2.reserve(classes2.size());
  for (auto const c : classes1) v1.emplace_back(c.graph().this_);
  for (auto const c : classes2) v2.emplace_back(c.graph().this_);
  auto const i =
    ClassGraph::combine(v1, v2, isSub1, isSub2, nonRegular1, nonRegular2);
  ClassVec out;
  out.reserve(i.size());
  for (auto const c : i) out.emplace_back(Class{ClassGraph { c }});
  return out;
}

Class::ClassVec Class::removeNonRegular(folly::Range<const Class*> classes) {
  ClassGraph::NodeVec v;
  v.reserve(classes.size());
  for (auto const c : classes) v.emplace_back(c.graph().this_);
  auto const i = ClassGraph::removeNonReg(v);
  ClassVec out;
  out.reserve(i.size());
  for (auto const c : i) out.emplace_back(Class{ClassGraph { c }});
  return out;
}

Class::ClassVec Class::intersect(folly::Range<const Class*> classes1,
                                 folly::Range<const Class*> classes2,
                                 bool nonRegular1,
                                 bool nonRegular2,
                                 bool& nonRegularOut) {
  ClassGraph::NodeVec v1;
  ClassGraph::NodeVec v2;
  v1.reserve(classes1.size());
  v2.reserve(classes2.size());
  for (auto const c : classes1) v1.emplace_back(c.graph().this_);
  for (auto const c : classes2) v2.emplace_back(c.graph().this_);
  auto const i =
    ClassGraph::intersect(v1, v2, nonRegular1, nonRegular2, nonRegularOut);
  ClassVec out;
  out.reserve(i.size());
  for (auto const c : i) out.emplace_back(Class{ClassGraph { c }});
  return out;
}

bool Class::couldBeIsect(folly::Range<const Class*> classes1,
                         folly::Range<const Class*> classes2,
                         bool nonRegular1,
                         bool nonRegular2) {
  ClassGraph::NodeVec v1;
  ClassGraph::NodeVec v2;
  v1.reserve(classes1.size());
  v2.reserve(classes2.size());
  for (auto const c : classes1) v1.emplace_back(c.graph().this_);
  for (auto const c : classes2) v2.emplace_back(c.graph().this_);
  return ClassGraph::couldBeIsect(v1, v2, nonRegular1, nonRegular2);
}

Optional<Class> Class::unserialize(const IIndex& index) const {
  if (opaque.left()) return *this;
  return index.resolve_class(opaque.right());
}

Class Class::serialize() const {
  return Class{ name() };
}

Class Class::get(SString name) {
  return Class{ ClassGraph::get(name) };
}

Class Class::get(const ClassInfo& cinfo) {
  assertx(cinfo.classGraph);
  return Class{ cinfo.classGraph };
}

Class Class::get(const ClassInfo2& cinfo) {
  assertx(cinfo.classGraph);
  return Class{ cinfo.classGraph };
}

Class Class::getOrCreate(SString name) {
  return Class{ ClassGraph::getOrCreate(name) };
}

Class Class::getUnresolved(SString name) {
  return Class{ ClassGraph::getMissing(name) };
}

void Class::makeConservativeForTest() {
  auto n = graph().this_;
  n->setFlags(ClassGraph::FlagConservative, ClassGraph::FlagChildren);
  n->children.clear();
}

#ifndef NDEBUG
bool Class::isMissingDebug() const {
  return graph().isMissing();
}
#endif

void Class::serde(BlobEncoder& sd) const {
  assertx(isSerialized());
  sd(ClassGraph::get(opaque.right()), nullptr);
}

Class Class::makeForSerde(BlobDecoder& sd) {
  ClassGraph g;
  sd(g, nullptr);
  assertx(g.this_);
  // Make the class start out as serialized.
  return Class{ g.name() };
}

//////////////////////////////////////////////////////////////////////

Func::Func(Rep val)
  : val(val)
{}

std::string Func::name() const {
  return match<std::string>(
    val,
    [] (FuncName s)   { return s.name->toCppString(); },
    [] (MethodName s) {
      if (s.cls) return folly::sformat("{}::{}", s.cls, s.name);
      return folly::sformat("???::{}", s.name);
    },
    [] (Fun f)        { return f.finfo->func->name->toCppString(); },
    [] (Fun2 f)       { return f.finfo->name->toCppString(); },
    [] (Method m)     { return func_fullname(*m.finfo->func); },
    [] (Method2 m)    { return func_fullname(*m.finfo->func); },
    [] (MethodFamily fam) {
      return folly::sformat(
        "*::{}",
        fam.family->possibleFuncs().front().ptr()->name
      );
    },
    [] (MethodFamily2 fam)  {
      return folly::sformat(
        "{}::{}",
        fam.family->m_id,
        fam.family->m_name
      );
    },
    [] (MethodOrMissing m)  { return func_fullname(*m.finfo->func); },
    [] (MethodOrMissing2 m) { return func_fullname(*m.finfo->func); },
    [] (MissingFunc m)      { return m.name->toCppString(); },
    [] (MissingMethod m)    {
      if (m.cls) return folly::sformat("{}::{}", m.cls, m.name);
      return folly::sformat("???::{}", m.name);
    },
    [] (const Isect& i) {
      assertx(i.families.size() > 1);
      return func_fullname(*i.families[0]->possibleFuncs().front().ptr());
    },
    [] (const Isect2& i) {
      assertx(i.families.size() > 1);
      using namespace folly::gen;
      return folly::sformat(
        "{}::{}",
        from(i.families)
          | map([] (const FuncFamily2* ff) { return ff->m_id.toString(); })
          | unsplit<std::string>("&"),
        i.families[0]->m_name
      );
    }
  );
}

const php::Func* Func::exactFunc() const {
  using Ret = const php::Func*;
  return match<Ret>(
    val,
    [] (FuncName)                    { return Ret{}; },
    [] (MethodName)                  { return Ret{}; },
    [] (Fun f)                       { return f.finfo->func; },
    [] (Fun2 f)                      { return f.finfo->func; },
    [] (Method m)                    { return m.finfo->func; },
    [] (Method2 m)                   { return m.finfo->func; },
    [] (MethodFamily)                { return Ret{}; },
    [] (MethodFamily2)               { return Ret{}; },
    [] (MethodOrMissing)             { return Ret{}; },
    [] (MethodOrMissing2)            { return Ret{}; },
    [] (MissingFunc)                 { return Ret{}; },
    [] (MissingMethod)               { return Ret{}; },
    [] (const Isect&)                { return Ret{}; },
    [] (const Isect2&)               { return Ret{}; }
  );
}

TriBool Func::exists() const {
  return match<TriBool>(
    val,
    [] (FuncName)                    { return TriBool::Maybe; },
    [] (MethodName)                  { return TriBool::Maybe; },
    [] (Fun)                         { return TriBool::Yes; },
    [] (Fun2)                        { return TriBool::Yes; },
    [] (Method)                      { return TriBool::Yes; },
    [] (Method2)                     { return TriBool::Yes; },
    [] (MethodFamily)                { return TriBool::Maybe; },
    [] (MethodFamily2)               { return TriBool::Maybe; },
    [] (MethodOrMissing)             { return TriBool::Maybe; },
    [] (MethodOrMissing2)            { return TriBool::Maybe; },
    [] (MissingFunc)                 { return TriBool::No; },
    [] (MissingMethod)               { return TriBool::No; },
    [] (const Isect&)                { return TriBool::Maybe; },
    [] (const Isect2&)               { return TriBool::Maybe; }
  );
}

bool Func::isFoldable() const {
  return match<bool>(
    val,
    [] (FuncName)   { return false; },
    [] (MethodName) { return false; },
    [] (Fun f) {
      return f.finfo->func->attrs & AttrIsFoldable;
    },
    [] (Fun2 f) {
      return f.finfo->func->attrs & AttrIsFoldable;
    },
    [] (Method m)  { return m.finfo->func->attrs & AttrIsFoldable; },
    [] (Method2 m) { return m.finfo->func->attrs & AttrIsFoldable; },
    [] (MethodFamily)    { return false; },
    [] (MethodFamily2)   { return false; },
    [] (MethodOrMissing) { return false; },
    [] (MethodOrMissing2){ return false; },
    [] (MissingFunc)     { return false; },
    [] (MissingMethod)   { return false; },
    [] (const Isect&)    { return false; },
    [] (const Isect2&)   { return false; }
  );
}

bool Func::couldHaveReifiedGenerics() const {
  return match<bool>(
    val,
    [] (FuncName s) { return true; },
    [] (MethodName) { return true; },
    [] (Fun f) { return f.finfo->func->isReified; },
    [] (Fun2 f) { return f.finfo->func->isReified; },
    [] (Method m) { return m.finfo->func->isReified; },
    [] (Method2 m) { return m.finfo->func->isReified; },
    [] (MethodFamily fa) {
      return fa.family->infoFor(fa.regularOnly).m_static->m_maybeReified;
    },
    [] (MethodFamily2 fa) {
      return fa.family->infoFor(fa.regularOnly).m_maybeReified;
    },
    [] (MethodOrMissing m) { return m.finfo->func->isReified; },
    [] (MethodOrMissing2 m) { return m.finfo->func->isReified; },
    [] (MissingFunc) { return false; },
    [] (MissingMethod) { return false; },
    [] (const Isect& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_static->m_maybeReified) return false;
      }
      return true;
    },
    [] (const Isect2& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_maybeReified) return false;
      }
      return true;
    }
  );
}

bool Func::mightCareAboutDynCalls() const {
  if (Cfg::Eval::NoticeOnBuiltinDynamicCalls && mightBeBuiltin()) {
    return true;
  }
  auto const mightCareAboutFuncs =
    Cfg::Eval::ForbidDynamicCallsToFunc > 0;
  auto const mightCareAboutInstMeth =
    Cfg::Eval::ForbidDynamicCallsToInstMeth > 0;
  auto const mightCareAboutClsMeth =
    Cfg::Eval::ForbidDynamicCallsToClsMeth > 0;

  return match<bool>(
    val,
    [&] (FuncName) { return mightCareAboutFuncs; },
    [&] (MethodName) {
      return mightCareAboutClsMeth || mightCareAboutInstMeth;
    },
    [&] (Fun f) {
      return dyn_call_error_level(f.finfo->func) > 0;
    },
    [&] (Fun2 f) {
      return dyn_call_error_level(f.finfo->func) > 0;
    },
    [&] (Method m)  { return dyn_call_error_level(m.finfo->func) > 0; },
    [&] (Method2 m) { return dyn_call_error_level(m.finfo->func) > 0; },
    [&] (MethodFamily fa) {
      return
        fa.family->infoFor(fa.regularOnly).m_static->m_maybeCaresAboutDynCalls;
    },
    [&] (MethodFamily2 fa) {
      return
        fa.family->infoFor(fa.regularOnly).m_maybeCaresAboutDynCalls;
    },
    [&] (MethodOrMissing m)  { return dyn_call_error_level(m.finfo->func) > 0; },
    [&] (MethodOrMissing2 m) { return dyn_call_error_level(m.finfo->func) > 0; },
    [&] (MissingFunc m) { return false; },
    [&] (MissingMethod m) { return false; },
    [&] (const Isect& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_static->m_maybeCaresAboutDynCalls) {
          return false;
        }
      }
      return true;
    },
    [&] (const Isect2& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_maybeCaresAboutDynCalls) {
          return false;
        }
      }
      return true;
    }
  );
}

bool Func::mightBeBuiltin() const {
  return match<bool>(
    val,
    [] (FuncName s) { return true; },
    [] (MethodName) { return true; },
    [] (Fun f) { return f.finfo->func->attrs & AttrBuiltin; },
    [] (Fun2 f) { return f.finfo->func->attrs & AttrBuiltin; },
    [] (Method m) { return m.finfo->func->attrs & AttrBuiltin; },
    [] (Method2 m) { return m.finfo->func->attrs & AttrBuiltin; },
    [] (MethodFamily fa) {
      return fa.family->infoFor(fa.regularOnly).m_static->m_maybeBuiltin;
    },
    [] (MethodFamily2 fa) {
      return fa.family->infoFor(fa.regularOnly).m_maybeBuiltin;
    },
    [] (MethodOrMissing m) { return m.finfo->func->attrs & AttrBuiltin; },
    [] (MethodOrMissing2 m) { return m.finfo->func->attrs & AttrBuiltin; },
    [] (MissingFunc m) { return false; },
    [] (MissingMethod m) { return false; },
    [] (const Isect& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_static->m_maybeBuiltin) return false;
      }
      return true;
    },
    [] (const Isect2& i) {
      for (auto const ff : i.families) {
        if (!ff->infoFor(i.regularOnly).m_maybeBuiltin) return false;
      }
      return true;
    }
  );
}

uint32_t Func::minNonVariadicParams() const {
  return match<uint32_t>(
    val,
    [] (FuncName) { return 0; },
    [] (MethodName) { return 0; },
    [] (Fun f) { return numNVArgs(*f.finfo->func); },
    [] (Fun2 f) { return numNVArgs(*f.finfo->func); },
    [] (Method m) { return numNVArgs(*m.finfo->func); },
    [] (Method2 m) { return numNVArgs(*m.finfo->func); },
    [] (MethodFamily fa) {
      return
        fa.family->infoFor(fa.regularOnly).m_static->m_minNonVariadicParams;
    },
    [&] (MethodFamily2 fa) {
      return fa.family->infoFor(fa.regularOnly).m_minNonVariadicParams;
    },
    [] (MethodOrMissing m) { return numNVArgs(*m.finfo->func); },
    [] (MethodOrMissing2 m) { return numNVArgs(*m.finfo->func); },
    [] (MissingFunc)   { return 0; },
    [] (MissingMethod) { return 0; },
    [] (const Isect& i) {
      uint32_t nv = 0;
      for (auto const ff : i.families) {
        nv = std::max(
          nv,
          ff->infoFor(i.regularOnly).m_static->m_minNonVariadicParams
        );
      }
      return nv;
    },
    [] (const Isect2& i) {
      uint32_t nv = 0;
      for (auto const ff : i.families) {
        nv = std::max(
          nv,
          ff->infoFor(i.regularOnly).m_minNonVariadicParams
        );
      }
      return nv;
    }
  );
}

uint32_t Func::maxNonVariadicParams() const {
  return match<uint32_t>(
    val,
    [] (FuncName) { return std::numeric_limits<uint32_t>::max(); },
    [] (MethodName) { return std::numeric_limits<uint32_t>::max(); },
    [] (Fun f) { return numNVArgs(*f.finfo->func); },
    [] (Fun2 f) { return numNVArgs(*f.finfo->func); },
    [] (Method m) { return numNVArgs(*m.finfo->func); },
    [] (Method2 m) { return numNVArgs(*m.finfo->func); },
    [] (MethodFamily fa) {
      return
        fa.family->infoFor(fa.regularOnly).m_static->m_maxNonVariadicParams;
    },
    [] (MethodFamily2 fa) {
      return fa.family->infoFor(fa.regularOnly).m_maxNonVariadicParams;
    },
    [] (MethodOrMissing m) { return numNVArgs(*m.finfo->func); },
    [] (MethodOrMissing2 m) { return numNVArgs(*m.finfo->func); },
    [] (MissingFunc) { return 0; },
    [] (MissingMethod) { return 0; },
    [] (const Isect& i) {
      auto nv = std::numeric_limits<uint32_t>::max();
      for (auto const ff : i.families) {
        nv = std::min(
          nv,
          ff->infoFor(i.regularOnly).m_static->m_maxNonVariadicParams
        );
      }
      return nv;
    },
    [] (const Isect2& i) {
      auto nv = std::numeric_limits<uint32_t>::max();
      for (auto const ff : i.families) {
        nv = std::min(
          nv,
          ff->infoFor(i.regularOnly).m_maxNonVariadicParams
        );
      }
      return nv;
    }
  );
}

const RuntimeCoeffects* Func::requiredCoeffects() const {
  return match<const RuntimeCoeffects*>(
    val,
    [] (FuncName) { return nullptr; },
    [] (MethodName) { return nullptr; },
    [] (Fun f) { return &f.finfo->func->requiredCoeffects; },
    [] (Fun2 f) { return &f.finfo->func->requiredCoeffects; },
    [] (Method m) { return &m.finfo->func->requiredCoeffects; },
    [] (Method2 m) { return &m.finfo->func->requiredCoeffects; },
    [] (MethodFamily fa) {
      return fa.family->infoFor(fa.regularOnly)
        .m_static->m_requiredCoeffects.get_pointer();
    },
    [] (MethodFamily2 fa) {
      return
        fa.family->infoFor(fa.regularOnly).m_requiredCoeffects.get_pointer();
    },
    [] (MethodOrMissing m) { return &m.finfo->func->requiredCoeffects; },
    [] (MethodOrMissing2 m) { return &m.finfo->func->requiredCoeffects; },
    [] (MissingFunc) { return nullptr; },
    [] (MissingMethod) { return nullptr; },
    [] (const Isect& i) {
      const RuntimeCoeffects* coeffects = nullptr;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (!info.m_requiredCoeffects) continue;
        assertx(IMPLIES(coeffects, *coeffects == *info.m_requiredCoeffects));
        if (!coeffects) coeffects = info.m_requiredCoeffects.get_pointer();
      }
      return coeffects;
    },
    [] (const Isect2& i) {
      const RuntimeCoeffects* coeffects = nullptr;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (!info.m_requiredCoeffects) continue;
        assertx(IMPLIES(coeffects, *coeffects == *info.m_requiredCoeffects));
        if (!coeffects) coeffects = info.m_requiredCoeffects.get_pointer();
      }
      return coeffects;
    }
  );
}

const CompactVector<CoeffectRule>* Func::coeffectRules() const {
  return match<const CompactVector<CoeffectRule>*>(
    val,
    [] (FuncName) { return nullptr; },
    [] (MethodName) { return nullptr; },
    [] (Fun f) { return &f.finfo->func->coeffectRules; },
    [] (Fun2 f) { return &f.finfo->func->coeffectRules; },
    [] (Method m) { return &m.finfo->func->coeffectRules; },
    [] (Method2 m) { return &m.finfo->func->coeffectRules; },
    [] (MethodFamily fa) {
      return fa.family->infoFor(fa.regularOnly)
        .m_static->m_coeffectRules.get_pointer();
    },
    [] (MethodFamily2 fa) {
      return fa.family->infoFor(fa.regularOnly).m_coeffectRules.get_pointer();
    },
    [] (MethodOrMissing m) { return &m.finfo->func->coeffectRules; },
    [] (MethodOrMissing2 m) { return &m.finfo->func->coeffectRules; },
    [] (MissingFunc) { return nullptr; },
    [] (MissingMethod) { return nullptr; },
    [] (const Isect& i) {
      const CompactVector<CoeffectRule>* coeffects = nullptr;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (!info.m_coeffectRules) continue;
        assertx(
          IMPLIES(
            coeffects,
            std::is_permutation(
              begin(*coeffects),
              end(*coeffects),
              begin(*info.m_coeffectRules),
              end(*info.m_coeffectRules)
            )
          )
        );
        if (!coeffects) coeffects = info.m_coeffectRules.get_pointer();
      }
      return coeffects;
    },
    [] (const Isect2& i) {
      const CompactVector<CoeffectRule>* coeffects = nullptr;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (!info.m_coeffectRules) continue;
        assertx(
          IMPLIES(
            coeffects,
            std::is_permutation(
              begin(*coeffects),
              end(*coeffects),
              begin(*info.m_coeffectRules),
              end(*info.m_coeffectRules)
            )
          )
        );
        if (!coeffects) coeffects = info.m_coeffectRules.get_pointer();
      }
      return coeffects;
    }
  );
}

TriBool Func::supportsAsyncEagerReturn() const {
  return match<TriBool>(
    val,
    [] (FuncName)   { return TriBool::Maybe; },
    [] (MethodName) { return TriBool::Maybe; },
    [] (Fun f)      { return yesOrNo(func_supports_AER(f.finfo->func)); },
    [] (Fun2 f)     { return yesOrNo(func_supports_AER(f.finfo->func)); },
    [] (Method m)   { return yesOrNo(func_supports_AER(m.finfo->func)); },
    [] (Method2 m)  { return yesOrNo(func_supports_AER(m.finfo->func)); },
    [] (MethodFamily fam) {
      return fam.family->infoFor(fam.regularOnly).m_static->m_supportsAER;
    },
    [] (MethodFamily2 fam) {
      return fam.family->infoFor(fam.regularOnly).m_supportsAER;
    },
    [] (MethodOrMissing m)  {
      return yesOrNo(func_supports_AER(m.finfo->func));
    },
    [] (MethodOrMissing2 m) {
      return yesOrNo(func_supports_AER(m.finfo->func));
    },
    [] (MissingFunc) { return TriBool::No; },
    [] (MissingMethod) { return TriBool::No; },
    [] (const Isect& i) {
      auto aer = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (info.m_supportsAER == TriBool::Maybe) continue;
        assertx(IMPLIES(aer != TriBool::Maybe, aer == info.m_supportsAER));
        if (aer == TriBool::Maybe) aer = info.m_supportsAER;
      }
      return aer;
    },
    [] (const Isect2& i) {
      auto aer = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (info.m_supportsAER == TriBool::Maybe) continue;
        assertx(IMPLIES(aer != TriBool::Maybe, aer == info.m_supportsAER));
        if (aer == TriBool::Maybe) aer = info.m_supportsAER;
      }
      return aer;
    }
  );
}

Optional<uint32_t> Func::lookupNumInoutParams() const {
  return match<Optional<uint32_t>>(
    val,
    [] (FuncName s)   -> Optional<uint32_t> { return std::nullopt; },
    [] (MethodName s) -> Optional<uint32_t> { return std::nullopt; },
    [] (Fun f)     { return func_num_inout(f.finfo->func); },
    [] (Fun2 f)    { return func_num_inout(f.finfo->func); },
    [] (Method m)  { return func_num_inout(m.finfo->func); },
    [] (Method2 m) { return func_num_inout(m.finfo->func); },
    [] (MethodFamily fam) {
      return fam.family->infoFor(fam.regularOnly).m_static->m_numInOut;
    },
    [] (MethodFamily2 fam) {
      return fam.family->infoFor(fam.regularOnly).m_numInOut;
    },
    [] (MethodOrMissing m)  { return func_num_inout(m.finfo->func); },
    [] (MethodOrMissing2 m) { return func_num_inout(m.finfo->func); },
    [] (MissingFunc)        { return 0; },
    [] (MissingMethod)      { return 0; },
    [] (const Isect& i) {
      Optional<uint32_t> numInOut;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (!info.m_numInOut) continue;
        assertx(IMPLIES(numInOut, *numInOut == *info.m_numInOut));
        if (!numInOut) numInOut = info.m_numInOut;
      }
      return numInOut;
    },
    [] (const Isect2& i) {
      Optional<uint32_t> numInOut;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (!info.m_numInOut) continue;
        assertx(IMPLIES(numInOut, *numInOut == *info.m_numInOut));
        if (!numInOut) numInOut = info.m_numInOut;
      }
      return numInOut;
    }
  );
}

PrepKind Func::lookupParamPrep(uint32_t paramId) const {
  auto const fromFuncFamily = [&] (FuncFamily* ff, bool regularOnly) {
    auto const& info = *ff->infoFor(regularOnly).m_static;
    if (paramId >= info.m_paramPreps.size()) {
      return PrepKind{TriBool::No, TriBool::No};
    }
    return info.m_paramPreps[paramId];
  };
  auto const fromFuncFamily2 = [&] (const FuncFamily2* ff, bool regularOnly) {
    auto const& info = ff->infoFor(regularOnly);
    if (paramId >= info.m_paramPreps.size()) {
      return PrepKind{TriBool::No, TriBool::No};
    }
    return info.m_paramPreps[paramId];
  };

  return match<PrepKind>(
    val,
    [&] (FuncName s)         { return PrepKind{TriBool::Maybe, TriBool::Maybe}; },
    [&] (MethodName s)       { return PrepKind{TriBool::Maybe, TriBool::Maybe}; },
    [&] (Fun f)              { return func_param_prep(f.finfo->func, paramId); },
    [&] (Fun2 f)             { return func_param_prep(f.finfo->func, paramId); },
    [&] (Method m)           { return func_param_prep(m.finfo->func, paramId); },
    [&] (Method2 m)          { return func_param_prep(m.finfo->func, paramId); },
    [&] (MethodFamily f)     { return fromFuncFamily(f.family, f.regularOnly); },
    [&] (MethodFamily2 f)    { return fromFuncFamily2(f.family, f.regularOnly); },
    [&] (MethodOrMissing m)  { return func_param_prep(m.finfo->func, paramId); },
    [&] (MethodOrMissing2 m) { return func_param_prep(m.finfo->func, paramId); },
    [&] (MissingFunc)        { return PrepKind{TriBool::No, TriBool::Yes}; },
    [&] (MissingMethod)      { return PrepKind{TriBool::No, TriBool::Yes}; },
    [&] (const Isect& i) {
      auto inOut = TriBool::Maybe;
      auto readonly = TriBool::Maybe;

      for (auto const ff : i.families) {
        auto const prepKind = fromFuncFamily(ff, i.regularOnly);
        if (prepKind.inOut != TriBool::Maybe) {
          assertx(IMPLIES(inOut != TriBool::Maybe, inOut == prepKind.inOut));
          if (inOut == TriBool::Maybe) inOut = prepKind.inOut;
        }

        if (prepKind.readonly != TriBool::Maybe) {
          assertx(
            IMPLIES(readonly != TriBool::Maybe, readonly == prepKind.readonly)
          );
          if (readonly == TriBool::Maybe) readonly = prepKind.readonly;
        }
      }

      return PrepKind{inOut, readonly};
    },
    [&] (const Isect2& i) {
      auto inOut = TriBool::Maybe;
      auto readonly = TriBool::Maybe;

      for (auto const ff : i.families) {
        auto const prepKind = fromFuncFamily2(ff, i.regularOnly);
        if (prepKind.inOut != TriBool::Maybe) {
          assertx(IMPLIES(inOut != TriBool::Maybe, inOut == prepKind.inOut));
          if (inOut == TriBool::Maybe) inOut = prepKind.inOut;
        }

        if (prepKind.readonly != TriBool::Maybe) {
          assertx(
            IMPLIES(readonly != TriBool::Maybe, readonly == prepKind.readonly)
          );
          if (readonly == TriBool::Maybe) readonly = prepKind.readonly;
        }
      }

      return PrepKind{inOut, readonly};
    }
  );
}

TriBool Func::lookupReturnReadonly() const {
  return match<TriBool>(
    val,
    [] (FuncName)     { return TriBool::Maybe; },
    [] (MethodName)   { return TriBool::Maybe; },
    [] (Fun f)        { return yesOrNo(f.finfo->func->isReadonlyReturn); },
    [] (Fun2 f)       { return yesOrNo(f.finfo->func->isReadonlyReturn); },
    [] (Method m)     { return yesOrNo(m.finfo->func->isReadonlyReturn); },
    [] (Method2 m)    { return yesOrNo(m.finfo->func->isReadonlyReturn); },
    [] (MethodFamily fam) {
      return fam.family->infoFor(fam.regularOnly).m_static->m_isReadonlyReturn;
    },
    [] (MethodFamily2 fam) {
      return fam.family->infoFor(fam.regularOnly).m_isReadonlyReturn;
    },
    [] (MethodOrMissing m)  { return yesOrNo(m.finfo->func->isReadonlyReturn); },
    [] (MethodOrMissing2 m) { return yesOrNo(m.finfo->func->isReadonlyReturn); },
    [] (MissingFunc)        { return TriBool::No; },
    [] (MissingMethod)      { return TriBool::No; },
    [] (const Isect& i) {
      auto readOnly = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (info.m_isReadonlyReturn == TriBool::Maybe) continue;
        assertx(IMPLIES(readOnly != TriBool::Maybe,
                        readOnly == info.m_isReadonlyReturn));
        if (readOnly == TriBool::Maybe) readOnly = info.m_isReadonlyReturn;
      }
      return readOnly;
    },
    [] (const Isect2& i) {
      auto readOnly = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (info.m_isReadonlyReturn == TriBool::Maybe) continue;
        assertx(IMPLIES(readOnly != TriBool::Maybe,
                        readOnly == info.m_isReadonlyReturn));
        if (readOnly == TriBool::Maybe) readOnly = info.m_isReadonlyReturn;
      }
      return readOnly;
    }
  );
}

TriBool Func::lookupReadonlyThis() const {
  return match<TriBool>(
    val,
    [] (FuncName s)   { return TriBool::Maybe; },
    [] (MethodName s) { return TriBool::Maybe; },
    [] (Fun f)        { return yesOrNo(f.finfo->func->isReadonlyThis); },
    [] (Fun2 f)       { return yesOrNo(f.finfo->func->isReadonlyThis); },
    [] (Method m)     { return yesOrNo(m.finfo->func->isReadonlyThis); },
    [] (Method2 m)    { return yesOrNo(m.finfo->func->isReadonlyThis); },
    [] (MethodFamily fam) {
      return fam.family->infoFor(fam.regularOnly).m_static->m_isReadonlyThis;
    },
    [] (MethodFamily2 fam) {
      return fam.family->infoFor(fam.regularOnly).m_isReadonlyThis;
    },
    [] (MethodOrMissing m)  { return yesOrNo(m.finfo->func->isReadonlyThis); },
    [] (MethodOrMissing2 m) { return yesOrNo(m.finfo->func->isReadonlyThis); },
    [] (MissingFunc)        { return TriBool::No; },
    [] (MissingMethod)      { return TriBool::No; },
    [] (const Isect& i) {
      auto readOnly = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = *ff->infoFor(i.regularOnly).m_static;
        if (info.m_isReadonlyThis == TriBool::Maybe) continue;
        assertx(IMPLIES(readOnly != TriBool::Maybe,
                        readOnly == info.m_isReadonlyThis));
        if (readOnly == TriBool::Maybe) readOnly = info.m_isReadonlyThis;
      }
      return readOnly;
    },
    [] (const Isect2& i) {
      auto readOnly = TriBool::Maybe;
      for (auto const ff : i.families) {
        auto const& info = ff->infoFor(i.regularOnly);
        if (info.m_isReadonlyThis == TriBool::Maybe) continue;
        assertx(IMPLIES(readOnly != TriBool::Maybe,
                        readOnly == info.m_isReadonlyThis));
        if (readOnly == TriBool::Maybe) readOnly = info.m_isReadonlyThis;
      }
      return readOnly;
    }
  );
}

Optional<SString> Func::triviallyWrappedFunc() const {
  auto const check = [](const php::Func* func) -> Optional<SString> {
    auto const it = func->userAttributes.find(s_TrivialHHVMBuiltinWrapper.get());
    if (it == func->userAttributes.end()) return std::nullopt;
    assertx(tvIsVec(it->second));
    auto const args = it->second.m_data.parr;
    if (args->size() != 1) return std::nullopt;
    auto const wrappedFunc = args->at(int64_t{0});
    if (!tvIsString(wrappedFunc)) return std::nullopt;
    assertx(wrappedFunc.m_data.pstr->isStatic());
    return wrappedFunc.m_data.pstr;
  };
  return match<Optional<SString>>(
    val,
    [] (Func::FuncName)          { return std::nullopt; },
    [] (Func::MethodName)        { return std::nullopt; },
    [&] (Func::Fun f)            { return check(f.finfo->func); },
    [&] (Func::Fun2 f)           { return check(f.finfo->func); },
    [] (Func::Method)            { return std::nullopt; },
    [] (Func::Method2)           { return std::nullopt; },
    [] (Func::MethodFamily)      { return std::nullopt; },
    [] (Func::MethodFamily2)     { return std::nullopt; },
    [] (Func::MethodOrMissing)   { return std::nullopt; },
    [] (Func::MethodOrMissing2)  { return std::nullopt; },
    [] (Func::MissingFunc)       { return std::nullopt; },
    [] (Func::MissingMethod)     { return std::nullopt; },
    [] (const Func::Isect&)      { return std::nullopt; },
    [] (const Func::Isect2&)     { return std::nullopt; }
  );
}

std::string show(const Func& f) {
  auto ret = f.name();
  match(
    f.val,
    [&] (Func::FuncName)          {},
    [&] (Func::MethodName)        {},
    [&] (Func::Fun)               { ret += "*"; },
    [&] (Func::Fun2)              { ret += "*"; },
    [&] (Func::Method)            { ret += "*"; },
    [&] (Func::Method2)           { ret += "*"; },
    [&] (Func::MethodFamily)      { ret += "+"; },
    [&] (Func::MethodFamily2)     { ret += "+"; },
    [&] (Func::MethodOrMissing)   { ret += "-"; },
    [&] (Func::MethodOrMissing2)  { ret += "-"; },
    [&] (Func::MissingFunc)       { ret += "!"; },
    [&] (Func::MissingMethod)     { ret += "!"; },
    [&] (const Func::Isect&)      { ret += "&"; },
    [&] (const Func::Isect2&)     { ret += "&"; }
  );
  return ret;
}

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

struct Index::IndexData {
  explicit IndexData(Index* index) : m_index{index} {}
  IndexData(const IndexData&) = delete;
  IndexData& operator=(const IndexData&) = delete;

  Index* m_index;

  bool frozen{false};
  bool ever_frozen{false};

  // If non-nullptr, log information about each pass into it.
  StructuredLogEntry* sample;

  // Async state:
  std::unique_ptr<TicketExecutor> executor;
  std::unique_ptr<Client> client;
  DisposeCallback disposeClient;

  // Global configeration, stored in extern-worker.
  std::unique_ptr<CoroAsyncValue<Ref<Config>>> configRef;

  // Maps unit/class/func name to the extern-worker ref representing
  // php::Program data for that. Any associated bytecode is stored
  // separately.
  SStringToOneT<UniquePtrRef<php::Unit>>   unitRefs;
  TSStringToOneT<UniquePtrRef<php::Class>> classRefs;
  FSStringToOneT<UniquePtrRef<php::Func>>  funcRefs;

  // Maps class name to the extern-worker ref representing the class's
  // associated ClassInfo2. Only has entries for instantiable classes.
  TSStringToOneT<UniquePtrRef<ClassInfo2>> classInfoRefs;

  // Maps func name (global functions, not methods) to the
  // extern-worked ref representing the func's associated
  // FuncInfo2. The FuncInfo2 for methods are stored in their parent
  // ClassInfo2.
  FSStringToOneT<UniquePtrRef<FuncInfo2>> funcInfoRefs;

  // Maps class/func names to the extern-worker ref representing the
  // bytecode for that class or (global) function. The bytecode of all
  // of a class' methods are stored together.
  TSStringToOneT<UniquePtrRef<php::ClassBytecode>> classBytecodeRefs;
  FSStringToOneT<UniquePtrRef<php::FuncBytecode>> funcBytecodeRefs;

  // Uninstantiable classes do not have ClassInfo2s, but their methods
  // still have FuncInfo2s. Since we don't have a ClassInfo2 to store
  // them on, we do it separately.
  TSStringToOneT<UniquePtrRef<MethodsWithoutCInfo>> uninstantiableClsMethRefs;

  // Func family entries representing all methods with a particular
  // name.
  SStringToOneT<FuncFamilyEntry> nameOnlyMethodFamilies;

  // Maps func-family ids to the func family group which contains the
  // func family with that id.
  hphp_fast_map<FuncFamily2::Id, Ref<FuncFamilyGroup>> funcFamilyRefs;

  // Maps of functions and classes to the names of closures defined
  // within.
  TSStringToOneT<TSStringSet> classToClosures;
  FSStringToOneT<TSStringSet> funcToClosures;

  // Maps a closure to it's declaring class or function.
  TSStringToOneT<SString> closureToClass;
  TSStringToOneT<SString> closureToFunc;

  // Maps entities to the unit they were declared in.
  TSStringToOneT<SString> classToUnit;
  FSStringToOneT<SString> funcToUnit;
  TSStringToOneT<SString> typeAliasToUnit;
  // If bool is true, then the constant is "dynamic" and has an
  // associated 86cinit function.
  SStringToOneT<std::pair<SString, bool>> constantToUnit;

  // Maps a class to the classes which it has inherited class
  // constants from.
  TSStringToOneT<TSStringSet> classToCnsBases;

  // Maps an unit to it's predeps, whether just in cinits or
  // everywhere.
  SStringToOneT<SStringSet> unitCInitPredeps;
  SStringToOneT<SStringSet> unitPredeps;

  SStringToOneT<UniquePtrRef<ClassBundle>> bundleRefs;

  // Maps entities to the bundle they belong to.
  TSStringToOneT<SString> classToBundle;
  FSStringToOneT<SString> funcToBundle;
  SStringToOneT<SString> unitToBundle;

  // All the classes that have a 86*init function.
  TSStringSet classesWith86Inits;
  // All the 86cinit functions for "dynamic" top-level constants.
  FSStringSet constantInitFuncs;
  // All the units that have type-aliases within them.
  SStringSet unitsWithTypeAliases;
  // All classes which should be analyzed (all classes except closures
  // declared within another class).
  TSStringSet allClassesToAnalyze;
  // All top-level functions
  FSStringSet allFuncs;
  // All units
  SStringSet allUnits;

  // Maps a class to any methods which must be analyzed as part of
  // that class.
  TSStringToOneT<MethRefSet> extraMethods;

  // Maps an unit to all the originalUnit fields of funcs in that
  // unit.
  SStringToOneT<SStringSet> unitToOriginalUnits;

  std::unique_ptr<php::Program> program;

  TSStringToOneT<php::Class*>      classes;
  FSStringToOneT<php::Func*>       funcs;
  TSStringToOneT<php::TypeAlias*>  typeAliases;
  TSStringToOneT<php::Class*>      enums;
  SStringToOneT<php::Constant*>    constants;
  SStringToOneT<php::Module*>      modules;
  SStringToOneT<php::Unit*>        units;

  /*
   * Func families representing methods with a particular name (across
   * all classes).
   */
  struct MethodFamilyEntry {
    FuncFamilyOrSingle m_all;
    FuncFamilyOrSingle m_regular;
  };
  SStringToOneT<MethodFamilyEntry> methodFamilies;

  hphp_fast_map<
    const php::Class*,
    hphp_fast_set<const php::Func*>
  > classExtraMethodMap;

  /*
   * Map from each class name to ClassInfo objects if one exists.
   *
   * It may not exists if we would fatal when defining the class. That could
   * happen for if the inheritance is bad or __Sealed or other things.
   */
  TSStringToOneT<ClassInfo*> classInfo;

  /*
   * All the ClassInfos, stored in no particular order.
   */
  std::vector<std::unique_ptr<ClassInfo>> allClassInfos;

  std::vector<FuncInfo> funcInfo;
  std::atomic<uint32_t> nextFuncId{};

  // Private instance and static property types are stored separately
  // from ClassInfo, because you don't need to resolve a class to get
  // at them.
  hphp_hash_map<
    const php::Class*,
    PropState
  > privatePropInfo;
  hphp_hash_map<
    const php::Class*,
    PropState
  > privateStaticPropInfo;

  /*
   * Public static property information:
   */

  // If this is true, we've seen mutations to public static
  // properties. Once this is true, it's no longer legal to report a
  // pessimistic static property set (unknown class and
  // property). Doing so is a monotonicity violation.
  bool seenPublicSPropMutations{false};

  // The set of gathered public static property mutations for each function. The
  // inferred types for the public static properties is the union of all these
  // mutations. If a function is not analyzed in a particular analysis round,
  // its mutations are left unchanged from the previous round.
  folly_concurrent_hash_map_simd<
    const php::Func*,
    PublicSPropMutations,
    pointer_hash<const php::Func>> publicSPropMutations;

  // All FuncFamilies. These are stored globally so we can avoid
  // generating duplicates.
  folly_concurrent_hash_map_simd<
    std::unique_ptr<FuncFamily>,
    bool,
    FuncFamilyPtrHasher,
    FuncFamilyPtrEquals
  > funcFamilies;

  folly_concurrent_hash_map_simd<
    std::unique_ptr<FuncFamily::StaticInfo>,
    bool,
    FFStaticInfoPtrHasher,
    FFStaticInfoPtrEquals
  > funcFamilyStaticInfos;

  /*
   * Map from interfaces to their assigned vtable slots, computed in
   * compute_iface_vtables().
   */
  TSStringToOneT<Slot> ifaceSlotMap;

  hphp_hash_map<
    const php::Class*,
    CompactVector<Type>
  > closureUseVars;

  struct ClsConstTypesHasher {
    bool operator()(const std::pair<const php::Class*, SString>& k) const {
      return folly::hash::hash_combine(
        pointer_hash<php::Class>{}(k.first),
        pointer_hash<StringData>{}(k.second)
      );
    }
  };
  struct ClsConstTypesEquals {
    bool operator()(const std::pair<const php::Class*, SString>& a,
                    const std::pair<const php::Class*, SString>& b) const {
      return a.first == b.first && a.second == b.second;
    }
  };

  // Cache for lookup_class_constant
  folly_concurrent_hash_map_simd<
    std::pair<const php::Class*, SString>,
    ClsConstLookupResult,
    ClsConstTypesHasher,
    ClsConstTypesEquals
  > clsConstLookupCache;

  bool useClassDependencies{};
  DepMap dependencyMap;

  /*
   * If a function is effect-free when called with a particular set of
   * literal arguments, and produces a literal result, there will be
   * an entry here representing the type.
   *
   * The map isn't just an optimization; we can't call
   * analyze_func_inline during the optimization phase, because the
   * bytecode could be modified while we do so.
   */
  ContextRetTyMap foldableReturnTypeMap;

  /*
   * Call-context sensitive return types are cached here.  This is not
   * an optimization.
   *
   * The reason we need to retain this information about the
   * calling-context-sensitive return types is that once the Index is
   * frozen (during the final optimization pass), calls to
   * lookup_return_type with a CallContext can't look at the bytecode
   * bodies of functions other than the calling function.  So we need
   * to know what we determined the last time we were allowed to do
   * that so we can return it again.
   */
  ContextRetTyMap contextualReturnTypes{};
};

//////////////////////////////////////////////////////////////////////

namespace { struct DepTracker; }

struct AnalysisIndex::IndexData {
  IndexData(AnalysisIndex& index,
            AnalysisWorklist& worklist,
            Mode mode)
    : index{index}
    , worklist{worklist}
    , deps{std::make_unique<DepTracker>(*this)}
    , mode{mode} {}

  IndexData(const IndexData&) = delete;
  IndexData& operator=(const IndexData&) = delete;

  AnalysisIndex& index;
  AnalysisWorklist& worklist;
  std::unique_ptr<DepTracker> deps;

  // Maps names to various data-structures. This works for both normal
  // classes and closures.
  TSStringToOneT<php::Class*> classes;
  TSStringToOneT<ClassInfo2*> cinfos;
  TSStringToOneT<MethodsWithoutCInfo*> minfos;

  FSStringToOneT<php::Func*> funcs;
  FSStringToOneT<FuncInfo2*> finfos;

  SStringToOneT<php::Unit*> units;

  SStringToOneT<std::pair<php::Constant*, php::Unit*>> constants;
  TSStringToOneT<std::pair<php::TypeAlias*, php::Unit*>> typeAliases;

  std::vector<FuncInfo2*> finfosByIdx;

  hphp_fast_map<const php::Func*,
                hphp_fast_set<const php::Class*>> funcToClosures;

  // Anything on these lists is known to definitely not exist.
  TSStringSet badClasses;
  FSStringSet badFuncs;
  SStringSet badConstants;

  SStringSet dynamicConstants;

  TSStringToOneT<SString> classToBundle;
  FSStringToOneT<SString> funcToBundle;
  SStringToOneT<SString> unitToBundle;

  // Maps interface names to their vtable slot. Only populated during
  // final pass.
  TSStringToOneT<Slot> ifaceSlotMap;

  hphp_fast_set<FuncClsUnit, FuncClsUnitHasher> toReport;
  SStringSet reportBundleNames;

  std::vector<std::unique_ptr<ClassBundle>> reportBundles;
  std::vector<std::unique_ptr<ClassBundle>> noReportBundles;

  // AnalysisIndex maintains a stack of the contexts being analyzed
  // (we can have multiple because of inline interp).
  std::vector<Context> contexts;

  // If we're currently resolving class type constants. This changes
  // how some of the dependencies are treated.
  bool inTypeCns{false};

  size_t foldableInterpNestingLevel{0};
  size_t contextualInterpNestingLevel{0};

  // The bucket id which AnalysisScheduler assigned to this worker.
  size_t bucketIdx;

  // Once the index is frozen, no further updates to it are allowed
  // (will assert). We only gather dependencies when the index is
  // frozen.
  bool frozen{false};

  // The type of analysis that we're doing.
  Mode mode;

  int traceBump{0};

  // We'll unserialize the same things over and over, so cache the
  // results.
  struct UnserializeKey {
    FuncClsUnit context;
    Type type;
    bool operator==(const UnserializeKey& o) const {
      return context == o.context && equal(type, o.type);
    }
    struct Hasher {
      size_t operator()(const UnserializeKey& k) const {
        return folly::hash::hash_combine(
          k.context.hash(),
          k.type.hash()
        );
      }
    };
  };
  hphp_fast_map<UnserializeKey, Type, UnserializeKey::Hasher> unserializeCache;
};

//////////////////////////////////////////////////////////////////////

namespace {

//////////////////////////////////////////////////////////////////////

// Obtain the current (most recently pushed) context. This corresponds
// to the context currently being analyzed.
const Context& current_context(const AnalysisIndex::IndexData& index) {
  always_assert_flog(
    !index.contexts.empty(),
    "Accessing current context without any contexts active"
  );
  return index.contexts.back();
}

// Obtain the context to use for the purposes of dependency
// tracking. This is the first context pushed. This differs from
// current_context() because of inline interp. If we're inline
// interp-ing a function, we still want to attribute the dependencies
// to the context which started the inline interp.
const Context& context_for_deps(const AnalysisIndex::IndexData& index) {
  always_assert_flog(
    !index.contexts.empty(),
    "Accessing dependency context without any contexts active"
  );
  return index.contexts.front();
}

const Context& first_func_context(const AnalysisIndex::IndexData& index) {
  for (auto const& ctx : index.contexts) {
    if (!ctx.func) continue;
    assertx(context_for_deps(index).cls == ctx.cls);
    return ctx;
  }
  always_assert_flog(
    false,
    "Unable to find any active context with a function"
  );
}

//////////////////////////////////////////////////////////////////////

FuncClsUnit fc_from_context(const Context& ctx,
                            const AnalysisIndex::IndexData& index) {
  if (ctx.cls) {
    // If this is a closure, the context is the closure's declaring
    // class or function.
    if (ctx.cls->closureContextCls) {
      auto const c =
        folly::get_default(index.classes, ctx.cls->closureContextCls);
      always_assert(c);
      return c;
    }
    if (ctx.cls->closureDeclFunc) {
      auto const f = folly::get_default(index.funcs, ctx.cls->closureDeclFunc);
      always_assert(f);
      return f;
    }
    return ctx.cls;
  }
  if (ctx.func) return ctx.func;
  assertx(ctx.unit);
  return &index.index.lookup_unit(ctx.unit);
}

//////////////////////////////////////////////////////////////////////

Trace::Bump bump_for_class(const Index::IndexData& index, SString c) {
  if constexpr (Trace::enabled) {
    return Trace::Bump{
      Trace::hhbbc_index,
      trace_bump_for(c, nullptr, folly::get_default(index.classToUnit, c))
    };
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

Trace::Bump bump_for_func(const Index::IndexData& index, SString f) {
  if constexpr (Trace::enabled) {
    return Trace::Bump{
      Trace::hhbbc_index,
      trace_bump_for(nullptr, f, folly::get_default(index.funcToUnit, f))
    };
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

Trace::Bump bump_for_unit(const Index::IndexData&, SString u) {
  if constexpr (Trace::enabled) {
    return Trace::Bump{Trace::hhbbc_index, trace_bump_for(nullptr, nullptr, u)};
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

Trace::Bump bump_for_constant(const Index::IndexData& index, SString cns) {
  if constexpr (Trace::enabled) {
    return Trace::Bump{
      Trace::hhbbc_index,
      [&] {
        if (auto const p = folly::get_ptr(index.constantToUnit, cns)) {
          return trace_bump_for(nullptr, nullptr, p->first);
        }
        return 0;
      }()
    };
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

//////////////////////////////////////////////////////////////////////

// Record the dependencies of all classes and functions being
// processed with an AnalysisIndex. These dependencies will ultimately
// be reported back to the master AnalysisScheduler, but will also
// drive the worklist on the local analysis job.
struct DepTracker {
  explicit DepTracker(const AnalysisIndex::IndexData& index)
    : index{index} {}

  using Type = AnalysisDeps::Type;
  using Func = AnalysisDeps::Func;
  using Class = AnalysisDeps::Class;
  using Constant = AnalysisDeps::Constant;
  using AnyClassConstant = AnalysisDeps::AnyClassConstant;
  using Property = AnalysisDeps::Property;
  using AnyProperty = AnalysisDeps::AnyProperty;

  // Register dependencies on various entities to the current
  // dependency context.

  void add(Class c, Type t = Type::Meta) {
    assertx(t != Type::None);

    auto const fc = context();
    if (index.frozen) {
      if (auto const c2 = fc.cls()) {
        if (c2->name->tsame(c.name)) return;
      }

      if (auto const DEBUG_ONLY added = deps[fc].add(c, t)) {
        FTRACE(
          2, "{} now depends on class {}{}\n",
          HHBBC::show(fc),
          displayAdded(added),
          c.name
        );
      }
    } else if (auto const cls = folly::get_default(index.classes, c.name)) {
      // Record dependency for worklist if anything can change within
      // the job.
      t &= AnalysisDeps::kValidForChanges;
      if (t != Type::None) classes[cls][fc] |= t;
    }
  }

  void add(const php::Func& f, Type t = Type::Meta) {
    assertx(t != Type::None);
    auto const fc = context();
    assertx(!fc.unit());
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (f.cls && c->name->tsame(f.cls->name)) return;
      } else if (auto const f2 = fc.func()) {
        if (!f.cls && f2->name->fsame(f.name)) return;
      }

      if (auto const DEBUG_ONLY added = deps[fc].add(f, t)) {
        FTRACE(
          2, "{} now depends on {}{} {}\n",
          HHBBC::show(fc), displayAdded(added),
          f.cls ? "method" : "func",
          func_fullname(f)
        );
      }
    } else {
      // Record dependency for worklist if anything can change within
      // the job.
      t &= AnalysisDeps::kValidForChanges;
      if (t != Type::None) funcs[&f][fc] |= t;
    }
  }

  void add(MethRef m, Type t = Type::Meta) {
    assertx(t != Type::None);
    auto const fc = context();
    assertx(!fc.unit());
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (c->name->tsame(m.cls)) return;
      }
      if (auto const DEBUG_ONLY added = deps[fc].add(m, t)) {
        FTRACE(2, "{} now depends on {}method {}\n",
               HHBBC::show(fc), displayAdded(added), display(m));
      }
    } else {
      // Record dependency for worklist if anything can change within
      // the job.
      t &= AnalysisDeps::kValidForChanges;
      if (t == Type::None) return;
      if (auto const p = from(m)) {
        FTRACE(2, "{} now depends on {} ({})\n",
               HHBBC::show(fc), func_fullname(*p), show(t));
        funcs[p][fc] |= t;
      }
    }
  }

  void add(Func f, Type t = Type::Meta) {
    assertx(t != Type::None);
    auto const fc = context();
    assertx(!fc.unit());
    if (index.frozen) {
      if (auto const f2 = fc.func()) {
        if (f2->name->fsame(f.name)) return;
      }
      if (auto const DEBUG_ONLY added = deps[fc].add(f, t)) {
        FTRACE(2, "{} now depends on {}func {}\n",
               HHBBC::show(fc), displayAdded(added), f.name);
      }
    } else {
      // Record dependency for worklist if anything can change within
      // the job.
      t &= AnalysisDeps::kValidForChanges;
      if (t == Type::None) return;
      if (auto const p = folly::get_default(index.funcs, f.name)) {
        funcs[p][fc] |= t;
      }
    }
  }

  void add(Property prop) {
    auto const fc = context();
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (c->name->tsame(prop.cls)) return;
      }
      if (deps[fc].add(prop)) {
        FTRACE(2, "{} now depends on property {}::{}\n",
               HHBBC::show(fc), prop.cls, prop.prop);
      }
    } else if (auto const cls = folly::get_default(index.classes, prop.cls)) {
      for (auto const& p : cls->properties) {
        if (p.name == prop.prop) {
          properties[&p].emplace(fc);
          break;
        }
      }
    }
  }

  void add(AnyProperty prop) {
    auto const fc = context();
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (c->name->tsame(prop.cls)) return;
      }
      if (deps[fc].add(prop)) {
        FTRACE(2, "{} now depends on any property from {}\n",
               HHBBC::show(fc), prop.cls);
      }
    } else if (auto const cls = folly::get_default(index.classes, prop.cls)) {
      anyProperties[cls].emplace(fc);
    }
  }

  void add(ConstIndex cns) {
    auto const fc = context();
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (c->name->tsame(cns.cls)) return;
      }
      if (deps[fc].add(cns, index.inTypeCns)) {
        FTRACE(2, "{} now depends on class constant {}{}\n",
               HHBBC::show(fc), display(cns),
               index.inTypeCns ? " (in type-cns)" : "");
      }
    } else if (auto const p = from(cns)) {
      clsConstants[p].emplace(fc);
    }
  }

  void add(AnyClassConstant cns) {
    auto const fc = context();
    if (index.frozen) {
      if (auto const c = fc.cls()) {
        if (c->name->tsame(cns.name)) return;
      }
      if (deps[fc].add(cns, index.inTypeCns)) {
        FTRACE(2, "{} now depends on any class constant from {}{}\n",
               HHBBC::show(fc), cns.name,
               index.inTypeCns ? " (in type-cns)" : "");
      }
    } else if (auto const cls = folly::get_default(index.classes, cns.name)) {
      anyClsConstants[cls].emplace(fc);
    }
  }

  void add(Constant cns) {
    auto const fc = context();
    assertx(!fc.unit());
    if (index.frozen) {
      if (deps[fc].add(cns)) {
        FTRACE(2, "{} now depends on constant {}\n", HHBBC::show(fc), cns.name);
      }
    } else if (auto const p = folly::get_ptr(index.constants, cns.name)) {
      constants[p->first].emplace(fc);
    }
  }

  // Mark that the given entity has changed in some way. This not only
  // results in the change being reported back to the
  // AnalysisScheduler, but will reschedule any work locally which has
  // a dependency.

  void update(const php::Class& c, Type t) {
    if (t == Type::None) return;
    assertx(AnalysisDeps::isValidForChanges(t));
    assertx(!index.frozen);
    FTRACE(
      2, "class {} {} changed, scheduling\n",
      c.name, show(t)
    );
    changes.changed(c, t);
    schedule(folly::get_ptr(classes, &c), t);
  }

  void update(const php::Func& f, Type t) {
    if (t == Type::None) return;
    assertx(AnalysisDeps::isValidForChanges(t));
    assertx(!index.frozen);
    FTRACE(
      2, "{} {} {} changed, scheduling\n",
      f.cls ? "method" : "func",
      func_fullname(f),
      show(t)
    );
    changes.changed(f, t);
    schedule(folly::get_ptr(funcs, &f), t);
  }

  void update(const php::Const& cns, ConstIndex idx) {
    assertx(!index.frozen);
    FTRACE(2, "constant {}::{} changed, scheduling\n", idx.cls, cns.name);
    changes.changed(idx);
    schedule(folly::get_ptr(clsConstants, &cns));
    if (auto const p = folly::get_default(index.classes, idx.cls)) {
      schedule(folly::get_ptr(anyClsConstants, p));
    }
  }

  void update(const php::Class& cls, const php::Prop& prop) {
    assertx(!index.frozen);
    FTRACE(2, "property {}::{} changed, scheduling\n", cls.name, prop.name);
    changes.changed(cls, prop);
    schedule(folly::get_ptr(properties, &prop));
    schedule(folly::get_ptr(anyProperties, &cls));
  }

  void update(const php::Constant& cns) {
    assertx(!index.frozen);
    FTRACE(2, "constant {} changed, scheduling\n", cns.name);
    changes.changed(cns);
    schedule(folly::get_ptr(constants, &cns));
  }

  // Add pre-known dependencies directly.
  void preadd(FuncClsUnit fc, Func f, Type t) {
    assertx(!index.frozen);
    assertx(!fc.unit());
    if (t == Type::None) return;
    auto const p = folly::get_default(index.funcs, f.name);
    if (!p) return;
    FTRACE(2, "{} pre-depends on {}func {}\n",
           HHBBC::show(fc), displayAdded(t), f.name);
    funcs[p][fc] |= t;
  }

  void preadd(FuncClsUnit fc, MethRef m, Type t) {
    assertx(!index.frozen);
    assertx(!fc.unit());
    if (t == Type::None) return;
    auto const p = from(m);
    if (!p) return;
    FTRACE(2, "{} pre-depends on {}method {}\n",
           HHBBC::show(fc), displayAdded(t), display(m));
    funcs[p][fc] |= t;
  }

  void preadd(FuncClsUnit fc, ConstIndex cns) {
    assertx(!index.frozen);
    auto const p = from(cns);
    if (!p) return;
    FTRACE(2, "{} pre-depends on class constant from {}\n",
           HHBBC::show(fc), display(cns));
    clsConstants[p].emplace(fc);
  }

  void preadd(FuncClsUnit fc, AnyClassConstant cns) {
    assertx(!index.frozen);
    auto const p = folly::get_default(index.classes, cns.name);
    if (!p) return;
    FTRACE(2, "{} pre-depends on any class constant from {}\n",
           HHBBC::show(fc), cns.name);
    anyClsConstants[p].emplace(fc);
  }

  void preadd(FuncClsUnit fc, Constant cns) {
    assertx(!index.frozen);
    assertx(!fc.unit());
    auto const p = folly::get_ptr(index.constants, cns.name);
    if (!p) return;
    FTRACE(2, "{} pre-depends on constant {}\n", HHBBC::show(fc), cns.name);
    constants[p->first].emplace(fc);
  }

  void reset(FuncClsUnit fc) { deps.erase(fc); }

  AnalysisDeps take(FuncClsUnit fc) {
    auto it = deps.find(fc);
    if (it == end(deps)) return AnalysisDeps{};
    it->second.clean();
    return std::move(it->second);
  }

  const AnalysisDeps* get(FuncClsUnit fc) const {
    return folly::get_ptr(deps, fc);
  }

  AnalysisChangeSet& getChanges() { return changes; }
  const AnalysisChangeSet& getChanges() const { return changes; }

private:
  using Mode = AnalysisMode;

  // Return appropriate entity to attribute the dependency to. If
  // we're analyzing a function within a class, use the class. If it's
  // a top-level function, use that.
  FuncClsUnit context() const {
    return fc_from_context(context_for_deps(index), index);
  }

  const php::Func* from(MethRef m) const {
    if (auto const cls = folly::get_default(index.classes, m.cls)) {
      assertx(m.idx < cls->methods.size());
      return cls->methods[m.idx].get();
    }
    return nullptr;
  }

  const php::Const* from(ConstIndex cns) const {
    if (auto const cls = folly::get_default(index.classes, cns.cls)) {
      assertx(cns.idx < cls->constants.size());
      return &cls->constants[cns.idx];
    }
    return nullptr;
  }

  std::string display(MethRef m) const {
    if (auto const p = from(m)) return func_fullname(*p);
    return show(m);
  }

  std::string display(ConstIndex cns) const {
    if (auto const p = from(cns)) {
      return folly::sformat("{}::{} ({})", p->cls, p->name, cns.idx);
    }
    return show(cns, AnalysisIndexAdaptor { index.index });
  }

  static std::string displayAdded(Type t) {
    auto out = show(t - Type::Meta);
    if (!out.empty()) folly::format(&out, " of ");
    return out;
  }

  using FuncClsUnitSet =
    hphp_fast_set<FuncClsUnit, FuncClsUnitHasher>;
  using FuncClsUnitToType =
    hphp_fast_map<FuncClsUnit, Type, FuncClsUnitHasher>;

  void schedule(const FuncClsUnitSet* fcs) {
    if (!fcs || fcs->empty() || index.mode == Mode::Final) return;
    TinyVector<FuncClsUnit, 4> v;
    v.insert(begin(*fcs), end(*fcs));
    addToWorklist(v);
  }

  void schedule(const FuncClsUnitToType* fcs, Type t) {
    assertx(!(t & Type::Meta));
    if (!fcs || fcs->empty() || index.mode == Mode::Final) return;
    TinyVector<FuncClsUnit, 4> v;
    for (auto const [fc, t2] : *fcs) {
      if (t & t2) v.emplace_back(fc);
    }
    addToWorklist(v);
  }

  void addToWorklist(TinyVector<FuncClsUnit, 4>& fcs) {
    assertx(index.mode != Mode::Final);
    if (fcs.empty()) return;
    std::sort(
      fcs.begin(), fcs.end(),
      [] (FuncClsUnit fc1, FuncClsUnit fc2) { return fc1.stableLT(fc2); }
    );
    Trace::Indent _;
    for (auto const fc : fcs) index.worklist.schedule(fc);
  }

  const AnalysisIndex::IndexData& index;
  AnalysisChangeSet changes;
  hphp_fast_map<FuncClsUnit, AnalysisDeps, FuncClsUnitHasher> deps;

  hphp_fast_map<const php::Class*, FuncClsUnitToType> classes;
  hphp_fast_map<const php::Func*, FuncClsUnitToType> funcs;
  hphp_fast_map<const php::Const*, FuncClsUnitSet> clsConstants;
  hphp_fast_map<const php::Constant*, FuncClsUnitSet> constants;
  hphp_fast_map<const php::Class*, FuncClsUnitSet> anyClsConstants;
  hphp_fast_map<const php::Prop*, FuncClsUnitSet> properties;
  hphp_fast_map<const php::Class*, FuncClsUnitSet> anyProperties;
};

//////////////////////////////////////////////////////////////////////

using IndexData = Index::IndexData;

std::mutex closure_use_vars_mutex;
std::mutex private_propstate_mutex;

DependencyContext make_dep(const php::Func* func) {
  return DependencyContext{DependencyContextType::Func, func};
}
DependencyContext make_dep(const php::Class* cls) {
  return DependencyContext{DependencyContextType::Class, cls};
}
DependencyContext make_dep(const php::Prop* prop) {
  return DependencyContext{DependencyContextType::Prop, prop};
}
DependencyContext make_dep(const FuncFamily* family) {
  return DependencyContext{DependencyContextType::FuncFamily, family};
}

DependencyContext dep_context(IndexData& data, const Context& baseCtx) {
  auto const& ctx = baseCtx.forDep();
  if (!ctx.cls || !data.useClassDependencies) return make_dep(ctx.func);
  auto const cls = ctx.cls->closureContextCls
    ? data.classes.at(ctx.cls->closureContextCls)
    : ctx.cls;
  if (is_used_trait(*cls)) return make_dep(ctx.func);
  return make_dep(cls);
}

template <typename T>
void add_dependency(IndexData& data,
                    T src,
                    const Context& dst,
                    Dep newMask) {
  if (data.frozen) return;

  auto d = dep_context(data, dst);
  DepMap::accessor acc;
  data.dependencyMap.insert(acc, make_dep(src));
  auto& current = acc->second[d];
  current = current | newMask;

  // We should only have a return type dependency on func families.
  assertx(
    IMPLIES(
      acc->first.tag() == DependencyContextType::FuncFamily,
      newMask == Dep::ReturnTy
    )
  );
}

template <typename T>
void find_deps(IndexData& data,
               T src,
               Dep mask,
               DependencyContextSet& deps) {
  auto const srcDep = make_dep(src);

  // We should only ever have return type dependencies on func family.
  assertx(
    IMPLIES(
      srcDep.tag() == DependencyContextType::FuncFamily,
      mask == Dep::ReturnTy
    )
  );

  DepMap::const_accessor acc;
  if (data.dependencyMap.find(acc, srcDep)) {
    for (auto const& kv : acc->second) {
      if (has_dep(kv.second, mask)) deps.insert(kv.first);
    }
  }
}

//////////////////////////////////////////////////////////////////////

FuncInfo* func_info(IndexData& data, const php::Func* f) {
  assertx(f->idx < data.funcInfo.size());
  auto const fi = &data.funcInfo[f->idx];
  assertx(fi->func == f);
  return fi;
}

FuncInfo2& func_info(AnalysisIndex::IndexData& data, const php::Func& f) {
  assertx(f.idx < data.finfosByIdx.size());
  auto const fi = data.finfosByIdx[f.idx];
  assertx(fi->func == &f);
  return *fi;
}

//////////////////////////////////////////////////////////////////////

// Obtain the php::Func* represented by a MethRef.
const php::Func* func_from_meth_ref(const IndexData& index,
                                    const MethRef& meth) {
  auto const cls = index.classes.at(meth.cls);
  assertx(meth.idx < cls->methods.size());
  return cls->methods[meth.idx].get();
}

const php::Func* func_from_meth_ref(const AnalysisIndex::IndexData& index,
                                    const MethRef& meth) {
  index.deps->add(AnalysisDeps::Class { meth.cls });
  auto const cls = folly::get_default(index.classes, meth.cls);
  if (!cls) {
    always_assert_flog(
      !index.badClasses.contains(meth.cls),
      "MethRef references non-existent class {}\n",
      meth.cls
    );
    return nullptr;
  }
  assertx(meth.idx < cls->methods.size());
  return cls->methods[meth.idx].get();
}

//////////////////////////////////////////////////////////////////////

bool should_retain(const FuncInfo2& finfo,
                   bool better,
                   AnalysisIndex::IndexData& index) {
  if (!index.frozen) return false;
  auto const fc = fc_from_context(context_for_deps(index), index);
  if (!index.toReport.contains(fc)) return false;
  auto const fc2 = fc_from_context(
    Context{ finfo.func->unit, finfo.func, finfo.func->cls },
    index
  );
  return fc != fc2 && (better || !index.toReport.contains(fc2));
}

bool should_retain(const ClassInfo2& cinfo,
                   bool better,
                   AnalysisIndex::IndexData& index) {
  if (!index.frozen) return false;
  auto const fc = fc_from_context(context_for_deps(index), index);
  if (!index.toReport.contains(fc)) return false;
  auto const fc2 = fc_from_context(
    Context{ cinfo.cls->unit, nullptr, cinfo.cls },
    index
  );
  return fc != fc2 && (better || !index.toReport.contains(fc2));
}

//////////////////////////////////////////////////////////////////////

RetainedInfo* retained_for_context(AnalysisIndex::IndexData& index) {
  auto const fc = fc_from_context(context_for_deps(index), index);
  if (auto const c = fc.cls()) {
    if (!c->cinfo) return nullptr;
    return &c->cinfo->retained;
  } else if (auto const f = fc.func()) {
    auto& fi = func_info(index, *f);
    return &fi.retained;
  } else {
    return nullptr;
  }
}

//////////////////////////////////////////////////////////////////////

template<typename Filter>
PropState make_unknown_propstate(const AnalysisIndex::IndexData& index,
                                 const php::Class& cls,
                                 Filter filter) {
  auto ret = PropState{};
  for (auto& prop : cls.properties) {
    if (filter(prop)) {
      auto& elem = ret[prop.name];
      elem.ty = adjust_type_for_prop(
        AnalysisIndexAdaptor { index.index },
        cls,
        &prop.typeConstraints,
        TCell
      );
      if (prop.attrs & AttrSystemInitialValue) {
        if (type(prop.val) == KindOfUninit) {
          index.deps->add(
            AnalysisDeps::Class { cls.name },
            AnalysisDeps::Type::PropInitVals
          );
        }
        auto initial = loosen_all(from_cell(prop.val));
        if (!initial.subtypeOf(BUninit)) elem.ty |= initial;
      }
      elem.tc = &prop.typeConstraints;
      elem.attrs = prop.attrs;
      elem.everModified = true;
    }
  }

  return ret;
}

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

// Defined here so that AnalysisIndex::IndexData is a complete type.

bool ClassGraph::storeAuxs(AnalysisIndex::IndexData& i, bool children) const {
  assertx(i.frozen);

  auto const add = [&] (AuxClassGraphs& auxs) {
    if (children) {
      if (!auxs.newWithChildren.emplace(*this).second) return false;
      auxs.newNoChildren.erase(*this);
      return true;
    } else {
      if (auxs.newWithChildren.contains(*this)) return false;
      return auxs.newNoChildren.emplace(*this).second;
    }
  };

  // Get the current context and store this ClassGraph on it's aux
  // list.
  auto const fc = fc_from_context(context_for_deps(i), i);
  if (auto const c = fc.cls()) {
    if (!c->cinfo) return false;
    if (c->cinfo == cinfo2()) return true;
    if (add(c->cinfo->auxClassGraphs)) {
      FTRACE(
        2, "{} now stores {} as an auxiliary ClassGraph{}\n",
        c->name, name(),
        children ? " (with children)" : ""
      );
    }
    return true;
  } else if (auto const f = fc.func()) {
    auto& fi = func_info(i, *f);
    if (!fi.auxClassGraphs) {
      fi.auxClassGraphs = std::make_unique<AuxClassGraphs>();
    }
    if (add(*fi.auxClassGraphs)) {
      FTRACE(
        2, "{} now stores {} as an auxiliary ClassGraph{}\n",
        f->name, name(),
        children ? " (with children)" : ""
      );
    }
    return true;
  }

  return false;
}

bool ClassGraph::onAuxs(AnalysisIndex::IndexData& i,
                        bool children,
                        bool regOnly) const {
  assertx(IMPLIES(!children, !regOnly));

  // Check if this ClassGraph is on the current context's aux set *or*
  // if it is implied by another ClassGraph on the aux set (for
  // example, if this ClassGraph is a parent of a ClassGraph already
  // present).
  auto const check = [&] (const AuxClassGraphs& auxs, Node* target) {
    assertx(!target || !target->isMissing());

    if (target == this_) return true;
    // Check for direct membership first
    if (auxs.withChildren.contains(*this)) return true;
    if (!children && auxs.noChildren.contains(*this)) return true;
    if (this_->isMissing()) return false;

    // Check if any parents of this Node are on the set.
    auto const a = forEachParent(
      *this_,
      [&] (Node& p) {
        if (target != &p && !auxs.withChildren.contains(ClassGraph { &p })) {
          return Action::Continue;
        }
        return p.hasCompleteChildren()
          ? Action::Stop
          : Action::Skip;
      }
    );
    if (a == Action::Stop) return true;

    if (children) {
      if (!this_->hasCompleteChildren()) return false;

      NodeMap<bool> parents;
      parents[this_] = false;

      Optional<bool> all;
      Optional<bool> allReg;
      forEachChild(
        *this_,
        [&] (Node& c) {
          if (&c == this_) return Action::Continue;
          auto const any = foldParentsImpl(
            c,
            [&] (Node& p) {
              if (target == &p) return true;
              if (!auxs.withChildren.contains(ClassGraph{ &p })) return false;
              assertx(p.hasCompleteChildren() || p.isConservative());
              return p.hasCompleteChildren();
            },
            [&] { return false; },
            parents,
            true
          );
          all = all.value_or(true) && any;
          if (c.isRegular()) allReg = allReg.value_or(true) && any;
          if (allReg && !*allReg) return Action::Stop;
          if (!regOnly && !*all) return Action::Stop;
          return (!regOnly || c.isRegular())
            ? Action::Skip
            : Action::Continue;
        }
      );
      if (all.value_or(false)) return true;
      return regOnly && allReg.value_or(false);
    }

    {
      TLNodeIdxSet visited;
      for (auto const n : auxs.noChildren) {
        if (n.this_->isMissing()) continue;
        if (findParent(*n.this_, *this_, *visited)) return true;
      }
      for (auto const n : auxs.withChildren) {
        if (n.this_->isMissing()) continue;
        if (findParent(*n.this_, *this_, *visited)) return true;
      }
      if (target && findParent(*target, *this_, *visited)) return true;
    }

    static thread_local NodeMap<NodeSet> cousinCache;
    auto const cache = [&] {
      if (auto const c = folly::get_ptr(cousinCache, this_)) return c;

      TLNodeIdxSet parents;
      NodeSet cousins;
      forEachChild(
        *this_,
        [&] (Node& c) {
          forEachParentImpl(
            c,
            [&] (Node& p) {
              cousins.emplace(&p);
              return Action::Continue;
            },
            &*parents,
            true
          );
          return Action::Continue;
        }
      );

      auto const [it, emplaced] =
        cousinCache.try_emplace(this_, std::move(cousins));
      always_assert(emplaced);
      return &it->second;
    }();

    if (cache->contains(target)) return true;
    for (auto const n : auxs.withChildren) {
      if (!cache->contains(n.this_)) continue;
      assertx(n.this_->hasCompleteChildren() || n.this_->isConservative());
      return n.this_->hasCompleteChildren();
    }

    return false;
  };

  auto const fc = fc_from_context(context_for_deps(i), i);
  AuxClassGraphs::CacheKey key{*this, children, regOnly};

  if (auto const c = fc.cls()) {
    if (!c->cinfo) return false;
    if (c->cinfo == cinfo2()) return true;
    if (auto const b = folly::get_ptr(c->cinfo->auxClassGraphs.cache, key)) {
      return *b;
    }
    auto const r = check(c->cinfo->auxClassGraphs, c->cinfo->classGraph.this_);
    c->cinfo->auxClassGraphs.cache.emplace(key, r);
    return r;
  }
  if (auto const f = fc.func()) {
    auto const& fi = func_info(i, *f);
    if (!fi.auxClassGraphs) return false;
    if (auto const b = folly::get_ptr(fi.auxClassGraphs->cache, key)) {
      return *b;
    }
    auto const r = check(*fi.auxClassGraphs, nullptr);
    fi.auxClassGraphs->cache.emplace(key, r);
    return r;
  }
  return false;
}

// Ensure ClassGraph is not missing
void ClassGraph::ensure() const {
  assertx(this_);
  auto const i = table().index;
  if (!i) return;

  if (onAuxs(*i, false, false)) {
    if (i->frozen) always_assert(storeAuxs(*i, false));
    return;
  }
  if (this_->isMissing()) i->deps->add(AnalysisDeps::Class { name() });
  if (!i->frozen) return;
  if (!storeAuxs(*i, false)) i->deps->add(AnalysisDeps::Class { name() });
}

// Ensure ClassGraph is not missing and has complete child
// information.
void ClassGraph::ensureWithChildren(bool regOnly) const {
  assertx(this_);
  auto const i = table().index;
  if (!i) return;

  if (onAuxs(*i, true, regOnly)) {
    if (i->frozen) always_assert(storeAuxs(*i, true));
    return;
  }
  if (this_->isMissing() ||
      (!this_->hasCompleteChildren() && !this_->isConservative())) {
    i->deps->add(AnalysisDeps::Class { name() });
  }
  if (!i->frozen) return;
  if (!storeAuxs(*i, true)) i->deps->add(AnalysisDeps::Class { name() });
}

// Ensure ClassGraph is not missing and has an associated ClassInfo2
// (strongest condition).
void ClassGraph::ensureCInfo() const {
  auto const i = table().index;
  if (!i) return;
  i->deps->add(AnalysisDeps::Class { name() });
}

//////////////////////////////////////////////////////////////////////

struct ClassBundle {
  std::vector<std::unique_ptr<php::Class>> classes;
  std::vector<std::unique_ptr<ClassInfo2>> classInfos;
  std::vector<std::unique_ptr<php::ClassBytecode>> classBytecode;
  std::vector<std::unique_ptr<php::Func>> funcs;
  std::vector<std::unique_ptr<FuncInfo2>> funcInfos;
  std::vector<std::unique_ptr<php::FuncBytecode>> funcBytecode;
  std::vector<std::unique_ptr<php::Unit>> units;
  std::vector<std::unique_ptr<MethodsWithoutCInfo>> methInfos;

  template <typename SerDe> void serde(SerDe& sd) {
    ScopedStringDataIndexer _;
    ClassGraph::ScopedSerdeState _2;
    sd(classes)
      (classInfos)
      (classBytecode)
      (funcs)
      (funcInfos)
      (funcBytecode)
      (units)
      (methInfos)
      ;
  }
};

//////////////////////////////////////////////////////////////////////

namespace {

//////////////////////////////////////////////////////////////////////

struct TraitMethod {
  using class_type = std::pair<const ClassInfo2*, const php::Class*>;
  using method_type = const php::Func*;
  using origin_type = SString;

  TraitMethod(class_type trait_, method_type method_, Attr modifiers_)
      : trait(trait_)
      , method(method_)
      , modifiers(modifiers_)
    {}

  class_type trait;
  method_type method;
  Attr modifiers;
};

struct TMIOps {
  using string_type = LSString;
  using class_type = TraitMethod::class_type;
  using method_type = TraitMethod::method_type;
  using origin_type = TraitMethod::origin_type;

  struct TMIException : std::exception {
    explicit TMIException(std::string msg) : msg(msg) {}
    const char* what() const noexcept override { return msg.c_str(); }
  private:
    std::string msg;
  };

  // Return the name for the trait class.
  static const string_type clsName(class_type traitCls) {
    return traitCls.first->name;
  }

  // Return the name of the trait where the method was originally defined
  static origin_type originalClass(method_type meth) {
    return meth->originalClass ? meth->originalClass : meth->cls->name;
  }

  // Is-a methods.
  static bool isAbstract(Attr modifiers) {
    return modifiers & AttrAbstract;
  }

  // Whether to exclude methods with name `methName' when adding.
  static bool exclude(string_type methName) {
    return Func::isSpecial(methName);
  }

  // Errors.
  static void errorDuplicateMethod(class_type cls,
                                   string_type methName,
                                   const std::vector<const StringData*>&) {
    auto const& m = cls.second->methods;
    if (std::find_if(m.begin(), m.end(),
                     [&] (auto const& f) {
                       return f->name->same(methName);
                     }) != m.end()) {
      // the duplicate methods will be overridden by the class method.
      return;
    }
    throw TMIException(folly::sformat("DuplicateMethod: {}", methName));
  }
};

using TMIData = TraitMethodImportData<TraitMethod, TMIOps>;

//////////////////////////////////////////////////////////////////////

template <typename T, typename R>
void add_symbol_to_index(R& map, T t, const char* type) {
  auto const name = t->name;
  auto const ret = map.emplace(name, std::move(t));
  always_assert_flog(
    ret.second,
    "More than one {} with the name {} "
    "(should have been caught by parser)",
    type,
    name
  );
}

template <typename T, typename R, typename E>
void add_symbol_to_index(R& map, T t, const char* type, const E& other) {
  auto const it = other.find(t->name);
  always_assert_flog(
    it == other.end(),
    "More than one symbol with the name {} "
    "(should have been caught by parser)",
    t->name
  );
  add_symbol_to_index(map, std::move(t), type);
}

// We want const qualifiers on various index data structures for php
// object pointers, but during index creation time we need to
// manipulate some of their attributes (changing the representation).
// This little wrapper keeps the const_casting out of the main line of
// code below.
void attribute_setter(const Attr& attrs, bool set, Attr attr) {
  attrSetter(const_cast<Attr&>(attrs), set, attr);
}

void add_system_constants_to_index(IndexData& index) {
  for (auto cnsPair : Native::getConstants()) {
    assertx(cnsPair.second.m_type != KindOfUninit);
    auto pc = new php::Constant {
      cnsPair.first,
      cnsPair.second,
      AttrPersistent
    };
    add_symbol_to_index(index.constants, pc, "constant");
  }
}

void add_unit_to_index(IndexData& index, php::Unit& unit) {
  always_assert_flog(
    index.units.emplace(unit.filename, &unit).second,
    "More than one unit with the same name {} "
    "(should have been caught by parser)",
    unit.filename
  );

  for (auto& ta : unit.typeAliases) {
    add_symbol_to_index(
      index.typeAliases,
      ta.get(),
      "type alias",
      index.classes
    );
  }

  for (auto& c : unit.constants) {
    add_symbol_to_index(index.constants, c.get(), "constant");
  }

  for (auto& m : unit.modules) {
    add_symbol_to_index(index.modules, m.get(), "module");
  }
}

void add_class_to_index(IndexData& index, php::Class& c) {
  if (c.attrs & AttrEnum) {
    add_symbol_to_index(index.enums, &c, "enum");
  }

  add_symbol_to_index(index.classes, &c, "class", index.typeAliases);

  for (auto& m : c.methods) {
    attribute_setter(m->attrs, false, AttrNoOverride);
    m->idx = index.nextFuncId++;
  }
}

void add_func_to_index(IndexData& index, php::Func& func) {
  add_symbol_to_index(index.funcs, &func, "function");
  func.idx = index.nextFuncId++;
}

void add_program_to_index(IndexData& index) {
  trace_time timer{"add program to index", index.sample};
  timer.ignore_client_stats();

  auto& program = *index.program;
  for (auto const& u : program.units) {
    add_unit_to_index(index, *u);
  }
  for (auto const& c : program.classes) {
    add_class_to_index(index, *c);
    for (auto const& clo : c->closures) {
      add_class_to_index(index, *clo);
    }
  }
  for (auto const& f : program.funcs) {
    add_func_to_index(index, *f);
  }

  // All funcs have been assigned indices above. Now for each func we
  // initialize a default FuncInfo in the funcInfo vec at the
  // appropriate index.

  trace_time timer2{"create func-infos"};
  timer2.ignore_client_stats();

  index.funcInfo.resize(index.nextFuncId);

  auto const create = [&] (const php::Func& f) {
    assertx(f.idx < index.funcInfo.size());
    auto& fi = index.funcInfo[f.idx];
    assertx(!fi.func);
    fi.func = &f;
  };

  parallel::for_each(
    program.classes,
    [&] (const std::unique_ptr<php::Class>& cls) {
      for (auto const& m : cls->methods) create(*m);
      for (auto const& clo : cls->closures) {
        assertx(clo->methods.size() == 1);
        create(*clo->methods[0]);
      }
    }
  );

  parallel::for_each(
    program.funcs,
    [&] (const std::unique_ptr<php::Func>& func) { create(*func); }
  );
}

//////////////////////////////////////////////////////////////////////

/*
 * Lists of interfaces that conflict with each other due to being
 * implemented by the same class.
 */

struct InterfaceConflicts {
  SString name{nullptr};
  // The number of classes which implements this interface (used to
  // prioritize lower slots for more heavily used interfaces).
  size_t usage{0};
  TSStringSet conflicts;
  template <typename SerDe> void serde(SerDe& sd) {
    sd(name)(usage)(conflicts, string_data_lt_type{});
  }
};

void compute_iface_vtables(IndexData& index,
                           std::vector<InterfaceConflicts> conflicts) {
  trace_time tracer{"compute interface vtables"};
  tracer.ignore_client_stats();

  if (conflicts.empty()) return;

  // Sort interfaces by usage frequencies. We assign slots greedily,
  // so sort the interface list so the most frequently implemented
  // ones come first.
  std::sort(
    begin(conflicts),
    end(conflicts),
    [&] (const InterfaceConflicts& a, const InterfaceConflicts& b) {
      if (a.usage != b.usage) return a.usage > b.usage;
      return string_data_lt_type{}(a.name, b.name);
    }
  );

  // Assign slots, keeping track of the largest assigned slot and the
  // total number of uses for each slot.

  Slot maxSlot = 0;
  hphp_fast_map<Slot, int> slotUses;
  boost::dynamic_bitset<> used;

  for (auto const& iface : conflicts) {
    used.reset();

    // Find the lowest Slot that doesn't conflict with anything in the
    // conflict set for iface.
    auto const slot = [&] () -> Slot {
      // No conflicts. This is the only interface implemented by the
      // classes that implement it.
      if (iface.conflicts.empty()) return 0;

      for (auto const conflict : iface.conflicts) {
        auto const it = index.ifaceSlotMap.find(conflict);
        if (it == end(index.ifaceSlotMap)) continue;
        auto const s = it->second;
        if (used.size() <= s) used.resize(s + 1);
        used.set(s);
      }

      used.flip();
      return used.any() ? used.find_first() : used.size();
    }();

    always_assert(
      index.ifaceSlotMap.emplace(iface.name, slot).second
    );
    maxSlot = std::max(maxSlot, slot);
    slotUses[slot] += iface.usage;
  }

  if constexpr (debug) {
    // Make sure we have an initialized entry for each slot for the sort below.
    for (Slot slot = 0; slot < maxSlot; ++slot) {
      always_assert(slotUses.contains(slot));
    }
  }

  // Finally, sort and reassign slots so the most frequently used
  // slots come first. This slightly reduces the number of wasted
  // vtable vector entries at runtime.

  auto const slots = [&] {
    std::vector<std::pair<Slot, int>> flattened{
      begin(slotUses), end(slotUses)
    };
    std::sort(
      begin(flattened),
      end(flattened),
      [&] (auto const& a, auto const& b) {
        if (a.second != b.second) return a.second > b.second;
        return a.first < b.first;
      }
    );
    std::vector<Slot> out;
    out.reserve(flattened.size());
    for (auto const& [slot, _] : flattened) out.emplace_back(slot);
    return out;
  }();

  std::vector<Slot> slotsPermute(maxSlot + 1, 0);
  for (size_t i = 0; i <= maxSlot; ++i) slotsPermute[slots[i]] = i;

  // re-map interfaces to permuted slots
  for (auto& [cls, slot] : index.ifaceSlotMap) {
    slot = slotsPermute[slot];
  }
}

//////////////////////////////////////////////////////////////////////

struct CheckClassInfoInvariantsJob {
  static std::string name() { return "hhbbc-check-cinfo-invariants"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
    ClassGraph::init();
  }
  static void fini() { ClassGraph::destroy(); }

  static bool run(std::unique_ptr<ClassInfo2> cinfo,
                  std::unique_ptr<php::Class> cls) {
    SCOPE_ASSERT_DETAIL("class") { return cls->name->toCppString(); };

    always_assert(check(*cls, false));

    auto const check = [] (const ClassInfo2* cinfo,
                           const php::Class* cls) {
      // ClassGraph stored in a ClassInfo should not be missing, always
      // have the ClassInfo stored, and have complete children
      // information.
      always_assert(cinfo->classGraph);
      always_assert(!cinfo->classGraph.isMissing());
      always_assert(cinfo->classGraph.name()->tsame(cinfo->name));
      always_assert(cinfo->classGraph.cinfo2() == cinfo);
      if (is_closure_base(cinfo->name)) {
        // The closure base class is special. We don't store it's
        // children information because it's too large.
        always_assert(!cinfo->classGraph.hasCompleteChildren());
        always_assert(cinfo->classGraph.isConservative());
      } else {
        always_assert(cinfo->classGraph.hasCompleteChildren() ||
                      cinfo->classGraph.isConservative());
      }

      // This class and withoutNonRegular should be equivalent when
      // ignoring non-regular classes. The withoutNonRegular class
      // should be a fixed-point.
      if (auto const without = cinfo->classGraph.withoutNonRegular()) {
        always_assert(without.hasCompleteChildren() ||
                      without.isConservative());
        always_assert(without.subSubtypeOf(cinfo->classGraph, false, false));
        always_assert(cinfo->classGraph.subSubtypeOf(without, false, false));
        always_assert(without.withoutNonRegular() == without);
        always_assert(cinfo->classGraph.mightBeRegular() ||
                      cinfo->classGraph.mightHaveRegularSubclass());
        always_assert(IMPLIES(cinfo->classGraph.mightBeRegular(),
                              without == cinfo->classGraph));
      } else if (!is_used_trait(*cls)) {
        always_assert(!cinfo->classGraph.mightBeRegular());
        always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
      }

      // AttrNoOverride is a superset of AttrNoOverrideRegular
      always_assert(
        IMPLIES(!(cls->attrs & AttrNoOverrideRegular),
                !(cls->attrs & AttrNoOverride))
      );

      // Override attrs and what we know about the subclasses should be in
      // agreement.
      if (cls->attrs & AttrNoOverride) {
        always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
        always_assert(!cinfo->classGraph.mightHaveNonRegularSubclass());
      } else if (cls->attrs & AttrNoOverrideRegular) {
        always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
        always_assert(cinfo->classGraph.mightHaveNonRegularSubclass());
      }

      // Make sure the information stored on the ClassInfo matches that
      // which the ClassGraph reports.
      if (cls->attrs & AttrNoMock) {
        always_assert(!cinfo->isMocked);
        always_assert(!cinfo->isSubMocked);
      } else {
        always_assert(cinfo->isSubMocked);
      }

      always_assert(
        bool(cls->attrs & AttrNoExpandTrait) ==
        cinfo->classGraph.usedTraits().empty()
      );

      for (auto const& [name, mte] : cinfo->methods) {
        // Interface method tables should only contain its own methods.
        if (cls->attrs & AttrInterface) {
          always_assert(mte.meth().cls->tsame(cinfo->name));
        }

        // AttrNoOverride implies noOverrideRegular
        always_assert(IMPLIES(mte.attrs & AttrNoOverride, mte.noOverrideRegular()));

        if (!is_special_method_name(name)) {
          // If the class isn't overridden, none of it's methods can be
          // either.
          always_assert(IMPLIES(cls->attrs & AttrNoOverride,
                                mte.attrs & AttrNoOverride));
        } else {
          always_assert(!(mte.attrs & AttrNoOverride));
          always_assert(!mte.noOverrideRegular());
        }

        if (cinfo->name->tsame(s_Closure.get()) || is_closure_name(cinfo->name)) {
          always_assert(mte.attrs & AttrNoOverride);
        }

        // Don't store method families for special methods.
        auto const famIt = cinfo->methodFamilies.find(name);
        if (is_special_method_name(name)) {
          always_assert(famIt == end(cinfo->methodFamilies));
          continue;
        }
        if (famIt == end(cinfo->methodFamilies)) {
          always_assert(is_closure_name(cinfo->name));
          continue;
        }
        auto const& entry = famIt->second;

        // No override methods should always have a single entry.
        if (mte.attrs & AttrNoOverride) {
          always_assert(
            std::get_if<FuncFamilyEntry::BothSingle>(&entry.m_meths) ||
            std::get_if<FuncFamilyEntry::SingleAndNone>(&entry.m_meths)
          );
          continue;
        }

        if (cinfo->isRegularClass) {
          // "all" should only be a func family. It can't be empty,
          // because we know there's at least one method in it (the one in
          // cinfo->methods). It can't be a single func, because one of
          // the methods must be the cinfo->methods method, and we know it
          // isn't AttrNoOverride, so there *must* be another method. So,
          // it must be a func family.
          always_assert(
            std::get_if<FuncFamilyEntry::BothFF>(&entry.m_meths) ||
            std::get_if<FuncFamilyEntry::FFAndSingle>(&entry.m_meths)
          );
          // This is a regular class, so we cannot have an incomplete
          // entry (can only happen with interfaces).
          always_assert(!entry.m_regularIncomplete);
        }
      }

      // If the class is marked as having not having bad initial prop
      // values, all of it's properties should have
      // AttrInitialSatisfiesTC set. Likewise, if it is, at least one
      // property should not have it set.
      if (!cinfo->hasBadInitialPropValues) {
        auto const all = std::all_of(
          begin(cls->properties),
          end(cls->properties),
          [] (const php::Prop& p) {
            return p.attrs & AttrInitialSatisfiesTC;
          }
        );
        always_assert(all);
      } else {
        auto const someBad = std::any_of(
          begin(cls->properties),
          end(cls->properties),
          [] (const php::Prop& p) {
            return !(p.attrs & AttrInitialSatisfiesTC);
          }
        );
        always_assert(someBad);
      }

      if (is_closure_name(cinfo->name)) {
        assertx(cinfo->classGraph.hasCompleteChildren());
        // Closures have no children.
        auto const subclasses = cinfo->classGraph.children();
        always_assert(subclasses.size() == 1);
        always_assert(subclasses[0].name()->tsame(cinfo->name));
      } else if (cinfo->classGraph.hasCompleteChildren()) {
        // Otherwise the children list is non-empty, contains this
        // class, and contains only unique elements.
        auto const subclasses = cinfo->classGraph.children();
        always_assert(
          std::find_if(
            begin(subclasses),
            end(subclasses),
            [&] (ClassGraph g) { return g.name()->tsame(cinfo->name); }
          ) != end(subclasses)
        );
        auto cpy = subclasses;
        std::sort(begin(cpy), end(cpy));
        cpy.erase(std::unique(begin(cpy), end(cpy)), end(cpy));
        always_assert(cpy.size() == subclasses.size());
      }

      // The base list is non-empty, and the last element is this class.
      auto const bases = cinfo->classGraph.bases();
      always_assert(!bases.empty());
      always_assert(cinfo->classGraph == bases.back());
      if (is_closure_base(cinfo->name)) {
        always_assert(bases.size() == 1);
      } else if (is_closure_name(cinfo->name)) {
        always_assert(bases.size() == 2);
        always_assert(bases[0].name()->tsame(s_Closure.get()));
      }

      always_assert(IMPLIES(is_closure(*cls), cls->closures.empty()));
      always_assert(cls->closures.size() == cinfo->closures.size());
    };

    check(cinfo.get(), cls.get());
    for (size_t i = 0, size = cls->closures.size(); i < size; ++i) {
      always_assert(cls->closures[i]->name->tsame(cinfo->closures[i]->name));
      always_assert(is_closure(*cls->closures[i]));
      check(cinfo->closures[i].get(), cls->closures[i].get());
    }

    return true;
  }
};

struct CheckFuncFamilyInvariantsJob {
  static std::string name() { return "hhbbc-check-ff-invariants"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
  }
  static void fini() {}

  static bool run(FuncFamilyGroup group) {
    for (auto const& ff : group.m_ffs) {
      // FuncFamily should always have more than one func on it.
      always_assert(
        ff->m_regular.size() +
        ff->m_nonRegularPrivate.size() +
        ff->m_nonRegular.size()
        > 1
      );

      // Every method should be sorted in its respective list. We
      // should never see a method for the same Class more than once.
      TSStringSet classes;
      Optional<MethRef> lastReg;
      Optional<MethRef> lastPrivate;
      Optional<MethRef> lastNonReg;
      for (auto const& meth : ff->m_regular) {
        if (lastReg) always_assert(*lastReg < meth);
        lastReg = meth;
        always_assert(classes.emplace(meth.cls).second);
      }
      for (auto const& meth : ff->m_nonRegularPrivate) {
        if (lastPrivate) always_assert(*lastPrivate < meth);
        lastPrivate = meth;
        always_assert(classes.emplace(meth.cls).second);
      }
      for (auto const& meth : ff->m_nonRegular) {
        if (lastNonReg) always_assert(*lastNonReg < meth);
        lastNonReg = meth;
        always_assert(classes.emplace(meth.cls).second);
      }

      always_assert(ff->m_allStatic.has_value());
      always_assert(
        ff->m_regularStatic.has_value() ==
        (!ff->m_regular.empty() || !ff->m_nonRegularPrivate.empty())
      );
    }
    return true;
  }
};

Job<CheckClassInfoInvariantsJob> s_checkCInfoInvariantsJob;
Job<CheckFuncFamilyInvariantsJob> s_checkFuncFamilyInvariantsJob;

void check_invariants(const IndexData& index) {
  if (!debug) return;

  trace_time trace{"check-invariants", index.sample};

  constexpr size_t kCInfoBucketSize = 3000;
  constexpr size_t kFFBucketSize = 500;

  auto cinfoBuckets = consistently_bucketize(
    [&] {
      std::vector<SString> roots;
      roots.reserve(index.classInfoRefs.size());
      for (auto const& [name, _] : index.classInfoRefs) {
        roots.emplace_back(name);
      }
      return roots;
    }(),
    kCInfoBucketSize
  );

  SStringToOneT<Ref<FuncFamilyGroup>> nameToFuncFamilyGroup;

  auto ffBuckets = consistently_bucketize(
    [&] {
      std::vector<SString> roots;
      roots.reserve(index.funcFamilyRefs.size());
      for (auto const& [_, ref] : index.funcFamilyRefs) {
        auto const name = makeStaticString(ref.id().toString());
        if (nameToFuncFamilyGroup.emplace(name, ref).second) {
          roots.emplace_back(name);
        }
      }
      return roots;
    }(),
    kFFBucketSize
  );

  using namespace folly::gen;

  auto const runCInfo = [&] (std::vector<SString> work) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    if (work.empty()) co_return;

    auto inputs = from(work)
      | map([&] (SString name) {
          return std::make_tuple(
            index.classInfoRefs.at(name),
            index.classRefs.at(name)
          );
        })
      | as<std::vector>();

    auto metadata = make_exec_metadata(
      "check cinfo invariants",
      work[0]->toCppString()
    );
    auto config = co_await index.configRef->getCopy();
    auto outputs = co_await index.client->exec(
      s_checkCInfoInvariantsJob,
      std::move(config),
      std::move(inputs),
      std::move(metadata)
    );
    assertx(outputs.size() == work.size());

    co_return;
  };

  auto const runFF = [&] (std::vector<SString> work) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    if (work.empty()) co_return;

    auto inputs = from(work)
      | map([&] (SString name) {
          return std::make_tuple(nameToFuncFamilyGroup.at(name));
        })
      | as<std::vector>();

    auto metadata = make_exec_metadata(
      "chunk func-family invariants",
      work[0]->toCppString()
    );
    auto config = co_await index.configRef->getCopy();
    auto outputs = co_await index.client->exec(
      s_checkFuncFamilyInvariantsJob,
      std::move(config),
      std::move(inputs),
      std::move(metadata)
    );
    assertx(outputs.size() == work.size());

    co_return;
  };

  std::vector<coro::TaskWithExecutor<void>> tasks;
  for (auto& work : cinfoBuckets) {
    tasks.emplace_back(
      co_withExecutor(index.executor->sticky(), runCInfo(std::move(work)))
    );
  }
  for (auto& work : ffBuckets) {
    tasks.emplace_back(
      co_withExecutor(index.executor->sticky(), runFF(std::move(work)))
    );
  }
  coro::blockingWait(coro::collectAllRange(std::move(tasks)));
}

void check_local_invariants(const IndexData& index, const ClassInfo* cinfo) {
  SCOPE_ASSERT_DETAIL("class") { return cinfo->cls->name->toCppString(); };

  always_assert(check(*cinfo->cls));

  // AttrNoOverride is a superset of AttrNoOverrideRegular
  always_assert(
    IMPLIES(!(cinfo->cls->attrs & AttrNoOverrideRegular),
            !(cinfo->cls->attrs & AttrNoOverride))
  );

  always_assert(cinfo->classGraph);
  always_assert(!cinfo->classGraph.isMissing());
  always_assert(cinfo->classGraph.name()->tsame(cinfo->cls->name));
  always_assert(cinfo->classGraph.cinfo() == cinfo);
  if (is_closure_base(cinfo->cls->name)) {
    // The closure base class is special. We don't store it's children
    // information because it's too large.
    always_assert(!cinfo->classGraph.hasCompleteChildren());
    always_assert(cinfo->classGraph.isConservative());
  } else {
    always_assert(cinfo->classGraph.hasCompleteChildren() ||
                  cinfo->classGraph.isConservative());
  }

  // This class and withoutNonRegular should be equivalent when
  // ignoring non-regular classes. The withoutNonRegular class should
  // be a fixed-point.
  if (auto const without = cinfo->classGraph.withoutNonRegular()) {
    always_assert(without.hasCompleteChildren() ||
                  without.isConservative());
    always_assert(without.subSubtypeOf(cinfo->classGraph, false, false));
    always_assert(cinfo->classGraph.subSubtypeOf(without, false, false));
    always_assert(without.withoutNonRegular() == without);
    always_assert(cinfo->classGraph.mightBeRegular() ||
                  cinfo->classGraph.mightHaveRegularSubclass());
    always_assert(IMPLIES(cinfo->classGraph.mightBeRegular(),
                          without == cinfo->classGraph));
  } else if (!is_used_trait(*cinfo->cls)) {
    always_assert(!cinfo->classGraph.mightBeRegular());
    always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
  }

  // Override attrs and what we know about the subclasses should be in
  // agreement.
  if (cinfo->cls->attrs & AttrNoOverride) {
    always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
    always_assert(!cinfo->classGraph.mightHaveNonRegularSubclass());
  } else if (cinfo->cls->attrs & AttrNoOverrideRegular) {
    always_assert(!cinfo->classGraph.mightHaveRegularSubclass());
    always_assert(cinfo->classGraph.mightHaveNonRegularSubclass());
  }

  if (cinfo->cls->attrs & AttrNoMock) {
    always_assert(!cinfo->isMocked);
    always_assert(!cinfo->isSubMocked);
  }

  // An AttrNoExpand class shouldn't have any used traits.
  always_assert(
    bool(cinfo->cls->attrs & AttrNoExpandTrait) ==
    cinfo->usedTraits.empty()
  );

  for (size_t idx = 0; idx < cinfo->cls->methods.size(); ++idx) {
    // Each method in a class has an entry in its ClassInfo method
    // table.
    auto const& m = cinfo->cls->methods[idx];
    auto const it = cinfo->methods.find(m->name);
    always_assert(it != cinfo->methods.end());
    always_assert(it->second.meth().cls->tsame(cinfo->cls->name));
    always_assert(it->second.meth().idx == idx);

    // Every method (except for constructors and special methods
    // should be in the global name-only tables.
    auto const nameIt = index.methodFamilies.find(m->name);
    if (!has_name_only_func_family(m->name)) {
      always_assert(nameIt == end(index.methodFamilies));
      continue;
    }
    always_assert(nameIt != end(index.methodFamilies));

    auto const& entry = nameIt->second;
    // The global name-only tables are never complete.
    always_assert(entry.m_all.isIncomplete());
    always_assert(entry.m_regular.isEmpty() || entry.m_regular.isIncomplete());

    // "all" should always be non-empty and contain this method.
    always_assert(!entry.m_all.isEmpty());
    if (auto const ff = entry.m_all.funcFamily()) {
      always_assert(ff->possibleFuncs().size() > 1);
      // The FuncFamily shouldn't have a section for regular results
      // if "regular" isn't using it.
      if (entry.m_regular.func() || entry.m_regular.isEmpty()) {
        always_assert(!ff->m_regular);
      } else {
        // "all" and "regular" always share the same func family.
        always_assert(entry.m_regular.funcFamily() == ff);
      }
    } else {
      auto const func = entry.m_all.func();
      always_assert(func);
      always_assert(func == m.get());
      // "regular" is always a subset of "all", so it can either be a
      // single func (the same as "all"), or empty.
      always_assert(entry.m_regular.func() || entry.m_regular.isEmpty());
      if (auto const func2 = entry.m_regular.func()) {
        always_assert(func == func2);
      }
    }

    // If this is a regular class, "regular" should be non-empty and
    // contain this method.
    if (auto const ff = entry.m_regular.funcFamily()) {
      always_assert(ff->possibleFuncs().size() > 1);
    } else if (auto const func = entry.m_regular.func()) {
      if (is_regular_class(*cinfo->cls)) {
        always_assert(func == m.get());
      }
    } else {
      always_assert(!is_regular_class(*cinfo->cls));
    }
  }

  // Interface ClassInfo method table should only contain methods from
  // the interface itself.
  if (cinfo->cls->attrs & AttrInterface) {
    always_assert(cinfo->cls->methods.size() == cinfo->methods.size());
  }

  // If a class isn't overridden, it shouldn't have any func families
  // (because the method table is sufficient).
  if (cinfo->cls->attrs & AttrNoOverride) {
    always_assert(cinfo->methodFamilies.empty());
    always_assert(cinfo->methodFamiliesAux.empty());
  }

  // The auxiliary method families map is only used by non-regular
  // classes.
  if (is_regular_class(*cinfo->cls)) {
    always_assert(cinfo->methodFamiliesAux.empty());
  }

  for (auto const& [name, mte] : cinfo->methods) {
    // Interface method tables should only contain its own methods.
    if (cinfo->cls->attrs & AttrInterface) {
      always_assert(mte.meth().cls->tsame(cinfo->cls->name));
    } else {
      // Non-interface method tables should not contain any methods
      // defined by an interface.
      auto const func = func_from_meth_ref(index, mte.meth());
      always_assert(!(func->cls->attrs & AttrInterface));
    }

    // AttrNoOverride implies noOverrideRegular
    always_assert(IMPLIES(mte.attrs & AttrNoOverride, mte.noOverrideRegular()));

    if (!is_special_method_name(name)) {
      // If the class isn't overridden, none of it's methods can be
      // either.
      always_assert(IMPLIES(cinfo->cls->attrs & AttrNoOverride,
                            mte.attrs & AttrNoOverride));
    } else {
      always_assert(!(mte.attrs & AttrNoOverride));
      always_assert(!mte.noOverrideRegular());
    }

    if (is_closure_base(*cinfo->cls) || is_closure(*cinfo->cls)) {
      always_assert(mte.attrs & AttrNoOverride);
    }

    auto const famIt = cinfo->methodFamilies.find(name);
    // Don't store method families for special methods, or if there's
    // no override.
    if (is_special_method_name(name) || (mte.attrs & AttrNoOverride)) {
      always_assert(famIt == end(cinfo->methodFamilies));
      always_assert(!cinfo->methodFamiliesAux.contains(name));
      continue;
    } else {
      always_assert(famIt != end(cinfo->methodFamilies));
    }
    auto const& entry = famIt->second;

    if (is_regular_class(*cinfo->cls)) {
      // "all" should only be a func family. It can't be empty,
      // because we know there's at least one method in it (the one in
      // cinfo->methods). It can't be a single func, because one of
      // the methods must be the cinfo->methods method, and we know it
      // isn't AttrNoOverride, so there *must* be another method. So,
      // it must be a func family.
      always_assert(entry.funcFamily());
      // This is a regular class, so we cannot have an incomplete
      // entry (can only happen with interfaces).
      always_assert(entry.isComplete());
    } else {
      // This class isn't AttrNoOverride, and since the method is on
      // this class, it should at least contain that.
      always_assert(!entry.isEmpty());
      // Only interfaces can have incomplete entries.
      always_assert(
        IMPLIES(entry.isIncomplete(), cinfo->cls->attrs & AttrInterface)
      );
      // If we got a single func, it should be the func on this
      // class. Since this isn't AttrNoOverride, it implies the entry
      // should be incomplete.
      always_assert(IMPLIES(entry.func(), entry.isIncomplete()));
      always_assert(
        IMPLIES(entry.func(),
                entry.func() == func_from_meth_ref(index, mte.meth()))
      );

      // The "aux" entry is optional. If it isn't present, it's the
      // same as the normal table.
      auto const auxIt = cinfo->methodFamiliesAux.find(name);
      if (auxIt != end(cinfo->methodFamiliesAux)) {
        auto const& aux = auxIt->second;

        // We shouldn't store in the aux table if the entry is the
        // same or if there's no override.
        always_assert(!mte.noOverrideRegular());
        always_assert(
          aux.isIncomplete() ||
          aux.func() != entry.func() ||
          aux.funcFamily() != entry.funcFamily()
        );

        // Normally the aux should be non-empty and complete. However
        // if this class is an interface, they could be.
        always_assert(
          IMPLIES(aux.isEmpty(), cinfo->cls->attrs & AttrInterface)
        );
        always_assert(
          IMPLIES(aux.isIncomplete(), cinfo->cls->attrs & AttrInterface)
        );

        // Since we know this was overridden (it wouldn't be in the
        // aux table otherwise), it must either be incomplete, or if
        // it has a single func, it cannot be the same func as this
        // class.
        always_assert(
          aux.isIncomplete() ||
          ((mte.attrs & AttrPrivate) && mte.topLevel()) ||
          aux.func() != func_from_meth_ref(index, mte.meth())
        );

        // Aux entry is a subset of the normal entry. If they both
        // have a func family or func, they must be the same. If the
        // normal entry has a func family, but aux doesn't, that func
        // family shouldn't have extra space allocated.
        always_assert(IMPLIES(entry.func(), !aux.funcFamily()));
        always_assert(IMPLIES(entry.funcFamily() && aux.funcFamily(),
                              entry.funcFamily() == aux.funcFamily()));
        always_assert(IMPLIES(entry.func() && aux.func(),
                              entry.func() == aux.func()));
        always_assert(IMPLIES(entry.funcFamily() && !aux.funcFamily(),
                              !entry.funcFamily()->m_regular));
      }
    }
  }

  // "Aux" entries should only exist for methods on this class, and
  // with a corresponding methodFamilies entry.
  for (auto const& [name, _] : cinfo->methodFamiliesAux) {
    always_assert(cinfo->methods.contains(name));
    always_assert(cinfo->methodFamilies.contains(name));
  }

  // We should only have func families for methods declared on this
  // class (except for interfaces and abstract classes).
  for (auto const& [name, entry] : cinfo->methodFamilies) {
    if (cinfo->methods.contains(name)) continue;
    // Interfaces and abstract classes can have func families for
    // methods not defined on this class.
    always_assert(cinfo->cls->attrs & (AttrInterface|AttrAbstract));
    // We don't expand func families for these.
    always_assert(name != s_construct.get() && !is_special_method_name(name));

    // We only expand entries for interfaces and abstract classes if
    // it appears in every regular subclass. Therefore it cannot be
    // empty and is complete.
    always_assert(!entry.isEmpty());
    always_assert(entry.isComplete());
    if (auto const ff = entry.funcFamily()) {
      always_assert(!ff->m_regular);
    } else if (auto const func = entry.func()) {
      always_assert(func->cls != cinfo->cls);
    }
  }

  // If the class is marked as having not having bad initial prop
  // values, all of it's properties should have AttrInitialSatisfiesTC
  // set. Likewise, if it is, at least one property should not have it
  // set.
  if (!cinfo->hasBadInitialPropValues) {
    auto const all = std::all_of(
      begin(cinfo->cls->properties),
      end(cinfo->cls->properties),
      [] (const php::Prop& p) {
        return p.attrs & AttrInitialSatisfiesTC;
      }
    );
    always_assert(all);
  } else {
    auto const someBad = std::any_of(
      begin(cinfo->cls->properties),
      end(cinfo->cls->properties),
      [] (const php::Prop& p) {
        return !(p.attrs & AttrInitialSatisfiesTC);
      }
    );
    always_assert(someBad);
  }

  if (is_closure_name(cinfo->cls->name)) {
    assertx(cinfo->classGraph.hasCompleteChildren());
    // Closures have no children.
    auto const subclasses = cinfo->classGraph.children();
    always_assert(subclasses.size() == 1);
    always_assert(subclasses[0].name()->tsame(cinfo->cls->name));
  } else if (cinfo->classGraph.hasCompleteChildren()) {
    // Otherwise the children list is non-empty, contains this
    // class, and contains only unique elements.
    auto const subclasses = cinfo->classGraph.children();
    always_assert(
      std::find_if(
        begin(subclasses),
        end(subclasses),
        [&] (ClassGraph g) { return g.name()->tsame(cinfo->cls->name); }
      ) != end(subclasses)
    );
    auto cpy = subclasses;
    std::sort(begin(cpy), end(cpy));
    cpy.erase(std::unique(begin(cpy), end(cpy)), end(cpy));
    always_assert(cpy.size() == subclasses.size());
  }

  // The base list is non-empty, and the last element is this class.
  auto const bases = cinfo->classGraph.bases();
  always_assert(!bases.empty());
  always_assert(cinfo->classGraph == bases.back());
  if (is_closure_base(cinfo->cls->name)) {
    always_assert(bases.size() == 1);
  } else if (is_closure_name(cinfo->cls->name)) {
    always_assert(bases.size() == 2);
    always_assert(bases[0].name()->tsame(s_Closure.get()));
  }
}

void check_local_invariants(const IndexData& data, const FuncFamily& ff) {
  // FuncFamily should always have more than one func on it.
  always_assert(ff.possibleFuncs().size() > 1);

  SString name{nullptr};
  FuncFamily::PossibleFunc last{nullptr, false};
  for (auto const pf : ff.possibleFuncs()) {
    // Should only contain methods
    always_assert(pf.ptr()->cls);

    // Every method on the list should have the same name.
    if (!name) {
      name = pf.ptr()->name;
    } else {
      always_assert(name == pf.ptr()->name);
    }

    // Verify the list is sorted and doesn't contain any duplicates.
    hphp_fast_set<const php::Func*> seen;
    if (last.ptr()) {
      always_assert(
        [&] {
          if (last.inRegular() && !pf.inRegular()) return true;
          if (!last.inRegular() && pf.inRegular()) return false;
          return string_data_lt_type{}(last.ptr()->cls->name, pf.ptr()->cls->name);
        }()
      );
    }
    always_assert(seen.emplace(pf.ptr()).second);
    last = pf;
  }

  if (!ff.possibleFuncs().front().inRegular() ||
      ff.possibleFuncs().back().inRegular()) {
    // If there's no funcs on a regular class, or if all functions are
    // on a regular class, we don't need to keep separate information
    // for the regular subset (it either doesn't exist, or it's equal to
    // the entire list).
    always_assert(!ff.m_regular);
  }
}

void check_local_invariants(const IndexData& data) {
  if (!debug) return;

  trace_time timer{"check-local-invariants"};

  parallel::for_each(
    data.allClassInfos,
    [&] (const std::unique_ptr<ClassInfo>& cinfo) {
      check_local_invariants(data, cinfo.get());
    }
  );

  std::vector<const FuncFamily*> funcFamilies;
  funcFamilies.reserve(data.funcFamilies.size());
  for (auto const& [ff, _] : data.funcFamilies) {
    funcFamilies.emplace_back(ff.get());
  }
  parallel::for_each(
    funcFamilies,
    [&] (const FuncFamily* ff) { check_local_invariants(data, *ff); }
  );
}

//////////////////////////////////////////////////////////////////////

Type adjust_closure_context(const IIndex& index, const CallContext& ctx) {
  if (ctx.callee->cls && ctx.callee->cls->closureContextCls) {
    auto const withClosureContext = Context {
      ctx.callee->unit,
      ctx.callee,
      index.lookup_closure_context(*ctx.callee->cls)
    };
    if (auto const s = selfCls(index, withClosureContext)) {
      return setctx(toobj(*s));
    }
    return TObj;
  }
  return ctx.context;
}

Index::ReturnType context_sensitive_return_type(IndexData& data,
                                                const Context& ctx,
                                                CallContext callCtx,
                                                Index::ReturnType returnType) {
  constexpr auto max_interp_nexting_level = 2;
  static __thread uint32_t interp_nesting_level;
  auto const finfo = func_info(data, callCtx.callee);

  auto const adjustedCtx = adjust_closure_context(
    IndexAdaptor { *data.m_index },
    callCtx
  );
  returnType.t = return_with_context(std::move(returnType.t), adjustedCtx);

  auto const checkParam = [&] (int i) {
    auto check = [&](const TypeConstraint& tc) {
      if (tc.hasConstraint() && !tc.isTypeVar() && !tc.isTypeConstant()) {
        return callCtx.args[i].strictlyMoreRefined(
          lookup_constraint(
            IndexAdaptor { *data.m_index },
            Context {finfo->func->unit, finfo->func, finfo->func->cls},
            tc
          ).upper
        );
      }
      return callCtx.args[i].strictSubtypeOf(TInitCell);
    };

    auto const& tcs = finfo->func->params[i].typeConstraints.range();
    return std::all_of(tcs.begin(), tcs.end(), check);
  };

  // TODO(#3788877): more heuristics here would be useful.
  auto const tryContextSensitive = [&] {
    if (finfo->func->noContextSensitiveAnalysis ||
        finfo->func->params.empty() ||
        interp_nesting_level + 1 >= max_interp_nexting_level ||
        returnType.t.is(BBottom)) {
      return false;
    }

    if (finfo->retParam != NoLocalId &&
        callCtx.args.size() > finfo->retParam &&
        checkParam(finfo->retParam)) {
      return true;
    }

    if (!options.ContextSensitiveInterp) return false;

    if (callCtx.args.size() < finfo->func->params.size()) return true;
    for (auto i = 0; i < finfo->func->params.size(); i++) {
      if (finfo->func->params[i].outOnly) continue;
      if (checkParam(i)) return true;
    }
    return false;
  }();

  if (!tryContextSensitive) return returnType;

  {
    ContextRetTyMap::const_accessor acc;
    if (data.contextualReturnTypes.find(acc, callCtx)) {
      if (data.frozen ||
          acc->second.t.is(BBottom) ||
          is_scalar(acc->second.t)) {
        return acc->second;
      }
    }
  }

  if (data.frozen) return returnType;

  auto contextType = [&] {
    ++interp_nesting_level;
    SCOPE_EXIT { --interp_nesting_level; };

    auto const func = finfo->func;
    auto const wf = php::WideFunc::cns(func);
    auto const calleeCtx = AnalysisContext {
      func->unit,
      wf,
      func->cls,
      &ctx.forDep()
    };
    auto fa = analyze_func_inline(
      IndexAdaptor { *data.m_index },
      calleeCtx,
      adjustedCtx,
      callCtx.args
    );
    return Index::ReturnType{
      return_with_context(std::move(fa.inferredReturn), adjustedCtx),
      fa.effectFree
    };
  }();

  if (!interp_nesting_level) {
    FTRACE(3,
           "Context sensitive type: {}\n"
           "Context insensitive type: {}\n",
           show(contextType.t), show(returnType.t));
  }

  if (!returnType.t.subtypeOf(BUnc)) {
    // If the context insensitive return type could be non-static, staticness
    // could be a result of temporary context sensitive bytecode optimizations.
    contextType.t = loosen_staticness(std::move(contextType.t));
  }

  auto ret = Index::ReturnType{
    intersection_of(std::move(returnType.t), std::move(contextType.t)),
    returnType.effectFree && contextType.effectFree
  };

  if (!interp_nesting_level) {
    FTRACE(3, "Context sensitive result: {}\n", show(ret.t));
  }

  ContextRetTyMap::accessor acc;
  if (data.contextualReturnTypes.insert(acc, callCtx) ||
      ret.t.strictSubtypeOf(acc->second.t) ||
      (ret.effectFree && !acc->second.effectFree)) {
    acc->second = ret;
  }

  return ret;
}

//////////////////////////////////////////////////////////////////////

Index::ReturnType context_sensitive_return_type(AnalysisIndex::IndexData& data,
                                                const CallContext& callCtx,
                                                Index::ReturnType returnType) {
  constexpr size_t maxNestingLevel = 2;

  using R = Index::ReturnType;

  auto const& func = *callCtx.callee;

  if (data.mode == AnalysisMode::Constants) {
    ITRACE_MOD(
      Trace::hhbbc, 4,
      "Skipping inline interp of {} because analyzing constants\n",
      func_fullname(func)
    );
    return returnType;
  }

  auto const& finfo = func_info(data, func);
  auto const& caller = first_func_context(data);

  auto const adjustedCtx = adjust_closure_context(
    AnalysisIndexAdaptor { data.index },
    callCtx
  );
  returnType.t = return_with_context(std::move(returnType.t), adjustedCtx);

  auto const checkParam = [&] (size_t i) {
    auto check = [&] (const TypeConstraint& tc) {
      if (tc.hasConstraint() && !tc.isTypeVar() && !tc.isTypeConstant()) {
        return callCtx.args[i].strictlyMoreRefined(
          lookup_constraint(
            AnalysisIndexAdaptor { data.index },
            Context { func.unit, &func, func.cls },
            tc
          ).upper
        );
      }
      return callCtx.args[i].strictSubtypeOf(TInitCell);
    };

    auto const& tcs = func.params[i].typeConstraints.range();
    return std::all_of(tcs.begin(), tcs.end(), check);
  };

  // TODO(#3788877): more heuristics here would be useful.
  auto const tryContextSensitive = [&] {
    if (func.noContextSensitiveAnalysis ||
        func.params.empty() ||
        data.contextualInterpNestingLevel + 1 >= maxNestingLevel ||
        returnType.t.is(BBottom)) {
      return false;
    }

    data.deps->add(func, AnalysisDeps::RetParam);
    auto ret = finfo.inferred.retParam;
    if (auto const rinfo = retained_for_context(data)) {
      auto better = false;
      if (auto const info = rinfo->get(finfo)) {
        if (info->retParam != NoLocalId) {
          if (ret == NoLocalId) {
            ret = info->retParam;
            better = true;
          } else {
            always_assert_flog(
              ret == info->retParam,
              "Inferred return param {} and retained return param {} "
              "for {} are contradictory",
              ret,
              info->retParam,
              func_fullname(func)
            );
            assertx(ret == info->retParam);
          }
        }
      }

      if (should_retain(finfo, better, data)) {
        ITRACE_MOD(
          Trace::hhbbc, 4,
          "Retaining inferred return param information ({}) about {}\n",
          (ret == NoLocalId) ? "-" : folly::sformat("{}", ret),
          func_fullname(func)
        );
        auto& info = rinfo->retain(finfo);
        info.retParam = ret;
      }

      if (ret != NoLocalId && ret < callCtx.args.size() && checkParam(ret)) {
        return true;
      }
    }

    if (!options.ContextSensitiveInterp) return false;

    auto const numParams = func.params.size();
    if (callCtx.args.size() < numParams) return true;
    for (size_t i = 0; i < numParams; ++i) {
      if (checkParam(i)) return true;
    }
    return false;
  }();

  if (!tryContextSensitive) {
    ITRACE_MOD(Trace::hhbbc, 4, "not trying context sensitive\n");
    return returnType;
  }

  data.deps->add(func, AnalysisDeps::Bytecode);
  if (!func.rawBlocks) {
    ITRACE_MOD(
      Trace::hhbbc, 4,
      "Skipping inline interp of {} because bytecode not present\n",
      func_fullname(func)
    );
    return returnType;
  }

  auto contextType = [&] {
    ++data.contextualInterpNestingLevel;
    SCOPE_EXIT { --data.contextualInterpNestingLevel; };

    auto const wf = php::WideFunc::cns(&func);
    auto fa = analyze_func_inline(
      AnalysisIndexAdaptor { data.index },
      AnalysisContext {
        func.unit,
        wf,
        func.cls,
        &context_for_deps(data)
      },
      adjustedCtx,
      callCtx.args
    );
    return R{
      return_with_context(std::move(fa.inferredReturn), std::move(adjustedCtx)),
      fa.effectFree
    };
  }();

  ITRACE_MOD(
    Trace::hhbbc, 4,
    "Context sensitive type: {}, context insensitive type: {}\n",
    show(contextType.t), show(returnType.t)
  );

  auto const error_context = [&] {
    using namespace folly::gen;
    return folly::sformat(
      "{} calling {} (context: {}, args: {})",
      show(caller),
      func_fullname(func),
      show(callCtx.context),
      from(callCtx.args)
        | map([] (const Type& t) { return show(t); })
        | unsplit<std::string>(",")
    );
  };

  // The context sensitive type could be a subtype of the insensitive
  // type if the analysis took advantage of the known arguments. On
  // the other-hand, it could be a supertype of the insensitive type
  // if we didn't have the same dependencies present as when the
  // insensitive type was produced. So, we cannot make any assumptions
  // about the two's relationship (except that they must have a
  // non-empty intersection).
  always_assert_flog(
    contextType.t.is(BBottom) || contextType.t.couldBe(returnType.t),
    "Context sensitive return type for {} is {} "
    "which is not compatible with context insensitive "
    "return type {}\n",
    error_context(),
    show(contextType.t),
    show(returnType.t)
  );

  contextType.t &= returnType.t;
  contextType.effectFree |= returnType.effectFree;
  return contextType;
}

//////////////////////////////////////////////////////////////////////

template<typename F> auto
visit_parent_cinfo(const ClassInfo* cinfo, F fun) -> decltype(fun(cinfo)) {
  for (auto ci = cinfo; ci != nullptr; ci = ci->parent) {
    if (auto const ret = fun(ci)) return ret;
    for (auto const trait : ci->usedTraits) {
      if (auto const ret = visit_parent_cinfo(trait, fun)) {
        return ret;
      }
    }
  }
  return {};
}

//////////////////////////////////////////////////////////////////////

// The type of a public static property, considering only it's initial
// value.
Type initial_type_for_public_sprop(const Index& index,
                                   const php::Class& cls,
                                   const php::Prop& prop) {
  /*
   * If the initializer type is TUninit, it means an 86sinit provides
   * the actual initialization type or it is AttrLateInit. So we don't
   * want to include the Uninit (which isn't really a user-visible
   * type for the property) or by the time we union things in we'll
   * have inferred nothing much.
   */
  auto const ty = from_cell(prop.val);
  if (ty.subtypeOf(BUninit)) return TBottom;
  if (prop.attrs & AttrSystemInitialValue) return ty;
  return adjust_type_for_prop(
    IndexAdaptor { index },
    cls,
    &prop.typeConstraints,
    ty
  );
}

Type initial_type_for_public_sprop(const AnalysisIndex::IndexData& index,
                                   const php::Class& cls,
                                   const php::Prop& prop) {
  /*
   * If the initializer type is TUninit, it means an 86sinit provides
   * the actual initialization type or it is AttrLateInit. So we don't
   * want to include the Uninit (which isn't really a user-visible
   * type for the property) or by the time we union things in we'll
   * have inferred nothing much.
   */
  auto const ty = from_cell(prop.val);
  if (ty.subtypeOf(BUninit)) {
    index.deps->add(
      AnalysisDeps::Class { cls.name },
      AnalysisDeps::Type::PropInitVals
    );
    return TBottom;
  }
  if (prop.attrs & AttrSystemInitialValue) return ty;
  return adjust_type_for_prop(
    AnalysisIndexAdaptor { index.index },
    cls,
    &prop.typeConstraints,
    ty
  );
}

Type lookup_public_prop_impl(
  const IndexData& data,
  const ClassInfo* cinfo,
  SString propName
) {
  // Find a property declared in this class (or a parent) with the same name.
  const php::Class* knownCls = nullptr;
  auto const prop = visit_parent_cinfo(
    cinfo,
    [&] (const ClassInfo* ci) -> const php::Prop* {
      for (auto const& prop : ci->cls->properties) {
        if (prop.name == propName) {
          knownCls = ci->cls;
          return &prop;
        }
      }
      return nullptr;
    }
  );

  if (!prop) return TCell;
  // Make sure its non-static and public. Otherwise its another function's
  // problem.
  if (prop->attrs & (AttrStatic | AttrPrivate)) return TCell;

  // Get a type corresponding to its declared type-hint (if any).
  auto ty = adjust_type_for_prop(
    IndexAdaptor { *data.m_index },
    *knownCls,
    &prop->typeConstraints,
    TCell
  );
  // We might have to include the initial value which might be outside of the
  // type-hint.
  auto initialTy = loosen_all(from_cell(prop->val));
  if (!initialTy.subtypeOf(TUninit) && (prop->attrs & AttrSystemInitialValue)) {
    ty |= initialTy;
  }
  return ty;
}

// Test if the given property (declared in `cls') is accessible in the
// given context (null if we're not in a class).
template <typename C>
bool static_is_accessible(const C* clsCtx,
                          const C* cls,
                          const php::Prop& prop) {
  assertx(prop.attrs & AttrStatic);
  switch (prop.attrs & (AttrPublic|AttrProtected|AttrPrivate)) {
    case AttrPublic:
      // Public is accessible everywhere
      return true;
    case AttrProtected:
      // Protected is accessible from both derived classes and parent
      // classes
      return clsCtx &&
        (clsCtx->classGraph.exactSubtypeOf(cls->classGraph, true, true) ||
         cls->classGraph.exactSubtypeOf(clsCtx->classGraph, true, true));
    case AttrPrivate:
      // Private is only accessible from within the declared class
      return clsCtx == cls;
  }
  always_assert(false);
}

// Return true if the given class can possibly throw when its
// initialized. Initialization can happen when an object of that class
// is instantiated, or (more importantly) when static properties are
// accessed.
bool class_init_might_raise(IndexData& data,
                            Context ctx,
                            const ClassInfo* cinfo) {
  // Check this class and all of its parents for possible inequivalent
  // redeclarations or bad initial values.
  do {
    // Be conservative for now if we have unflattened traits.
    if (!cinfo->traitProps.empty()) return true;
    if (cinfo->hasBadRedeclareProp) return true;
    if (cinfo->hasBadInitialPropValues) {
      add_dependency(data, cinfo->cls, ctx, Dep::PropBadInitialValues);
      return true;
    }
    cinfo = cinfo->parent;
  } while (cinfo);
  return false;
}

// Return true if the given class can possibly throw when its
// initialized. Initialization can happen when an object of that class
// is instantiated, or when static properties are accessed. Registers
// dependencies on parent classes to ensure they're re-analyzed if
// initialization behavior changes.
bool class_init_might_raise(AnalysisIndex::IndexData& index,
                            const DCls& dcls) {
  // Check this class and all of its parents for possible inequivalent
  // redeclarations or bad initial values.
  auto const walk = [&] (const ClassInfo2& cinfo) {
    auto might = false;

    cinfo.classGraph.walkParents(
      [&] (ClassGraph parent) {
        parent.ensureCInfo();
        auto const pinfo = parent.cinfo2();
        if (!pinfo) {
          might = true;
          // Keep going so we can record as many dependencies as we
          // can.
          return true;
        }
        // Be conservative for now if we have unflattened traits.
        if (!pinfo->traitProps.empty()) {
          might = true;
          return false;
        }
        if (pinfo->hasBadRedeclareProp) {
          might = true;
          return false;
        }
        // This is the only thing which can actually change from
        // analysis
        if (pinfo->hasBadInitialPropValues) {
          // Only register the dependency if it is true. If it is
          // false, then it must stay false, so there's no need for a
          // specific dependency on it.
          index.deps->add(
            AnalysisDeps::Class { pinfo->name },
            AnalysisDeps::Type::ClassInitMightRaise
          );
          might = true;
          return false;
        }
        return true;
      }
    );

    return might;
  };

  auto const onRCls = [&] (res::Class rcls) {
    rcls.graph().ensureCInfo();
    auto const cinfo = rcls.graph().cinfo2();
    if (!cinfo) return true;
    return walk(*cinfo);
  };

  if (dcls.isExact() || dcls.isSub()) {
    return onRCls(dcls.cls());
  } else if (dcls.isIsect()) {
    auto const& isect = dcls.isect();
    assertx(isect.size() > 1);
    auto might = true;
    for (auto const r : isect) might &= onRCls(r);
    return might;
  } else {
    // Even though this has an intersection list, it must be the exact
    // class, so it's sufficient to process that.
    assertx(dcls.isIsectAndExact());
    return onRCls(dcls.isectAndExact().first);
  }
}

/*
 * Calculate the effects of applying the given type against the
 * type-constraints for the given prop. This includes the subtype
 * which will succeed (if any), and if the type-constraint check might
 * throw.
 */
PropMergeResult prop_tc_effects(const IIndex& index,
                                const php::Class& cls,
                                const php::Prop& prop,
                                const Type& val,
                                bool checkUB) {
  assertx(prop.typeConstraints.validForProp());

  using R = PropMergeResult;

  // If we're not actually checking property type-hints, everything
  // goes
  if (Cfg::Eval::CheckPropTypeHints <= 0) return R{ val, TriBool::No };

  auto const ctx = Context { nullptr, nullptr, &cls };

  auto const check = [&] (const TypeConstraint& tc, const Type& t) {
    // If the type as is satisfies the constraint, we won't throw and
    // the type is unchanged.
    if (t.moreRefined(lookup_constraint(index, ctx, tc, t).lower)) {
      return R{ t, TriBool:: No };
    }
    // Otherwise adjust the type. If we get a Bottom we'll definitely
    // throw. We already know the type doesn't completely satisfy the
    // constraint, so we'll at least maybe throw.
    auto adjusted = adjust_type_for_prop(index, *ctx.cls, tc, t);
    auto const throws = yesOrMaybe(adjusted.subtypeOf(BBottom));
    return R{ std::move(adjusted), throws };
  };

  R result{val, TriBool::No};
  for (auto const& tc : prop.typeConstraints.range()) {
    if (checkUB || !tc.isUpperBound()) {
      // Otherwise check every eligible type constraint. We'll feed the
      // narrowed type into each successive round. If we reach the point
      // where we'll know we'll definitely fail, just stop.
      auto r = check(tc, result.adjusted);
      result.throws &= r.throws;
      result.adjusted = std::move(r.adjusted);
      if (result.throws == TriBool::Yes) break;
    }
  }
  return result;
}

/*
 * Lookup data for the static property named `propName', starting from
 * the specified class `start'. If `propName' is nullptr, then any
 * accessible static property in the class hierarchy is considered. If
 * `startOnly' is specified, if the property isn't found in `start',
 * it is treated as a lookup failure. Otherwise the lookup continues
 * in all parent classes of `start', until a property is found, or
 * until all parent classes have been exhausted (`startOnly' is used
 * to avoid redundant class hierarchy walks). `clsCtx' is the current
 * context, converted to a ClassInfo* (or nullptr if not in a class).
*/
PropLookupResult lookup_static_impl(IndexData& data,
                                    Context ctx,
                                    const ClassInfo* clsCtx,
                                    const PropertiesInfo& privateProps,
                                    const ClassInfo* start,
                                    SString propName,
                                    bool startOnly) {
  ITRACE(
    6, "lookup_static_impl: {} {} {}\n",
    clsCtx ? clsCtx->cls->name->toCppString() : std::string{"-"},
    start->cls->name,
    propName ? propName->toCppString() : std::string{"*"}
  );
  Trace::Indent _;

  auto const type = [&] (const php::Prop& prop,
                         const ClassInfo* ci) {
    switch (prop.attrs & (AttrPublic|AttrProtected|AttrPrivate)) {
      case AttrPublic:
      case AttrProtected: {
        if (ctx.unit) add_dependency(data, &prop, ctx, Dep::PublicSProp);
        if (!data.seenPublicSPropMutations) {
          // If we haven't recorded any mutations yet, we need to be
          // conservative and consider only the type-hint and initial
          // value.
          return union_of(
            adjust_type_for_prop(
              IndexAdaptor { *data.m_index },
              *ci->cls,
              &prop.typeConstraints,
              TInitCell
            ),
            initial_type_for_public_sprop(
              *data.m_index,
              *ci->cls, prop
            )
          );
        }
        auto const it = ci->publicStaticProps.find(propName);
        if (it == end(ci->publicStaticProps)) {
          // We've recorded mutations, but have no information for
          // this property. That means there's no mutations so only
          // consider the initial value.
          return initial_type_for_public_sprop(
            *data.m_index,
            *ci->cls,
            prop
          );
        }
        return it->second.inferredType;
      }
      case AttrPrivate: {
        assertx(clsCtx == ci);
        auto const elem = privateProps.readPrivateStatic(prop.name);
        if (!elem) return TInitCell;
        return remove_uninit(elem->ty);
      }
    }
    always_assert(false);
  };

  auto const initMightRaise = class_init_might_raise(data, ctx, start);

  auto const fromProp = [&] (const php::Prop& prop,
                             const ClassInfo* ci) {
    // The property was definitely found. Compute its attributes
    // from the prop metadata.
    return PropLookupResult{
      type(prop, ci),
      propName,
      TriBool::Yes,
      yesOrNo(prop.attrs & AttrIsConst),
      yesOrNo(prop.attrs & AttrIsReadonly),
      yesOrNo(prop.attrs & AttrLateInit),
      yesOrNo(prop.attrs & AttrInternal),
      initMightRaise
    };
  };

  auto const notFound = [&] {
    // The property definitely wasn't found.
    return PropLookupResult{
      TBottom,
      propName,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      false
    };
  };

  if (!propName) {
    // We don't statically know the prop name. Walk up the hierarchy
    // and union the data for any accessible static property.
    ITRACE(4, "no prop name, considering all accessible\n");
    auto result = notFound();
    visit_parent_cinfo(
      start,
      [&] (const ClassInfo* ci) {
        for (auto const& prop : ci->cls->properties) {
          if (!(prop.attrs & AttrStatic) ||
              !static_is_accessible(clsCtx, ci, prop)) {
            ITRACE(
              6, "skipping inaccessible {}::${}\n",
              ci->cls->name, prop.name
            );
            continue;
          }
          auto const r = fromProp(prop, ci);
          ITRACE(6, "including {}:${} {}\n", ci->cls->name, prop.name, show(r));
          result |= r;
        }
        // If we're only interested in the starting class, don't walk
        // up to the parents.
        return startOnly;
      }
    );
    return result;
  }

  // We statically know the prop name. Walk up the hierarchy and stop
  // at the first matching property and use that data.
  assertx(!startOnly);
  auto const result = visit_parent_cinfo(
    start,
    [&] (const ClassInfo* ci) -> Optional<PropLookupResult> {
      for (auto const& prop : ci->cls->properties) {
        if (prop.name != propName) continue;
        // We have a matching prop. If its not static or not
        // accessible, the access will not succeed.
        if (!(prop.attrs & AttrStatic) ||
            !static_is_accessible(clsCtx, ci, prop)) {
          ITRACE(
            6, "{}::${} found but inaccessible, stopping\n",
            ci->cls->name, propName
          );
          return notFound();
        }
        // Otherwise its a match
        auto const r = fromProp(prop, ci);
        ITRACE(6, "found {}:${} {}\n", ci->cls->name, propName, show(r));
        return r;
      }
      return std::nullopt;
    }
  );
  if (!result) {
    // We walked up to all of the base classes and didn't find a
    // property with a matching name. The access will fail.
    ITRACE(6, "nothing found\n");
    return notFound();
  }
  return *result;
}

/*
 * Lookup the static property named `propName', starting from the
 * specified class `start'. If an accessible property is found, then
 * merge the given type `val' into the already known type for that
 * property. If `propName' is nullptr, then any accessible static
 * property in the class hierarchy is considered. If `startOnly' is
 * specified, if the property isn't found in `start', then the nothing
 * is done. Otherwise the lookup continues in all parent classes of
 * `start', until a property is found, or until all parent classes
 * have been exhausted (`startOnly' is to avoid redundant class
 * hierarchy walks). `clsCtx' is the current context, converted to a
 * ClassInfo* (or nullptr if not in a class). If `ignoreConst' is
 * false, then AttrConst properties will not have their type
 * modified. `mergePublic' is a lambda with the logic to merge a type
 * for a public property (this is needed to avoid cyclic
 * dependencies).
 */
template <typename F>
PropMergeResult merge_static_type_impl(IndexData& data,
                                       Context ctx,
                                       F mergePublic,
                                       PropertiesInfo& privateProps,
                                       const ClassInfo* clsCtx,
                                       const ClassInfo* start,
                                       SString propName,
                                       const Type& val,
                                       bool checkUB,
                                       bool ignoreConst,
                                       bool mustBeReadOnly,
                                       bool startOnly) {
  ITRACE(
    6, "merge_static_type_impl: {} {} {} {}\n",
    clsCtx ? clsCtx->cls->name->toCppString() : std::string{"-"},
    start->cls->name,
    propName ? propName->toCppString() : std::string{"*"},
    show(val)
  );
  Trace::Indent _;

  assertx(!val.subtypeOf(BBottom));

  // Perform the actual merge for a given property, returning the
  // effects of that merge.
  auto const merge = [&] (const php::Prop& prop, const ClassInfo* ci) {
    // First calculate the effects of the type-constraint.
    auto const effects = prop_tc_effects(
      IndexAdaptor { *data.m_index },
      *ci->cls,
      prop,
      val,
      checkUB
    );
    // No point in merging if the type-constraint will always fail.
    if (effects.throws == TriBool::Yes) {
      ITRACE(
        6, "tc would throw on {}::${} with {}, skipping\n",
        ci->cls->name, prop.name, show(val)
      );
      return effects;
    }
    assertx(!effects.adjusted.subtypeOf(BBottom));

    ITRACE(
      6, "merging {} into {}::${}\n",
      show(effects), ci->cls->name, prop.name
    );

    switch (prop.attrs & (AttrPublic|AttrProtected|AttrPrivate)) {
      case AttrPublic:
      case AttrProtected:
        mergePublic(ci, prop, unctx(effects.adjusted));
         // If the property is internal, accessing it may throw
         // TODO(T131951529): we can do better by checking modules here
        if ((prop.attrs & AttrInternal) && effects.throws == TriBool::No) {
          ITRACE(6, "{}::${} is internal, "
                 "being pessimistic with regards to throwing\n",
                 ci->cls->name, prop.name);
          return PropMergeResult{
            effects.adjusted,
            TriBool::Maybe
          };
        }
        return effects;
      case AttrPrivate: {
        assertx(clsCtx == ci);
        privateProps.mergeInPrivateStaticPreAdjusted(
          prop.name,
          unctx(effects.adjusted)
        );
        return effects;
      }
    }
    always_assert(false);
  };

  // If we don't find a property, then the mutation will definitely
  // fail.
  auto const notFound = [&] {
    return PropMergeResult{
      TBottom,
      TriBool::Yes
    };
  };

  if (!propName) {
    // We don't statically know the prop name. Walk up the hierarchy
    // and merge the type for any accessible static property.
    ITRACE(6, "no prop name, considering all accessible\n");
    auto result = notFound();
    visit_parent_cinfo(
      start,
      [&] (const ClassInfo* ci) {
        for (auto const& prop : ci->cls->properties) {
          if (!(prop.attrs & AttrStatic) ||
              !static_is_accessible(clsCtx, ci, prop)) {
            ITRACE(
              6, "skipping inaccessible {}::${}\n",
              ci->cls->name, prop.name
            );
            continue;
          }
          if (!ignoreConst && (prop.attrs & AttrIsConst)) {
            ITRACE(6, "skipping const {}::${}\n", ci->cls->name, prop.name);
            continue;
          }
          if (mustBeReadOnly && !(prop.attrs & AttrIsReadonly)) {
            ITRACE(6, "skipping mutable property that must be readonly {}::${}\n",
              ci->cls->name, prop.name);
            continue;
          }
          result |= merge(prop, ci);
        }
        return startOnly;
      }
    );
    return result;
  }

  // We statically know the prop name. Walk up the hierarchy and stop
  // at the first matching property and merge the type there.
  assertx(!startOnly);
  auto result = visit_parent_cinfo(
    start,
    [&] (const ClassInfo* ci) -> Optional<PropMergeResult> {
      for (auto const& prop : ci->cls->properties) {
        if (prop.name != propName) continue;
        // We found a property with the right name, but its
        // inaccessible from this context (or not even static). This
        // mutation will fail, so we don't need to modify the type.
        if (!(prop.attrs & AttrStatic) ||
            !static_is_accessible(clsCtx, ci, prop)) {
          ITRACE(
            6, "{}::${} found but inaccessible, stopping\n",
            ci->cls->name, propName
          );
          return notFound();
        }
        // Mutations to AttrConst properties will fail as well, unless
        // it we want to override that behavior.
        if (!ignoreConst && (prop.attrs & AttrIsConst)) {
          ITRACE(
            6, "{}:${} found but const, stopping\n",
            ci->cls->name, propName
          );
          return notFound();
        }
        if (mustBeReadOnly && !(prop.attrs & AttrIsReadonly)) {
          ITRACE(
            6, "{}:${} found but is mutable and must be readonly, stopping\n",
            ci->cls->name, propName
          );
          return notFound();
        }
        return merge(prop, ci);
      }
      return std::nullopt;
    }
  );
  if (!result) {
    ITRACE(6, "nothing found\n");
    return notFound();
  }

  // If the mutation won't throw, we still need to check if the class
  // initialization can throw. If we might already throw (or
  // definitely will throw), this doesn't matter.
  if (result->throws == TriBool::No) {
    return PropMergeResult{
      std::move(result->adjusted),
      maybeOrNo(class_init_might_raise(data, ctx, start))
    };
  }
  return *result;
}

//////////////////////////////////////////////////////////////////////

/*
 * Split a group of buckets so that no bucket is larger (including its
 * dependencies) than the given max size. The given callable is used
 * to add to the dependencies of bucket item.
 *
 * Note: if a single item has dependencies larger than maxSize, you'll
 * get a bucket with just that and its dependencies (which will be
 * larger than maxSize). This is the only situation where a returned
 * bucket will be larger than maxSize.
 */
template <typename AddDeps>
std::vector<std::vector<SString>>
split_buckets(const std::vector<std::vector<SString>>& items,
              size_t maxSize,
              const AddDeps& addDeps) {
  // Split all of the buckets in parallel
  auto rebuckets = parallel::map(
    items,
    [&] (const std::vector<SString>& bucket) {
      // If there's only one thing in a bucket, there's no point in
      // splitting it.
      if (bucket.size() <= 1) return singleton_vec(bucket);

      // The splitting algorithm is simple. Iterate over each element
      // in the bucket. As long as all the dependencies are less than
      // the maximum size, we put it into a new bucket. If we exceed
      // the max size, create a new bucket.
      std::vector<std::vector<SString>> out;
      out.emplace_back();
      out.back().emplace_back(bucket[0]);

      TSStringSet deps;
      addDeps(bucket[0], deps);
      for (size_t i = 1, size = bucket.size(); i < size; ++i) {
        addDeps(bucket[i], deps);
        auto const newSize = deps.size() + out.back().size() + 1;

        if (newSize > maxSize) {
          deps.clear();
          addDeps(bucket[i], deps);
          out.emplace_back();
        }
        out.back().emplace_back(bucket[i]);
      }

      return out;
    }
  );

  // Flatten all of the new buckets into a single list of buckets.
  std::vector<std::vector<SString>> flattened;
  flattened.reserve(items.size());
  for (auto& r : rebuckets) {
    for (auto& b : r) flattened.emplace_back(std::move(b));
  }
  return flattened;
}

//////////////////////////////////////////////////////////////////////

/*
 * For efficiency reasons, we often want to process classes in as few
 * passes as possible. However, this is tricky because the algorithms
 * are usually naturally iterative. You start at the root classes
 * (which may be the top classes in the hierarchy, or leaf classes),
 * and flow data down to each of their parents or children. This
 * requires N passes, where N is the maximum depth of the class
 * hierarchy. N can get large.
 *
 * Instead when we process a class, we ensure that all of it's
 * dependencies (all the way up to the roots) are also present in the
 * job. Since we're doing this in one pass, none of the dependencies
 * will have any calculated information, and the job will have to do
 * this first.
 *
 * It is not, in general, possible to ensure that each dependency is
 * present in exactly one job (because the dependency may be shared by
 * lots of classes which are not bucketed together). So, any given
 * dependency may end up on multiple jobs and have the same
 * information calculated for it. This is fine, as it just results in
 * some wasted work.
 *
 * We perform flattening using the following approach:
 *
 * - First we Bucketize the root classes (using the standard
 *   consistent hashing algorithm) into N buckets.
 *
 * - We split any buckets which are larger than the specified maximum
 *   size. This prevents buckets from becoming pathologically large if
 *   there's many dependencies.
 *
 * - For each bucket, find all of the (transitive) dependencies of the
 *   leaves and add them to that bucket (as dependencies). As stated
 *   above, the same class may end up in multiple buckets as
 *   dependencies.
 *
 * - So far for each bucket (each bucket will map to one job), we have
 *   a set of input classes (the roots), and all of the dependencies
 *   for each roots.
 *
 * - We want results for every class, not just the roots, so the
 *   dependencies need to become inputs of the first kind in at least
 *   one bucket. So, for each dependency, in one of the buckets
 *   they're already present in, we "promote" it to a full input (and
 *   will receive output for it). This is done by hashing the bucket
 *   index and class name and picking the bucket that results in the
 *   lowest hash. In some situations we don't want a dependency to
 *   ever be promoted, so those will be skipped.
*/

// Single output bucket for assign_hierarchical_work. Each bucket
// contains classes which will be processed and returned as output,
// and a set of dependency classes which will just be used as inputs.
struct HierarchicalWorkBucket {
  std::vector<SString> classes;
  std::vector<SString> deps;
  std::vector<SString> uninstantiable;
};

/*
 * Assign work for a set of root classes (using the above
 * algorithm). The function is named because it's meant for situations
 * where we're processing classes in a "hierarchical" manner (either
 * from parent class to children, or from leaf class to parents).
 *
 * The dependencies for each class is provided by the addDeps
 * callable. For the purposes of promoting a class to a full output
 * (see above algorithm description), each class must be assigned an
 * index. The (optional) index for a class is provided by the getIdx
 * callable. If getIdx returns std::nullopt, then that class won't be
 * considered for promotion. The given "numClasses" parameter is an
 * upper bound on the possible returned indices.
 */
template <typename AddDeps, typename IsInstan, typename GetIdx>
std::vector<HierarchicalWorkBucket>
build_hierarchical_work(std::vector<std::vector<SString>>& buckets,
                        size_t numClasses,
                        const AddDeps& addDeps,
                        const IsInstan& isInstan,
                        const GetIdx& getIdx) {
  struct DepHashState {
    std::mutex lock;
    size_t lowestHash{std::numeric_limits<size_t>::max()};
    size_t lowestBucket{std::numeric_limits<size_t>::max()};
  };
  std::vector<DepHashState> depHashState{numClasses};

  // For each bucket (which right now just contains the root classes),
  // find all the transitive dependencies those root classes need. A
  // dependency might end up in multiple buckets (because multiple
  // roots in different buckets depend on it). We only want to
  // actually perform the flattening for those dependencies in one of
  // the buckets. So, we need a tie-breaker. We hash the name of the
  // dependency along with the bucket number. The bucket that the
  // dependency is present in with the lowest hash is what "wins".
  auto const bucketDeps = parallel::gen(
    buckets.size(),
    [&] (size_t bucketIdx) {
      assertx(bucketIdx < buckets.size());
      auto& bucket = buckets[bucketIdx];
      const TSStringSet roots{begin(bucket), end(bucket)};

      // Gather up all dependencies for this bucket
      TSStringSet deps;
      for (auto const cls : bucket) addDeps(cls, deps);

      // Make sure dependencies and roots are disjoint.
      for (auto const c : bucket) deps.erase(c);

      // For each dependency, store the bucket with the lowest hash.
      for (auto const d : deps) {
        auto const idx = getIdx(roots, bucketIdx, d);
        if (!idx.has_value()) continue;
        assertx(*idx < depHashState.size());
        auto& s = depHashState[*idx];
        auto const hash = hash_int64_pair(
          d->hashStatic(),
          bucketIdx
        );
        std::lock_guard<std::mutex> _{s.lock};
        if (hash < s.lowestHash) {
          s.lowestHash = hash;
          s.lowestBucket = bucketIdx;
        } else if (hash == s.lowestHash) {
          s.lowestBucket = std::min(s.lowestBucket, bucketIdx);
        }
      }

      return deps;
    }
  );

  // Now for each bucket, "promote" dependencies into a full input
  // class. The dependency is promoted in the bucket with the lowest
  // hash, which we've already calculated.
  assertx(buckets.size() == bucketDeps.size());
  return parallel::gen(
    buckets.size(),
    [&] (size_t bucketIdx) {
      auto& bucket = buckets[bucketIdx];
      auto const& deps = bucketDeps[bucketIdx];
      const TSStringSet roots{begin(bucket), end(bucket)};

      std::vector<SString> depOut;
      depOut.reserve(deps.size());

      for (auto const d : deps) {
        // Calculate the hash for the dependency for this bucket. If
        // the hash equals the already calculated lowest hash, promote
        // this dependency.
        auto const idx = getIdx(roots, bucketIdx, d);
        if (!idx.has_value()) {
          depOut.emplace_back(d);
          continue;
        }
        assertx(*idx < depHashState.size());
        auto const& s = depHashState[*idx];
        auto const hash = hash_int64_pair(
          d->hashStatic(),
          bucketIdx
        );
        if (hash == s.lowestHash && bucketIdx == s.lowestBucket) {
          bucket.emplace_back(d);
        } else if (isInstan(d)) {
          // Otherwise keep it as a dependency, but only if it's
          // actually instantiable.
          depOut.emplace_back(d);
        }
      }

      // Split off any uninstantiable classes in the bucket.
      auto const bucketEnd = std::partition(
        begin(bucket),
        end(bucket),
        [&] (SString cls) { return isInstan(cls); }
      );
      std::vector<SString> uninstantiable{bucketEnd, end(bucket)};
      bucket.erase(bucketEnd, end(bucket));

      // Keep deterministic ordering. Make sure there's no duplicates.
      std::sort(bucket.begin(), bucket.end(), string_data_lt_type{});
      std::sort(depOut.begin(), depOut.end(), string_data_lt_type{});
      std::sort(uninstantiable.begin(), uninstantiable.end(),
                string_data_lt_type{});
      assertx(std::adjacent_find(bucket.begin(), bucket.end()) == bucket.end());
      assertx(std::adjacent_find(depOut.begin(), depOut.end()) == depOut.end());
      assertx(
        std::adjacent_find(uninstantiable.begin(), uninstantiable.end()) ==
        uninstantiable.end()
      );

      bucket.shrink_to_fit();
      depOut.shrink_to_fit();
      uninstantiable.shrink_to_fit();

      return HierarchicalWorkBucket{
        std::move(bucket),
        std::move(depOut),
        std::move(uninstantiable)
      };
    }
  );
}

template <typename AddDeps, typename IsInstan, typename GetIdx>
std::vector<HierarchicalWorkBucket>
assign_hierarchical_work(std::vector<SString> roots,
                         size_t numClasses,
                         size_t bucketSize,
                         size_t maxSize,
                         const AddDeps& addDeps,
                         const IsInstan& isInstan,
                         const GetIdx& getIdx) {
  // First turn roots into buckets, and split if any exceed the
  // maximum size.
  auto buckets = split_buckets(
    consistently_bucketize(roots, bucketSize),
    maxSize,
    addDeps
  );
  return build_hierarchical_work(
    buckets,
    numClasses,
    addDeps,
    isInstan,
    getIdx
  );
}

//////////////////////////////////////////////////////////////////////
// Class flattening:

const StaticString
  s___Sealed("__Sealed"),
  s___EnableMethodTraitDiamond("__EnableMethodTraitDiamond"),
  s___ModuleLevelTrait("__ModuleLevelTrait");

/*
 * Extern-worker job to build ClassInfo2s (which involves flattening
 * data across the hierarchy) and flattening traits.
 */
struct FlattenJob {
  static std::string name() { return "hhbbc-flatten"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
    ClassGraph::init();
  }
  static void fini() { ClassGraph::destroy(); }

  /*
   * Metadata representing results of flattening. This is information
   * that the local coordinator (as opposed to later remote jobs) will
   * need to consume.
   */
  struct OutputMeta {
    // Classes which have been determined to be uninstantiable
    // (therefore have no result output data).
    TSStringSet uninstantiable;
    // New closures produced from trait flattening. Such new closures
    // will require "fixups" in the php::Program data.
    struct NewClosure {
      SString unit;
      SString name;
      SString context;
      template <typename SerDe> void serde(SerDe& sd) {
        sd(unit)(name)(context);
      }
    };
    std::vector<NewClosure> newClosures;
    // Report parents of each class. A class is a parent of another if
    // it would appear on a subclass list. The parents of a closure
    // are not reported because that's implicit.
    struct Parents {
      std::vector<SString> names;
      template <typename SerDe> void serde(SerDe& sd) { sd(names); }
    };
    std::vector<Parents> parents;
    // Classes which are interfaces.
    TSStringSet interfaces;
    // Classes which have 86init functions. A class can gain a 86init
    // from flattening even if it didn't have it before.
    TSStringSet with86init;
    // The types used by the type-constraints of input classes and
    // functions.
    std::vector<TSStringSet> classTypeUses;
    std::vector<TSStringSet> funcTypeUses;
    std::vector<InterfaceConflicts> interfaceConflicts;
    std::vector<MethRefSet> extraMethods;
    std::vector<TSStringSet> flattenedInto;

    // Flattening can cause a class or func to inherit more pre-deps.
    struct NewPredeps {
      SStringSet predeps;
      SStringSet cinitPredeps;
      template <typename SerDe> void serde(SerDe& sd) {
        sd(predeps, string_data_lt{})
          (cinitPredeps, string_data_lt{})
          ;
      }
    };
    std::vector<NewPredeps> newClassPredeps;
    std::vector<NewPredeps> newFuncPredeps;

    // If an unit contains a function or method with an originalUnit
    // field different than unit, it is recorded here. This is used
    // for the final pass, where we have to ensure that both a
    // function's unit and it's original unit is present.
    SStringToOneT<SStringSet> originalUnits;

    template <typename SerDe> void serde(SerDe& sd) {
      ScopedStringDataIndexer _;
      sd(uninstantiable, string_data_lt_type{})
        (newClosures)
        (parents)
        (interfaces, string_data_lt_type{})
        (with86init, string_data_lt_type{})
        (classTypeUses, string_data_lt_type{})
        (funcTypeUses, string_data_lt_type{})
        (interfaceConflicts)
        (extraMethods, std::less<>{})
        (flattenedInto, string_data_lt_type{})
        (newClassPredeps)
        (newFuncPredeps)
        (originalUnits, string_data_lt{}, string_data_lt{})
        ;
    }
  };

  /*
   * Job returns a list of (potentially modified) php::Class, a list
   * of new ClassInfo2, a list of (potentially modified) php::Func,
   * and metadata for the entire job. The order of the lists reflects
   * the order of the input classes and functions (skipping over
   * classes marked as uninstantiable in the metadata).
   */
  using Output = Multi<
    Variadic<std::unique_ptr<php::Class>>,
    Variadic<std::unique_ptr<php::ClassBytecode>>,
    Variadic<std::unique_ptr<ClassInfo2>>,
    Variadic<std::unique_ptr<php::Func>>,
    Variadic<std::unique_ptr<FuncInfo2>>,
    Variadic<std::unique_ptr<MethodsWithoutCInfo>>,
    OutputMeta
  >;

  /*
   * Job takes a list of classes which are to be flattened. In
   * addition to this, it also takes a list of classes which are
   * dependencies of the classes to be flattened. (A class might be
   * one of the inputs *and* a dependency, in which case it should
   * just be on the input list). It is expected that *all*
   * dependencies are provided. All instantiable classes will have
   * their type-constraints resolved to their ultimate type, or left
   * as unresolved if it refers to a missing/invalid type. The
   * provided functions only have their type-constraints updated. The
   * provided type-mappings and list of missing types is used for
   * type-constraint resolution (if a type isn't in a type mapping and
   * isn't a missing type, it is assumed to be a object type).
   *
   * Bytecode needs to be provided for every provided class. The
   * bytecode must be provided in the same order as the bytecode's
   * associated class.
   */
  static Output run(Variadic<std::unique_ptr<php::Class>> classes,
                    Variadic<std::unique_ptr<php::Class>> deps,
                    Variadic<std::unique_ptr<php::ClassBytecode>> classBytecode,
                    Variadic<std::unique_ptr<php::Func>> funcs,
                    Variadic<std::unique_ptr<php::Class>> uninstantiable,
                    std::vector<TypeMapping> typeMappings,
                    std::vector<SString> missingTypes) {
    LocalIndex index;

    for (auto& tc : typeMappings) {
      auto const name = tc.name;
      always_assert(index.m_typeMappings.emplace(name, std::move(tc)).second);
    }
    for (auto const m : missingTypes) {
      always_assert(index.m_missingTypes.emplace(m).second);
    }
    typeMappings.clear();
    missingTypes.clear();

    // Bytecode should have been provided for every class provided.
    always_assert(
      classBytecode.vals.size() == (classes.vals.size() + deps.vals.size())
    );
    // Store the provided bytecode in the matching php::Class.
    for (size_t i = 0,
         size = classBytecode.vals.size(),
         classesSize = classes.vals.size();
         i < size; ++i) {
      auto& cls = i < classesSize
        ? classes.vals[i]
        : deps.vals[i - classesSize];
      auto& bytecode = classBytecode.vals[i];

      // We shouldn't have closures here. They're children of the
      // declaring class.
      assertx(!cls->closureContextCls);
      auto const numMethods = cls->methods.size();
      auto const numClosures = cls->closures.size();
      always_assert(bytecode->methodBCs.size() == numMethods + numClosures);

      for (size_t j = 0, bcSize = bytecode->methodBCs.size(); j < bcSize; ++j) {
        if (j < numMethods) {
          cls->methods[j]->rawBlocks = std::move(bytecode->methodBCs[j].bc);
        } else {
          assertx(cls->closures[j-numMethods]->methods.size() == 1);
          cls->closures[j-numMethods]->methods[0]->rawBlocks =
            std::move(bytecode->methodBCs[j].bc);
        }
      }
    }
    classBytecode.vals.clear();

    // Some classes might be dependencies of another. Moreover, some
    // classes might share dependencies. Topologically sort all of the
    // classes and process them in that order. Information will flow
    // from parent classes to their children.
    auto const worklist = prepare(
      index,
      [&] {
        TSStringToOneT<php::Class*> out;
        out.reserve(classes.vals.size() + deps.vals.size());
        for (auto const& c : classes.vals) {
          always_assert(out.emplace(c->name, c.get()).second);
          for (auto const& clo : c->closures) {
            always_assert(out.emplace(clo->name, clo.get()).second);
          }
        }
        for (auto const& c : deps.vals) {
          always_assert(out.emplace(c->name, c.get()).second);
          for (auto const& clo : c->closures) {
            always_assert(out.emplace(clo->name, clo.get()).second);
          }
        }
        return out;
      }()
    );

    for (auto const& cls : uninstantiable.vals) {
      always_assert(index.m_uninstantiable.emplace(cls->name).second);
      for (auto const& clo : cls->closures) {
        always_assert(index.m_uninstantiable.emplace(clo->name).second);
      }
    }

    std::vector<const php::Class*> newClosures;

    for (auto const cls : worklist) {
      auto const UNUSED bump = trace_bump(*cls, Trace::hhbbc_index);

      ITRACE(2, "flatten class: {}\n", cls->name);
      Trace::Indent indent;

      index.m_ctx = cls;
      SCOPE_EXIT { index.m_ctx = nullptr; };

      auto state = std::make_unique<State>();
      // Attempt to make the ClassInfo2 for this class. If we can't,
      // it means the class is not instantiable.
      auto newInfo = make_info(index, *cls, *state);
      if (!newInfo) {
        ITRACE(4, "{} is not instantiable\n", cls->name);
        always_assert(index.m_uninstantiable.emplace(cls->name).second);
        continue;
      }
      auto const cinfo = newInfo.get();

      ITRACE(5, "adding state for class '{}' to local index\n", cls->name);
      assertx(cinfo->name->tsame(cls->name));

      // We might look up this class when flattening itself, so add it
      // to the local index before we start.
      always_assert(index.m_classes.emplace(cls->name, cls).second);
      always_assert(
        index.m_classInfos.emplace(cls->name, std::move(newInfo)).second
      );

      auto const [stateIt, stateSuccess] =
        index.m_states.emplace(cls->name, std::move(state));
      always_assert(stateSuccess);

      auto closureIdx = cls->closures.size();
      auto closures = flatten_traits(index, *cls, *cinfo, *stateIt->second);

      // Trait flattening may produce new closures, so those need to
      // be added to the local index as well.
      for (auto& i : closures) {
        assertx(closureIdx < cls->closures.size());
        auto& c = cls->closures[closureIdx++];
        ITRACE(5, "adding state for closure '{}' to local index\n", c->name);
        assertx(!is_closure(*cls));
        assertx(c->name->tsame(i->name));
        assertx(is_closure(*c));
        assertx(c->closureContextCls);
        assertx(c->closures.empty());
        assertx(i->closures.empty());
        always_assert(index.m_classes.emplace(c->name, c.get()).second);
        always_assert(index.m_classInfos.emplace(c->name, std::move(i)).second);
        newClosures.emplace_back(c.get());
      }

      std::sort(
        begin(cls->closures), end(cls->closures),
        [] (const std::unique_ptr<php::Class>& c1,
            const std::unique_ptr<php::Class>& c2) {
          return string_data_lt_type{}(c1->name, c2->name);
        }
      );

      // We're done with this class. All of it's parents are now
      // added.
      cinfo->classGraph.finalizeParents();
    }

    // Format the output data and put it in a deterministic order.
    Variadic<std::unique_ptr<php::Class>> outClasses;
    Variadic<std::unique_ptr<ClassInfo2>> outInfos;
    Variadic<std::unique_ptr<MethodsWithoutCInfo>> outMethods;
    OutputMeta outMeta;
    TSStringSet outNames;

    outClasses.vals.reserve(classes.vals.size());
    outInfos.vals.reserve(classes.vals.size());
    outNames.reserve(classes.vals.size());
    outMeta.parents.reserve(classes.vals.size());
    outMeta.newClosures.reserve(newClosures.size());
    outMeta.classTypeUses.reserve(classes.vals.size());
    outMeta.extraMethods.reserve(classes.vals.size());
    outMeta.flattenedInto.reserve(classes.vals.size());

    auto const makeMethodsWithoutCInfo = [&] (const php::Class& cls) {
      always_assert(outMeta.uninstantiable.emplace(cls.name).second);
      // Even though the class is uninstantiable, we still need to
      // create FuncInfos for it's methods. These are stored
      // separately (there's no ClassInfo to store it in!)
      auto methods = std::make_unique<MethodsWithoutCInfo>();
      methods->cls = cls.name;
      for (auto const& func : cls.methods) {
        methods->finfos.emplace_back(make_func_info(index, *func));
      }
      for (auto const& clo : cls.closures) {
        assertx(clo->methods.size() == 1);
        methods->closureInvokes.emplace_back(
          make_func_info(index, *clo->methods[0])
        );
      }
      outMethods.vals.emplace_back(std::move(methods));
    };

    // Do the processing which relies on a fully accessible
    // LocalIndex

    TSStringToOneT<InterfaceConflicts> ifaceConflicts;
    for (auto& cls : classes.vals) {
      assertx(!cls->closureContextCls);
      auto const cinfoIt = index.m_classInfos.find(cls->name);
      if (cinfoIt == end(index.m_classInfos)) {
        ITRACE(
          4, "{} discovered to be not instantiable, instead "
          "creating MethodsWithoutCInfo for it\n",
          cls->name
        );
        always_assert(index.uninstantiable(cls->name));
        makeMethodsWithoutCInfo(*cls);
        continue;
      }
      auto& cinfo = cinfoIt->second;

      index.m_ctx = cls.get();
      SCOPE_EXIT { index.m_ctx = nullptr; };

      outMeta.classTypeUses.emplace_back();
      outMeta.newClassPredeps.emplace_back();
      update_type_constraints(
        index,
        *cls,
        outMeta.classTypeUses.back(),
        outMeta.newClassPredeps.back()
      );
      optimize_properties(index, *cls, *cinfo);
      for (auto const& func : cls->methods) {
        cinfo->funcInfos.emplace_back(make_func_info(index, *func));
      }

      assertx(cinfo->closures.empty());
      for (auto& clo : cls->closures) {
        auto const it = index.m_classInfos.find(clo->name);
        always_assert(it != end(index.m_classInfos));
        auto& cloinfo = it->second;
        update_type_constraints(
          index,
          *clo,
          outMeta.classTypeUses.back(),
          outMeta.newClassPredeps.back()
        );
        optimize_properties(index, *clo, *cloinfo);
        assertx(clo->methods.size() == 1);
        cloinfo->funcInfos.emplace_back(
          make_func_info(index, *clo->methods[0])
        );
      }

      outMeta.extraMethods.emplace_back(cinfo->extraMethods);
      outNames.emplace(cls->name);

      // Record interface conflicts

      // Only consider normal or abstract classes
      if (cls->attrs &
          (AttrInterface | AttrTrait | AttrEnum | AttrEnumClass)) {
        continue;
      }

      auto const interfaces = cinfo->classGraph.interfaces();

      if constexpr (debug) {
        always_assert(IMPLIES(is_closure(*cls), interfaces.empty()));
        for (auto const& cloinfo : cinfo->closures) {
          always_assert(cloinfo->classGraph.interfaces().empty());
        }
      }

      for (auto const i1 : interfaces) {
        auto& conflicts = ifaceConflicts[i1.name()];
        conflicts.name = i1.name();
        ++conflicts.usage;
        for (auto const i2 : interfaces) {
          if (i1 == i2) continue;
          conflicts.conflicts.emplace(i2.name());
        }
      }
    }

    outMeta.interfaceConflicts.reserve(ifaceConflicts.size());
    for (auto& [_, c] : ifaceConflicts) {
      outMeta.interfaceConflicts.emplace_back(std::move(c));
    }
    std::sort(
      begin(outMeta.interfaceConflicts),
      end(outMeta.interfaceConflicts),
      [] (auto const& c1, auto const& c2) {
        return string_data_lt_type{}(c1.name, c2.name);
      }
    );

    // We don't process classes marked as uninstantiable beforehand,
    // except for creating method FuncInfos for them.
    for (auto const& cls : uninstantiable.vals) {
      ITRACE(
        4, "{} already known to be not instantiable, creating "
        "MethodsWithoutCInfo for it\n",
        cls->name
      );
      makeMethodsWithoutCInfo(*cls);
    }

    // Now move the classes out of LocalIndex and into the output. At
    // this point, it's not safe to access the LocalIndex unless
    // you're sure something hasn't been moved yet.
    for (auto& cls : classes.vals) {
      auto const name = cls->name;

      auto const cinfoIt = index.m_classInfos.find(name);
      if (cinfoIt == end(index.m_classInfos)) {
        assertx(outMeta.uninstantiable.contains(name));
        continue;
      }
      auto& cinfo = cinfoIt->second;

      // Check if this class has a 86*init function (it might have
      // already or might have gained one from trait flattening).
      auto const has86init =
        std::any_of(
          begin(cls->methods), end(cls->methods),
          [] (auto const& m) { return is_86init_func(*m); }
        ) ||
        std::any_of(
          begin(cinfo->clsConstants), end(cinfo->clsConstants),
          [] (auto const& cns) {
            return cns.second.kind == ConstModifierFlags::Kind::Type;
          }
        );
      if (has86init) {
        assertx(!is_closure(*cls));
        outMeta.with86init.emplace(name);
      }

      index.m_ctx = cls.get();
      SCOPE_EXIT { index.m_ctx = nullptr; };

      // For building FuncFamily::StaticInfo, we need to ensure that
      // every method has an entry in methodFamilies. Make all of the
      // initial entries here (they'll be created assuming this method
      // is AttrNoOverride).
      for (auto const& [methname, mte] : cinfo->methods) {
        if (is_special_method_name(methname)) continue;
        auto entry = make_initial_func_family_entry(*cls, index.meth(mte), mte);
        always_assert(
          cinfo->methodFamilies.emplace(methname, std::move(entry)).second
        );
      }

      auto const& state = index.m_states.at(name);
      outMeta.flattenedInto.emplace_back();
      outMeta.flattenedInto.back() = std::move(state->m_flattenedInto);

      if (!is_closure(*cls)) {
        outMeta.parents.emplace_back();
        auto& parents = outMeta.parents.back().names;
        parents.reserve(state->m_parents.size());
        for (auto const p : state->m_parents) {
          parents.emplace_back(p->name);
        }
      }

      if (cls->attrs & AttrInterface) {
        outMeta.interfaces.emplace(name);
      }

      // We always know the subclass status of closures and the
      // closure base class.
      if (is_closure_base(*cls)) {
        cinfo->classGraph.setClosureBase();
      } else if (is_closure(*cls)) {
        cinfo->classGraph.setComplete();
      }

      assertx(cinfo->closures.empty());
      for (auto& clo : cls->closures) {
        auto const it = index.m_classInfos.find(clo->name);
        always_assert(it != end(index.m_classInfos));
        auto& cloinfo = it->second;
        assertx(cloinfo->extraMethods.empty());

        // Closures are always leafs.
        cloinfo->classGraph.setComplete();
        assertx(!cloinfo->classGraph.mightHaveRegularSubclass());
        assertx(!cloinfo->classGraph.mightHaveNonRegularSubclass());

        for (auto const& [methname, mte] : cloinfo->methods) {
          if (is_special_method_name(methname)) continue;
          auto entry =
            make_initial_func_family_entry(*clo, index.meth(mte), mte);
          always_assert(
            cloinfo->methodFamilies.emplace(methname, std::move(entry)).second
          );
        }

        for (auto const& m : clo->methods) {
          if (!m->originalUnit || m->originalUnit == m->unit) continue;
          outMeta.originalUnits[m->unit].emplace(m->originalUnit);
        }

        cinfo->closures.emplace_back(std::move(cloinfo));
      }

      for (auto const& m : cls->methods) {
        if (!m->originalUnit || m->originalUnit == m->unit) continue;
        outMeta.originalUnits[m->unit].emplace(m->originalUnit);
      }

      outClasses.vals.emplace_back(std::move(cls));
      outInfos.vals.emplace_back(std::move(cinfo));
    }

    std::sort(
      begin(newClosures), end(newClosures),
      [] (const php::Class* c1, const php::Class* c2) {
        return string_data_lt_type{}(c1->name, c2->name);
      }
    );
    for (auto clo : newClosures) {
      assertx(clo->closureContextCls);
      if (!outNames.contains(clo->closureContextCls)) continue;
      outMeta.newClosures.emplace_back(
        OutputMeta::NewClosure{clo->unit, clo->name, clo->closureContextCls}
      );
      for (auto const& m : clo->methods) {
        if (!m->originalUnit || m->originalUnit == m->unit) continue;
        outMeta.originalUnits[m->unit].emplace(m->originalUnit);
      }
    }

    Variadic<std::unique_ptr<FuncInfo2>> funcInfos;
    funcInfos.vals.reserve(funcs.vals.size());
    for (auto& func : funcs.vals) {
      outMeta.funcTypeUses.emplace_back();
      outMeta.newFuncPredeps.emplace_back();
      update_type_constraints(
        index,
        *func,
        outMeta.funcTypeUses.back(),
        outMeta.newFuncPredeps.back()
      );
      funcInfos.vals.emplace_back(make_func_info(index, *func));
    }

    // Provide any updated bytecode back to the caller.
    Variadic<std::unique_ptr<php::ClassBytecode>> outBytecode;
    outBytecode.vals.reserve(outClasses.vals.size());
    for (auto& cls : outClasses.vals) {
      auto bytecode = std::make_unique<php::ClassBytecode>();
      bytecode->cls = cls->name;
      bytecode->methodBCs.reserve(cls->methods.size());
      for (auto& method : cls->methods) {
        bytecode->methodBCs.emplace_back(
          method->name,
          std::move(method->rawBlocks)
        );
      }
      for (auto& clo : cls->closures) {
        assertx(clo->methods.size() == 1);
        auto& method = clo->methods[0];
        bytecode->methodBCs.emplace_back(
          method->name,
          std::move(method->rawBlocks)
        );
      }
      outBytecode.vals.emplace_back(std::move(bytecode));
    }

    return std::make_tuple(
      std::move(outClasses),
      std::move(outBytecode),
      std::move(outInfos),
      std::move(funcs),
      std::move(funcInfos),
      std::move(outMethods),
      std::move(outMeta)
    );
  }

private:
  /*
   * State which needs to be propagated from a dependency to a child
   * class during flattening, but not required after flattening (so
   * doesn't belong in ClassInfo2).
   */
  struct State {
    struct PropTuple {
      SString name;
      SString src;
      php::Prop prop;
    };
    // Maintain order of properties as we inherit them.
    CompactVector<PropTuple> m_props;
    SStringToOneT<size_t> m_propIndices;
    CompactVector<php::Const> m_traitCns;
    SStringSet m_cnsFromTrait;
    SStringToOneT<size_t> m_methodIndices;
    CompactVector<const php::Class*> m_parents;
    TSStringSet m_flattenedInto;

    size_t& methodIdx(SString context, SString cls, SString name) {
      auto const it = m_methodIndices.find(name);
      always_assert_flog(
        it != m_methodIndices.end(),
        "While processing '{}', "
        "tried to access missing method index for '{}::{}'",
        context, cls, name
      );
      return it->second;
    }

    size_t methodIdx(SString context, SString cls, SString name) const {
      return const_cast<State*>(this)->methodIdx(context, cls, name);
    }
  };

  /*
   * LocalIndex is similar to Index, but for this job. It maps names
   * to class information needed during flattening. It also verifies
   * we don't try to access information about a class until it's
   * actually available (which shouldn't happen if our dataflow is
   * correct).
   */
  struct LocalIndex {
    const php::Class* m_ctx{nullptr};

    TSStringToOneT<const php::Class*> m_classes;
    TSStringToOneT<std::unique_ptr<ClassInfo2>> m_classInfos;
    TSStringToOneT<std::unique_ptr<State>> m_states;

    TSStringSet m_uninstantiable;

    TSStringToOneT<TypeMapping> m_typeMappings;
    TSStringSet m_missingTypes;

    const php::Class& cls(SString name) const {
      if (m_ctx->name->tsame(name)) return *m_ctx;
      auto const it = m_classes.find(name);
      always_assert_flog(
        it != m_classes.end(),
        "While processing '{}', tried to access missing class '{}' from index",
        m_ctx->name,
        name
      );
      assertx(it->second);
      return *it->second;
    }

    const ClassInfo2& classInfo(SString name) const {
      auto const it = m_classInfos.find(name);
      always_assert_flog(
        it != m_classInfos.end(),
        "While processing '{}', tried to access missing class-info for '{}' "
        "from index",
        m_ctx->name,
        name
      );
      assertx(it->second.get());
      return *it->second;
    }

    const State& state(SString name) const {
      auto const it = m_states.find(name);
      always_assert_flog(
        it != m_states.end(),
        "While processing '{}', tried to access missing flatten state for '{}' "
        "from index",
        m_ctx->name,
        name
      );
      assertx(it->second.get());
      return *it->second;
    }

    bool uninstantiable(SString name) const {
      return m_uninstantiable.contains(name);
    }

    const TypeMapping* typeMapping(SString name) const {
      return folly::get_ptr(m_typeMappings, name);
    }

    bool missingType(SString name) const {
      return m_missingTypes.contains(name);
    }

    const php::Func& meth(const MethRef& r) const {
      auto const& mcls = cls(r.cls);
      assertx(r.idx < mcls.methods.size());
      return *mcls.methods[r.idx];
    }
    const php::Func& meth(const MethTabEntry& mte) const {
      return meth(mte.meth());
    }

    const php::Const& cns(const ConstIndex& idx) const {
      auto const& c = cls(idx.cls);
      assertx(idx.idx < c.constants.size());
      return c.constants[idx.idx];
    }

    size_t methodIdx(SString cls, SString name) const {
      return state(cls).methodIdx(m_ctx->name, cls, name);
    }
  };

  /*
   * Calculate the order in which the classes should be flattened,
   * taking into account dependencies.
   */
  static std::vector<php::Class*> prepare(
    LocalIndex& index,
    const TSStringToOneT<php::Class*>& classes
  ) {
    // We might not have any classes if we're just processing funcs.
    if (classes.empty()) return {};

    auto const get = [&] (SString name) -> php::Class& {
      auto const it = classes.find(name);
      always_assert_flog(
        it != classes.end(),
        "Tried to access missing class '{}' while calculating flattening order",
        name
      );
      return *it->second;
    };

    auto const forEachDep = [&] (php::Class& c, auto const& f) {
      if (c.parentName) f(get(c.parentName));
      for (auto const i : c.interfaceNames)    f(get(i));
      for (auto const e : c.includedEnumNames) f(get(e));
      for (auto const t : c.usedTraitNames)    f(get(t));
      for (auto const& clo : c.closures) {
        f(const_cast<php::Class&>(*clo));
      }
    };

    /*
     * Perform a standard topological sort:
     *
     * - For each class, calculate the number of classes which depend on it.
     *
     * - Any class which has a use count of zero is not depended on by
     *   anyone and goes onto the intitial worklist.
     *
     * - For every class on the worklist, push it onto the output
     *   list, and decrement the use count of all of it's
     *   dependencies.
     *
     * - For any class which now has a use count of zero, push it onto
     *   the worklist and repeat above step until all classes are
     *   pushed onto the output list.
     *
     * - Reverse the list.
     *
     * - This does not handle cycles, but we should not encounter any
     *   here, as such cycles should be detected earlier and not be
     *   scheduled in a job.
     */
    hphp_fast_map<const php::Class*, size_t> uses;
    uses.reserve(classes.size());
    for (auto const& [_, cls] : classes) {
      forEachDep(*cls, [&] (const php::Class& d) { ++uses[&d]; });
    }

    std::vector<php::Class*> worklist;
    for (auto const [_, cls] : classes) {
      if (!uses[cls]) worklist.emplace_back(cls);
    }
    always_assert(!worklist.empty());
    std::sort(
      worklist.begin(),
      worklist.end(),
      [] (const php::Class* c1, const php::Class* c2) {
        return string_data_lt_type{}(c1->name, c2->name);
      }
    );

    std::vector<php::Class*> ordered;
    ordered.reserve(classes.size());
    do {
      auto const cls = worklist.back();
      assertx(!uses[cls]);
      worklist.pop_back();
      forEachDep(
        *cls,
        [&] (php::Class& d) {
          if (!--uses.at(&d)) worklist.emplace_back(&d);
        }
      );
      ordered.emplace_back(cls);
    } while (!worklist.empty());

    for (auto const& [_, cls] : classes) always_assert(!uses.at(cls));
    std::reverse(ordered.begin(), ordered.end());
    return ordered;
  }

  /*
   * Create a FuncFamilyEntry for the give method. This
   * FuncFamilyEntry assumes that the method is AttrNoOverride, and
   * hence reflects just this method. The method isn't necessarily
   * actually AttrNoOverride, but if not, it will be updated in
   * BuildSubclassListJob (that job needs the initial entries).
   */
  static FuncFamilyEntry make_initial_func_family_entry(
    const php::Class& cls,
    const php::Func& meth,
    const MethTabEntry& mte
  ) {
    FuncFamilyEntry entry;
    entry.m_allIncomplete = false;
    entry.m_regularIncomplete = false;
    entry.m_privateAncestor = is_regular_class(cls) && mte.hasPrivateAncestor();

    FuncFamilyEntry::MethMetadata meta;

    for (size_t i = 0; i < meth.params.size(); ++i) {
      meta.m_prepKinds.emplace_back(func_param_prep(&meth, i));
    }
    // Any param beyond the size of m_paramPreps is implicitly
    // TriBool::No, so we can drop trailing entries which are
    // TriBool::No.
    while (!meta.m_prepKinds.empty()) {
      auto& back = meta.m_prepKinds.back();
      if (back.inOut != TriBool::No || back.readonly != TriBool::No) break;
      meta.m_prepKinds.pop_back();
    }
    meta.m_numInOut = func_num_inout(&meth);
    meta.m_nonVariadicParams = numNVArgs(meth);
    meta.m_coeffectRules = meth.coeffectRules;
    meta.m_requiredCoeffects = meth.requiredCoeffects;
    meta.m_isReadonlyReturn = meth.isReadonlyReturn;
    meta.m_isReadonlyThis = meth.isReadonlyThis;
    meta.m_supportsAER = func_supports_AER(&meth);
    meta.m_isReified = meth.isReified;
    meta.m_caresAboutDyncalls = (dyn_call_error_level(&meth) > 0);
    meta.m_builtin = meth.attrs & AttrBuiltin;

    if (is_regular_class(cls)) {
      entry.m_meths =
        FuncFamilyEntry::BothSingle{mte.meth(), std::move(meta), false};
    } else if (bool(mte.attrs & AttrPrivate) && mte.topLevel()) {
      entry.m_meths =
        FuncFamilyEntry::BothSingle{mte.meth(), std::move(meta), true};
    } else {
      entry.m_meths =
        FuncFamilyEntry::SingleAndNone{mte.meth(), std::move(meta)};
    }

    return entry;
  }

  static std::unique_ptr<ClassInfo2> make_info(const LocalIndex& index,
                                               php::Class& cls,
                                               State& state) {
    if (debug && (is_closure(cls) || is_closure_base(cls))) {
      if (is_closure(cls)) {
        always_assert(cls.parentName->tsame(s_Closure.get()));
      } else {
        always_assert(!cls.parentName);
      }
      always_assert(cls.interfaceNames.empty());
      always_assert(cls.includedEnumNames.empty());
      always_assert(cls.usedTraitNames.empty());
      always_assert(cls.requirements.empty());
      always_assert(cls.constants.empty());
      always_assert(cls.userAttributes.empty());
      always_assert(!(cls.attrs & (AttrTrait | AttrInterface | AttrAbstract)));
    }

    // Set up some initial values for ClassInfo properties. If this
    // class is a leaf (we can't actually determine that yet), these
    // will be valid and remain as-is. If not, they'll be updated
    // properly when calculating subclass information in another pass.
    auto cinfo = std::make_unique<ClassInfo2>();
    cinfo->name = cls.name;
    cinfo->hasConstProp = cls.hasConstProp;
    cinfo->hasReifiedParent = cls.hasReifiedGenerics;
    cinfo->hasReifiedGeneric = cls.userAttributes.contains(s___Reified.get());
    cinfo->subHasReifiedGeneric = cinfo->hasReifiedGeneric;
    cinfo->initialNoReifiedInit = cls.attrs & AttrNoReifiedInit;
    cinfo->isMockClass = is_mock_class(&cls);
    cinfo->isRegularClass = is_regular_class(cls);

    // Create a ClassGraph for this class. If we decide to not keep
    // the ClassInfo, reset the ClassGraph to keep it from ending up
    // in the graph.
    cinfo->classGraph = ClassGraph::create(cls);
    auto success = false;
    SCOPE_EXIT { if (!success) cinfo->classGraph.reset(); };

    // Assume the class isn't overridden. This is true for leafs and
    // non-leafs will get updated when we build subclass information.
    if (!is_closure_base(cls)) {
      attribute_setter(cls.attrs, true, AttrNoOverride);
      attribute_setter(cls.attrs, true, AttrNoOverrideRegular);
    } else {
      attribute_setter(cls.attrs, false, AttrNoOverride);
      attribute_setter(cls.attrs, false, AttrNoOverrideRegular);
    }

    // Assume this. If not a leaf, will be updated in
    // BuildSubclassList job.
    attribute_setter(cls.attrs, true, AttrNoMock);

    for (auto const& clo : cls.closures) {
      if (index.uninstantiable(clo->name)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "its closure `{}' is uninstantiable\n",
               cls.name, clo->name);
        return nullptr;
      }
    }

    if (cls.parentName) {
      assertx(!is_closure_base(cls));
      assertx(is_closure(cls) == cls.parentName->tsame(s_Closure.get()));

      if (index.uninstantiable(cls.parentName)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "its parent `{}' is uninstantiable\n",
               cls.name, cls.parentName);
        return nullptr;
      }
      auto const& parent = index.cls(cls.parentName);
      auto const& parentInfo = index.classInfo(cls.parentName);

      assertx(!is_closure(parent));
      if (parent.attrs & (AttrInterface | AttrTrait)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "its parent `{}' is not a class\n",
               cls.name, cls.parentName);
        return nullptr;
      }
      if (!enforce_sealing(*cinfo, cls, parent)) return nullptr;

      cinfo->parent = cls.parentName;
      cinfo->hasConstProp |= parentInfo.hasConstProp;
      cinfo->hasReifiedParent |= parentInfo.hasReifiedParent;

      state.m_parents.emplace_back(&parent);
      cinfo->classGraph.setBase(parentInfo.classGraph);
    } else if (!cinfo->hasReifiedGeneric) {
      attribute_setter(cls.attrs, true, AttrNoReifiedInit);
    }

    for (auto const iname : cls.interfaceNames) {
      assertx(!is_closure(cls));
      assertx(!is_closure_base(cls));
      if (index.uninstantiable(iname)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "{} is uninstantiable\n",
               cls.name, iname);
        return nullptr;
      }
      auto const& iface = index.cls(iname);
      auto const& ifaceInfo = index.classInfo(iname);

      assertx(!is_closure(iface));
      if (!(iface.attrs & AttrInterface)) {
        ITRACE(2,
               "Making class-info failed for `{}' because `{}' "
               "is not an interface\n",
               cls.name, iname);
        return nullptr;
      }
      if (!enforce_sealing(*cinfo, cls, iface)) return nullptr;

      cinfo->hasReifiedParent |= ifaceInfo.hasReifiedParent;

      state.m_parents.emplace_back(&iface);
      cinfo->classGraph.addParent(ifaceInfo.classGraph);
    }

    for (auto const ename : cls.includedEnumNames) {
      assertx(!is_closure(cls));
      assertx(!is_closure_base(cls));
      if (index.uninstantiable(ename)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "{} is uninstantiable\n",
               cls.name, ename);
        return nullptr;
      }
      auto const& e = index.cls(ename);
      auto const& einfo = index.classInfo(ename);

      assertx(!is_closure(e));
      auto const wantAttr = cls.attrs & (AttrEnum | AttrEnumClass);
      if (!(e.attrs & wantAttr)) {
        ITRACE(2,
               "Making class-info failed for `{}' because `{}' "
               "is not an enum{}\n",
               cls.name, ename,
               wantAttr & AttrEnumClass ? " class" : "");
        return nullptr;
      }
      if (!enforce_sealing(*cinfo, cls, e)) return nullptr;

      for (auto const iface : einfo.classGraph.declInterfaces()) {
        cinfo->classGraph.addParent(iface);
      }
    }

    auto const clsHasModuleLevelTrait =
      cls.userAttributes.contains(s___ModuleLevelTrait.get());
    if (clsHasModuleLevelTrait &&
        (!(cls.attrs & AttrTrait) || (cls.attrs & AttrInternal))) {
      ITRACE(2,
             "Making class-info failed for `{}' because "
             "attribute <<__ModuleLevelTrait>> can only be "
             "specified on public traits\n",
             cls.name);
      return nullptr;
    }

    for (auto const tname : cls.usedTraitNames) {
      assertx(!is_closure(cls));
      assertx(!is_closure_base(cls));
      if (index.uninstantiable(tname)) {
        ITRACE(2,
               "Making class-info failed for `{}' because "
               "{} is uninstantiable\n",
               cls.name, tname);
        return nullptr;
      }
      auto const& trait = index.cls(tname);
      auto const& traitInfo = index.classInfo(tname);

      assertx(!is_closure(trait));
      if (!(trait.attrs & AttrTrait)) {
        ITRACE(2,
               "Making class-info failed for `{}' because `{}' "
               "is not a trait\n",
               cls.name, tname);
        return nullptr;
      }
      if (!enforce_sealing(*cinfo, cls, trait)) return nullptr;

      cinfo->hasConstProp |= traitInfo.hasConstProp;
      cinfo->hasReifiedParent |= traitInfo.hasReifiedParent;

      state.m_parents.emplace_back(&trait);
      cinfo->classGraph.addParent(traitInfo.classGraph);
    }

    if (cls.attrs & AttrEnum) {
      auto const baseType = [&] {
        auto const& base = cls.enumBaseTy;
        if (!base.isUnresolved()) return base.type();
        auto const tm = index.typeMapping(base.typeName());
        if (!tm) return AnnotType::Unresolved;
        // enums cannot use case types
        assertx(!tm->value.isUnion());
        return tm->value.type();
      }();
      if (!enumSupportsAnnot(baseType)) {
        ITRACE(2,
               "Making class-info failed for `{}' because {} "
               "is not a valid enum base type\n",
               cls.name, annotName(baseType));
        return nullptr;
      }
    }

    if (!build_methods(index, cls, *cinfo, state))    return nullptr;
    if (!build_properties(index, cls, *cinfo, state)) return nullptr;
    if (!build_constants(index, cls, *cinfo, state))  return nullptr;

    std::sort(
      begin(state.m_parents),
      end(state.m_parents),
      [] (const php::Class* a, const php::Class* b) {
        return string_data_lt_type{}(a->name, b->name);
      }
    );
    state.m_parents.erase(
      std::unique(begin(state.m_parents), end(state.m_parents)),
      end(state.m_parents)
    );
    assertx(
      std::none_of(
        begin(state.m_parents), end(state.m_parents),
        [] (const php::Class* c) { return is_closure(*c); }
      )
    );

    cinfo->subHasConstProp = cinfo->hasConstProp;

    // All methods are originally not overridden (we'll update this as
    // necessary later), except for special methods, which are always
    // considered to be overridden.
    for (auto& [name, mte] : cinfo->methods) {
      assertx(!cinfo->missingMethods.contains(name));

      auto const noOverride = !is_special_method_name(name);
      attribute_setter(mte.attrs, noOverride, AttrNoOverride);
      if (noOverride) {
        mte.setNoOverrideRegular();
      } else {
        mte.clearNoOverrideRegular();
      }

      // Keep func attrs in sync
      auto const meth = mte.meth();
      if (meth.cls->tsame(cinfo->name)) {
        assertx(meth.idx < cls.methods.size());
        auto& m = cls.methods[meth.idx];
        attribute_setter(m->attrs, noOverride, AttrNoOverride);
      }
    }

    // We don't calculate subclass information for closures, so make
    // sure their initial values are all what they should be.
    if (debug && (is_closure(cls) || is_closure_base(cls))) {
      if (is_closure(cls)) {
        always_assert(is_closure_name(cls.name));
        always_assert(state.m_parents.size() == 1);
        always_assert(state.m_parents[0]->name->tsame(s_Closure.get()));
        always_assert(!(cls.attrs & AttrNoReifiedInit));
      } else {
        always_assert(state.m_parents.empty());
        always_assert(cls.attrs & AttrNoReifiedInit);
      }
      always_assert(cinfo->missingMethods.empty());
      always_assert(!cinfo->hasConstProp);
      always_assert(!cinfo->subHasConstProp);
      always_assert(!cinfo->hasReifiedParent);
      always_assert(!cinfo->hasReifiedGeneric);
      always_assert(!cinfo->subHasReifiedGeneric);
      always_assert(!cinfo->initialNoReifiedInit);
      always_assert(!cinfo->isMockClass);
      always_assert(cinfo->isRegularClass);
      always_assert(!is_mock_class(&cls));
    }

    ITRACE(2, "new class-info: {}\n", cls.name);
    if (Trace::moduleEnabled(Trace::hhbbc_index, 3)) {
      if (cinfo->parent) {
        ITRACE(3, "           parent: {}\n", cinfo->parent);
      }
      auto const cg = cinfo->classGraph;
      for (auto const DEBUG_ONLY base : cg.bases()) {
        ITRACE(3, "             base: {}\n", base.name());
      }
      for (auto const DEBUG_ONLY iface : cls.interfaceNames) {
        ITRACE(3, "  decl implements: {}\n", iface);
      }
      for (auto const DEBUG_ONLY iface : cg.interfaces()) {
        ITRACE(3, "       implements: {}\n", iface.name());
      }
      for (auto const DEBUG_ONLY e : cls.includedEnumNames) {
        ITRACE(3, "             enum: {}\n", e);
      }
      for (auto const DEBUG_ONLY trait : cls.usedTraitNames) {
        ITRACE(3, "             uses: {}\n", trait);
      }
      for (auto const& DEBUG_ONLY closure : cls.closures) {
        ITRACE(3, "          closure: {}\n", closure->name);
      }
    }

    // We're going to use this ClassInfo.
    success = true;
    return cinfo;
  }

  static bool enforce_sealing(const ClassInfo2& cinfo,
                              const php::Class& cls,
                              const php::Class& parent) {
    if (is_mock_class(&cls)) return true;
    if (!(parent.attrs & AttrSealed)) return true;
    auto const it = parent.userAttributes.find(s___Sealed.get());
    assertx(it != parent.userAttributes.end());
    assertx(tvIsArrayLike(it->second));
    auto allowed = false;
    IterateV(
      it->second.m_data.parr,
      [&] (TypedValue v) {
        assertx(tvIsStringLike(v));
        if (tvAssertStringLike(v)->tsame(cinfo.name)) {
          allowed = true;
          return true;
        }
        return false;
      }
    );
    if (!allowed) {
      ITRACE(
        2,
        "Making class-info failed for `{}' because "
        "`{}' is sealed\n",
        cinfo.name, parent.name
      );
    }
    return allowed;
  }

  static bool build_properties(const LocalIndex& index,
                               const php::Class& cls,
                               ClassInfo2& cinfo,
                               State& state) {
    if (cls.parentName) {
      auto const& parentState = index.state(cls.parentName);
      state.m_props = parentState.m_props;
      state.m_propIndices = parentState.m_propIndices;

      assertx(cinfo.propDeclInfo.empty());
      cinfo.propDeclInfo = index.classInfo(cls.parentName).propDeclInfo;
    }

    for (auto const iface : cls.interfaceNames) {
      if (!merge_properties(cinfo, state, index.state(iface))) {
        return false;
      }
    }
    for (auto const trait : cls.usedTraitNames) {
      if (!merge_properties(cinfo, state, index.state(trait))) {
        return false;
      }
    }
    for (auto const e : cls.includedEnumNames) {
      if (!merge_properties(cinfo, state, index.state(e))) {
        return false;
      }
    }

    if (cls.attrs & AttrInterface) return true;

    auto const cannotDefineInternalProperties =
      // public traits cannot define internal properties unless they
      // have the __ModuleLevelTrait attribute
      ((cls.attrs & AttrTrait) && (cls.attrs & AttrPublic)) &&
        !(cls.userAttributes.contains(s___ModuleLevelTrait.get()));

    for (auto const& p : cls.properties) {
      if (cannotDefineInternalProperties && (p.attrs & AttrInternal)) {
        ITRACE(2,
               "Adding property failed for `{}' because property `{}' "
               "is internal and public traits cannot define internal properties\n",
               cinfo.name, p.name);
        return false;
      }
      if (!add_property(cinfo, state, p.name, p, cinfo.name, false)) {
        return false;
      }
    }

    // There's no need to do this work if traits have been flattened
    // already, or if the top level class has no traits.  In those
    // cases, we might be able to rule out some instantiations, but it
    // doesn't seem worth it.
    if (cls.attrs & AttrNoExpandTrait) return true;

    for (auto const traitName : cls.usedTraitNames) {
      auto const& trait = index.cls(traitName);
      auto const& traitInfo = index.classInfo(traitName);
      for (auto const& p : trait.properties) {
        if (!add_property(cinfo, state, p.name, p, cinfo.name, true)) {
          return false;
        }
      }
      for (auto const& p : traitInfo.traitProps) {
        if (!add_property(cinfo, state, p.name, p, cinfo.name, true)) {
          return false;
        }
      }
    }

    return true;
  }

  static bool add_property(ClassInfo2& cinfo,
                           State& state,
                           SString name,
                           const php::Prop& prop,
                           SString src,
                           bool trait) {
    auto& dinfo = cinfo.propDeclInfo[name];

    auto const [it, emplaced] =
      state.m_propIndices.emplace(name, state.m_props.size());
    if (emplaced) {
      state.m_props.emplace_back(State::PropTuple{name, src, prop});
      if (trait) cinfo.traitProps.emplace_back(prop);

      assertx(!dinfo.decl);
      assertx(dinfo.subDecls.empty());
      dinfo.decl = src;
      return true;
    }
    assertx(it->second < state.m_props.size());
    auto& prevTuple = state.m_props[it->second];
    auto const& prev = prevTuple.prop;
    auto const prevSrc = prevTuple.src;

    if (cinfo.name->tsame(prevSrc)) {
      if ((prev.attrs ^ prop.attrs) &
          (AttrStatic | AttrPublic | AttrProtected | AttrPrivate) ||
          (!(prop.attrs & AttrSystemInitialValue) &&
           !(prev.attrs & AttrSystemInitialValue) &&
           !Class::compatibleTraitPropInit(prev.val, prop.val))) {
        ITRACE(2,
               "Adding property failed for `{}' because "
               "two declarations of `{}' at the same level had "
               "different attributes\n",
               cinfo.name, prop.name);
        return false;
      }
      return true;
    }

    if (!(prev.attrs & AttrPrivate)) {
      if ((prev.attrs ^ prop.attrs) & AttrStatic) {
        ITRACE(2,
               "Adding property failed for `{}' because "
               "`{}' was defined both static and non-static\n",
               cinfo.name, prop.name);
        return false;
      }
      if (prop.attrs & AttrPrivate) {
        ITRACE(2,
               "Adding property failed for `{}' because "
               "`{}' was re-declared private\n",
               cinfo.name, prop.name);
        return false;
      }
      if (prop.attrs & AttrProtected && !(prev.attrs & AttrProtected)) {
        ITRACE(2,
               "Adding property failed for `{}' because "
               "`{}' was redeclared protected from public\n",
               cinfo.name, prop.name);
        return false;
      }
    }

    if (trait) {
      cinfo.traitProps.emplace_back(prop);
    } else {
      dinfo.decl = src;
    }
    prevTuple = State::PropTuple{name, src, prop};
    return true;
  }

  static bool merge_properties(ClassInfo2& cinfo,
                               State& dst,
                               const State& src) {
    for (auto const& [name, src, prop] : src.m_props) {
      if (!add_property(cinfo, dst, name, prop, src, false)) {
        return false;
      }
    }
    return true;
  }

  static bool build_constants(const LocalIndex& index,
                              php::Class& cls,
                              ClassInfo2& cinfo,
                              State& state) {
    if (cls.parentName) {
      cinfo.clsConstants = index.classInfo(cls.parentName).clsConstants;
      state.m_cnsFromTrait = index.state(cls.parentName).m_cnsFromTrait;

      assertx(cinfo.cnsSubInfo.empty());
      cinfo.cnsSubInfo = index.classInfo(cls.parentName).cnsSubInfo;
    }

    for (auto const iname : cls.interfaceNames) {
      auto const& iface = index.classInfo(iname);
      auto const& ifaceState = index.state(iname);
      for (auto const& [cnsName, cnsIdx] : iface.clsConstants) {
        auto const added = add_constant(
          index, cinfo, state, cnsName,
          cnsIdx,
          folly::get_ptr(iface.cnsSubInfo, cnsName),
          ifaceState.m_cnsFromTrait.contains(cnsName)
        );
        if (!added) return false;
      }
    }

    auto const addShallowConstants = [&] {
      auto const numConstants = cls.constants.size();
      for (uint32_t idx = 0; idx < numConstants; ++idx) {
        auto const& cns = cls.constants[idx];
        auto const cnsSubInfo = ClsCnsSubInfo::fromCns(cls, cns);
        auto const added = add_constant(
          index, cinfo, state,
          cns.name,
          ClassInfo2::ConstIndexAndKind {
            ConstIndex { cls.name, idx },
            cns.kind
          },
          &cnsSubInfo,
          false
        );
        if (!added) return false;
      }
      return true;
    };

    auto const addTraitConstants = [&] {
      for (auto const tname : cls.usedTraitNames) {
        auto const& trait = index.classInfo(tname);
        for (auto const& [cnsName, cnsIdx] : trait.clsConstants) {
          auto const added = add_constant(
            index, cinfo, state, cnsName,
            cnsIdx,
            folly::get_ptr(trait.cnsSubInfo, cnsName),
            true
          );
          if (!added) return false;
        }
      }
      return true;
    };

    if (Cfg::Eval::TraitConstantInterfaceBehavior) {
      // trait constants must be inserted before constants shallowly
      // declared on the class to match the interface semantics
      if (!addTraitConstants()) return false;
      if (!addShallowConstants()) return false;
    } else {
      if (!addShallowConstants()) return false;
      if (!addTraitConstants()) return false;
    }

    for (auto const ename : cls.includedEnumNames) {
      auto const& e = index.classInfo(ename);
      for (auto const& [cnsName, cnsIdx] : e.clsConstants) {
        auto const added = add_constant(
          index, cinfo, state, cnsName,
          cnsIdx, folly::get_ptr(e.cnsSubInfo, cnsName), false
        );
        if (!added) return false;
      }
    }

    auto const addTraitConst = [&] (const php::Const& c) {
     /*
      * Only copy in constants that win. Otherwise, in the runtime, if
      * we have a constant from an interface implemented by a trait
      * that wins over this fromTrait constant, we won't know which
      * trait it came from, and therefore won't know which constant
      * should win. Dropping losing constants here works because if
      * they fatal with constants in declared interfaces, we catch that
      * above.
      */
      auto const& existing = cinfo.clsConstants.find(c.name);
      if (existing->second.idx.cls->tsame(c.cls)) {
        state.m_traitCns.emplace_back(c);
        state.m_traitCns.back().isFromTrait = true;
      }
    };
    for (auto const tname : cls.usedTraitNames) {
      auto const& trait      = index.cls(tname);
      auto const& traitState = index.state(tname);
      for (auto const& c : trait.constants)       addTraitConst(c);
      for (auto const& c : traitState.m_traitCns) addTraitConst(c);
    }

    if (cls.attrs & (AttrAbstract | AttrInterface | AttrTrait)) return true;

    std::vector<SString> sortedClsConstants;
    sortedClsConstants.reserve(cinfo.clsConstants.size());
    for (auto const& [name, _] : cinfo.clsConstants) {
      sortedClsConstants.emplace_back(name);
    }
    std::sort(
      sortedClsConstants.begin(),
      sortedClsConstants.end(),
      string_data_lt{}
    );

    SStringSet copied;

    for (auto const name : sortedClsConstants) {
      auto& cnsIdx = cinfo.clsConstants.find(name)->second;
      if (cnsIdx.idx.cls->tsame(cls.name)) continue;

      auto const& cns = index.cns(cnsIdx.idx);
      if (!cns.isAbstract || !cns.val) continue;

      if (cns.val->m_type == KindOfUninit) {
        auto const& cnsCls = index.cls(cnsIdx.idx.cls);
        assertx(!cnsCls.methods.empty());
        assertx(cnsCls.methods.back()->name == s_86cinit.get());
        auto const& cnsCInit = *cnsCls.methods.back();

        if (cls.methods.empty() ||
            cls.methods.back()->name != s_86cinit.get()) {
          ClonedClosures clonedClosures;
          auto cloned = clone(
            index,
            cnsCInit,
            cnsCInit.name,
            cnsCInit.attrs,
            cls,
            clonedClosures,
            true
          );
          assertx(cloned);
          assertx(clonedClosures.empty());
          assertx(cloned->cls == &cls);
          cloned->clsIdx = cls.methods.size();
          auto const DEBUG_ONLY emplaced =
            cinfo.methods.emplace(cloned->name, MethTabEntry { *cloned });
          assertx(emplaced.second);
          cls.methods.emplace_back(std::move(cloned));
        } else {
          auto const DEBUG_ONLY succeeded =
            append_86cinit(cls.methods.back().get(), cnsCInit);
          assertx(succeeded);
        }
      }

      // This is similar to trait constant flattening
      auto copy = cns;
      copy.cls = cls.name;
      copy.isAbstract = false;
      state.m_cnsFromTrait.erase(copy.name);
      copied.emplace(copy.name);

      if (auto sinfo = ClsCnsSubInfo::fromCns(cls, copy); !sinfo.isMissing()) {
        cinfo.cnsSubInfo.insert_or_assign(copy.name, std::move(sinfo));
      } else {
        cinfo.cnsSubInfo.erase(copy.name);
      }

      cnsIdx.idx.cls = cls.name;
      cnsIdx.idx.idx = cls.constants.size();
      cnsIdx.kind = copy.kind;
      cls.constants.emplace_back(std::move(copy));
    }
    // If we've copied a constant into this class, remove it from
    // the m_traitCns list, or we might try to import an identical
    // copy again when we flatten traits.
    if (!copied.empty()) {
      state.m_traitCns.erase(
        std::remove_if(
          begin(state.m_traitCns), end(state.m_traitCns),
          [&] (const php::Const& cns) { return copied.contains(cns.name); }
        ),
        end(state.m_traitCns)
      );
    }
    return true;
  }

  static bool add_constant(const LocalIndex& index,
                           ClassInfo2& cinfo,
                           State& state,
                           SString name,
                           const ClassInfo2::ConstIndexAndKind& cnsIdx,
                           const ClsCnsSubInfo* fromSubInfo,
                           bool fromTrait) {
    auto [it, emplaced] = cinfo.clsConstants.emplace(name, cnsIdx);
    if (emplaced) {
      if (fromTrait) {
        always_assert(state.m_cnsFromTrait.emplace(name).second);
      } else {
        always_assert(!state.m_cnsFromTrait.contains(name));
      }

      if (fromSubInfo && !fromSubInfo->isMissing()) {
        always_assert(
          cinfo.cnsSubInfo.emplace(name, *fromSubInfo).second
        );
      } else {
        assertx(!cinfo.cnsSubInfo.contains(name));
      }
      return true;
    }
    auto& existingIdx = it->second;

    // Same constant (from an interface via two different paths) is ok
    if (existingIdx.idx.cls->tsame(cnsIdx.idx.cls)) return true;

    auto const& existingCnsCls = index.cls(existingIdx.idx.cls);
    auto const& existing = index.cns(existingIdx.idx);
    auto const& cns = index.cns(cnsIdx.idx);

    if (existing.kind != cns.kind) {
      ITRACE(
        2,
        "Adding constant failed for `{}' because `{}' was defined by "
        "`{}' as a {} and by `{}' as a {}\n",
        cinfo.name,
        name,
        cnsIdx.idx.cls,
        ConstModifierFlags::show(cns.kind),
        existingIdx.idx.cls,
        ConstModifierFlags::show(existing.kind)
      );
      return false;
    }

    // Ignore abstract constants
    if (cns.isAbstract && !cns.val) return true;
    // If the existing constant in the map is concrete, then don't
    // overwrite it with an incoming abstract constant's default
    if (!existing.isAbstract && cns.isAbstract) return true;

    if (existing.val) {
      /*
       * A constant from a declared interface collides with a constant
       * (Excluding constants from interfaces a trait implements).
       *
       * Need this check otherwise constants from traits that conflict
       * with declared interfaces will silently lose and not conflict
       * in the runtime.
       *
       * Type and Context constants can be overridden.
       */
      auto const& cnsCls = index.cls(cnsIdx.idx.cls);
      if (cns.kind == ConstModifierFlags::Kind::Value &&
          !existing.isAbstract &&
          (existingCnsCls.attrs & AttrInterface) &&
          !((cnsCls.attrs & AttrInterface) && fromTrait)) {
        auto const& cls = index.cls(cinfo.name);
        for (auto const iface : cls.interfaceNames) {
          if (existingIdx.idx.cls->tsame(iface)) {
            ITRACE(
              2,
              "Adding constant failed for `{}' because "
              "`{}' was defined by both `{}' and `{}'\n",
              cinfo.name,
              name,
              cnsIdx.idx.cls,
              existingIdx.idx.cls
            );
            return false;
          }
        }
      }

      // Constants from traits silently lose
      if (!Cfg::Eval::TraitConstantInterfaceBehavior && fromTrait) return true;

      if ((cnsCls.attrs & AttrInterface ||
           (Cfg::Eval::TraitConstantInterfaceBehavior &&
            (cnsCls.attrs & AttrTrait))) &&
          (existing.isAbstract ||
           cns.kind == ConstModifierFlags::Kind::Type)) {
        // Because existing has val, this covers the case where it is
        // abstract with default allow incoming to win.  Also, type
        // constants from interfaces may be overridden even if they're
        // not abstract.
      } else {
        // A constant from an interface or from an included enum
        // collides with an existing constant.
        if (cnsCls.attrs & (AttrInterface | AttrEnum | AttrEnumClass) ||
            (Cfg::Eval::TraitConstantInterfaceBehavior &&
             (cnsCls.attrs & AttrTrait))) {
          ITRACE(
            2,
            "Adding constant failed for `{}' because "
            "`{}' was defined by both `{}' and `{}'\n",
            cinfo.name,
            name,
            cnsIdx.idx.cls,
            existingIdx.idx.cls
          );
          return false;
        }
      }
    }

    if (fromTrait) {
      state.m_cnsFromTrait.emplace(name);
    } else {
      state.m_cnsFromTrait.erase(name);
    }

    if (fromSubInfo && !fromSubInfo->isMissing()) {
      cinfo.cnsSubInfo.insert_or_assign(name, *fromSubInfo);
    } else {
      cinfo.cnsSubInfo.erase(name);
    }

    existingIdx = cnsIdx;
    return true;
  }

  /*
   * Make a flattened table of the methods on this class.
   *
   * Duplicate method names override parent methods, unless the parent
   * method is final and the class is not a __MockClass, in which case
   * this class definitely would fatal if ever defined.
   *
   * Note: we're leaving non-overridden privates in their subclass
   * method table, here. This isn't currently "wrong", because calling
   * it would be a fatal, but note that resolve_method needs to be
   * pretty careful about privates and overriding in general.
   */
  static bool build_methods(const LocalIndex& index,
                            const php::Class& cls,
                            ClassInfo2& cinfo,
                            State& state) {
    // Since interface methods are not inherited, any methods in
    // interfaces this class implements are automatically missing.
    assertx(cinfo.methods.empty());
    for (auto const iname : cls.interfaceNames) {
      auto const& iface = index.classInfo(iname);
      for (auto const& [name, _] : iface.methods) {
        if (is_special_method_name(name)) continue;
        cinfo.missingMethods.emplace(name);
      }
      for (auto const name : iface.missingMethods) {
        assertx(!is_special_method_name(name));
        cinfo.missingMethods.emplace(name);
      }
    }

    // Interface methods are just stubs which return null. They don't
    // get inherited by their implementations.
    if (cls.attrs & AttrInterface) {
      assertx(!cls.parentName);
      assertx(cls.usedTraitNames.empty());
      uint32_t idx = cinfo.methods.size();
      assertx(!idx);
      for (auto const& m : cls.methods) {
        auto const res = cinfo.methods.emplace(m->name, MethTabEntry { *m });
        always_assert(res.second);
        always_assert(state.m_methodIndices.emplace(m->name, idx++).second);
        if (cinfo.missingMethods.contains(m->name)) {
          assertx(!res.first->second.firstName());
          cinfo.missingMethods.erase(m->name);
        } else {
          res.first->second.setFirstName();
        }
        ITRACE(4, "  {}: adding method {}::{}\n",
               cls.name, cls.name, m->name);
      }
      return true;
    }

    auto const overridden = [&] (MethTabEntry& existing,
                                 MethRef meth,
                                 Attr attrs) {
      auto const& existingMeth = index.meth(existing);
      if (existingMeth.attrs & AttrFinal) {
        if (!is_mock_class(&cls)) {
          ITRACE(
            2,
            "Adding methods failed for `{}' because "
            "it tried to override final method `{}::{}'\n",
            cls.name,
            existing.meth().cls,
            existingMeth.name
          );
          return false;
        }
      }
      ITRACE(
        4,
        "{}: overriding method {}::{} with {}::{}\n",
        cls.name,
        existing.meth().cls,
        existingMeth.name,
        meth.cls,
        existingMeth.name
      );
      if (existingMeth.attrs & AttrPrivate) existing.setHasPrivateAncestor();
      existing.setMeth(meth);
      existing.attrs = attrs;
      existing.setTopLevel();
      return true;
    };

    // If there's a parent, start by copying its methods
    if (cls.parentName) {
      auto const& parentInfo = index.classInfo(cls.parentName);

      assertx(cinfo.methods.empty());
      cinfo.missingMethods.insert(
        begin(parentInfo.missingMethods),
        end(parentInfo.missingMethods)
      );

      for (auto const& mte : parentInfo.methods) {
        // Don't inherit the 86* methods
        if (HPHP::Func::isSpecial(mte.first)) continue;

        auto const emplaced = cinfo.methods.emplace(mte);
        always_assert(emplaced.second);
        emplaced.first->second.clearTopLevel();
        emplaced.first->second.clearFirstName();

        always_assert(
          state.m_methodIndices.emplace(
            mte.first,
            index.methodIdx(cls.parentName, mte.first)
          ).second
        );

        cinfo.missingMethods.erase(mte.first);

        ITRACE(
          4,
          "{}: inheriting method {}::{}\n",
          cls.name,
          cls.parentName,
          mte.first
        );
      }
    }

    auto idx = cinfo.methods.size();
    auto const clsHasModuleLevelTrait =
      cls.userAttributes.contains(s___ModuleLevelTrait.get());

    // Now add our methods.
    for (auto const& m : cls.methods) {
      if ((cls.attrs & AttrTrait) &&
          (!((cls.attrs & AttrInternal) || clsHasModuleLevelTrait)) &&
          (m->attrs & AttrInternal)) {
        ITRACE(2,
            "Adding methods failed for `{}' because "
            "method `{}' is internal and public traits "
            "cannot define internal methods unless they have "
            "the <<__ModuleLevelTrait>> attribute\n",
            cls.name, m->name);
        return false;
      }
      auto const emplaced = cinfo.methods.emplace(m->name, MethTabEntry { *m });
      if (emplaced.second) {
        ITRACE(
          4,
          "{}: adding method {}::{}\n",
          cls.name,
          cls.name,
          m->name
        );
        always_assert(state.m_methodIndices.emplace(m->name, idx++).second);
        if (cinfo.missingMethods.contains(m->name)) {
          assertx(!emplaced.first->second.firstName());
          cinfo.missingMethods.erase(m->name);
        } else {
          emplaced.first->second.setFirstName();
        }
        continue;
      }

      // If the method is already in our table, it shouldn't be
      // missing.
      assertx(!cinfo.missingMethods.contains(m->name));

      assertx(!emplaced.first->second.firstName());

      if ((m->attrs & AttrTrait) && (m->attrs & AttrAbstract)) {
        // Abstract methods from traits never override anything.
        continue;
      }
      if (!overridden(emplaced.first->second, MethRef { *m }, m->attrs)) {
        return false;
      }
    }

    // If our traits were previously flattened, we're done.
    if (cls.attrs & AttrNoExpandTrait) return true;

    try {
      TMIData tmid;
      for (auto const tname : cls.usedTraitNames) {
        auto const& tcls = index.cls(tname);
        auto const& t = index.classInfo(tname);
        std::vector<std::pair<SString, const MethTabEntry*>>
          methods(t.methods.size());
        for (auto const& [name, mte] : t.methods) {
          if (HPHP::Func::isSpecial(name)) continue;
          auto const idx = index.methodIdx(tname, name);
          assertx(!methods[idx].first);
          methods[idx] = std::make_pair(name, &mte);
          if (auto it = cinfo.methods.find(name);
              it != end(cinfo.methods)) {
            it->second.clearFirstName();
          }
        }

        for (auto const name : t.missingMethods) {
          assertx(!is_special_method_name(name));
          if (cinfo.methods.contains(name)) continue;
          cinfo.missingMethods.emplace(name);
        }

        for (auto const& [name, mte] : methods) {
          if (!name) continue;
          auto const& meth = index.meth(*mte);
          tmid.add(
            TraitMethod { std::make_pair(&t, &tcls), &meth, mte->attrs },
            name
          );
        }
        for (auto const& clo : tcls.closures) {
          auto const invoke = find_method(clo.get(), s_invoke.get());
          assertx(invoke);
          cinfo.extraMethods.emplace(MethRef { *invoke });
        }
      }

      auto const traitMethods = tmid.finish(
        std::make_pair(&cinfo, &cls),
        cls.userAttributes.contains(s___EnableMethodTraitDiamond.get())
      );

      // Import the methods.
      for (auto const& mdata : traitMethods) {
        auto const method = mdata.tm.method;
        auto attrs = mdata.tm.modifiers;

        if (attrs == AttrNone) {
          attrs = method->attrs;
        } else {
          auto const attrMask =
            (Attr)(AttrPublic | AttrProtected | AttrPrivate |
                   AttrAbstract | AttrFinal);
          attrs = (Attr)((attrs        &  attrMask) |
                         (method->attrs & ~attrMask));
        }

        auto const emplaced = cinfo.methods.emplace(
          mdata.name,
          MethTabEntry { *method, attrs }
        );
        if (emplaced.second) {
          ITRACE(
            4,
            "{}: adding trait method {}::{} as {}\n",
            cls.name,
            method->cls->name, method->name, mdata.name
          );
          always_assert(
            state.m_methodIndices.emplace(mdata.name, idx++).second
          );
          cinfo.missingMethods.erase(mdata.name);
        } else {
          assertx(!cinfo.missingMethods.contains(mdata.name));
          if (attrs & AttrAbstract) continue;
          if (emplaced.first->second.meth().cls->tsame(cls.name)) continue;
          if (!overridden(emplaced.first->second, MethRef { *method }, attrs)) {
            return false;
          }
          state.methodIdx(index.m_ctx->name, cinfo.name, mdata.name) = idx++;
        }
        cinfo.extraMethods.emplace(MethRef { *method });
      }
    } catch (const TMIOps::TMIException& exn) {
      ITRACE(
        2,
        "Adding methods failed for `{}' importing traits: {}\n",
        cls.name, exn.what()
      );
      return false;
    }

    return true;
  }

  using ClonedClosures =
    hphp_fast_map<const php::Class*, std::unique_ptr<php::Class>>;

  static SString rename_closure(const php::Class& closure,
                                const php::Class& newContext) {
    auto n = closure.name->slice();
    auto const p = n.find(';');
    if (p != std::string::npos) n = n.subpiece(0, p);
    return makeStaticString(folly::sformat("{};{}", n, newContext.name));
  }

  static std::unique_ptr<php::Class>
  clone_closure(const LocalIndex& index,
                const php::Class& closure,
                const php::Class& newContext,
                bool requiresFromOriginalModule,
                ClonedClosures& clonedClosures) {
    auto clone = std::make_unique<php::Class>(closure);
    assertx(clone->closureContextCls);

    clone->name = rename_closure(closure, newContext);
    clone->closureContextCls = newContext.name;
    clone->unit = newContext.unit;

    ITRACE(4, "- cloning closure {} as {} (with context {})\n",
           closure.name, clone->name, newContext.name);

    for (size_t i = 0, numMeths = clone->methods.size(); i < numMeths; ++i) {
      auto meth = std::move(clone->methods[i]);
      meth->cls = clone.get();
      assertx(meth->clsIdx == i);
      if (!meth->originalUnit)     meth->originalUnit = meth->unit;
      if (!meth->originalClass)    meth->originalClass = closure.name;
      meth->requiresFromOriginalModule = requiresFromOriginalModule;
      meth->unit = newContext.unit;

      clone->methods[i] =
        clone_closures(index, std::move(meth), requiresFromOriginalModule, clonedClosures);
      if (!clone->methods[i]) return nullptr;
    }

    return clone;
  }

  static std::unique_ptr<php::Func>
  clone_closures(const LocalIndex& index,
                 std::unique_ptr<php::Func> cloned,
                 bool requiresFromOriginalModule,
                 ClonedClosures& clonedClosures) {
    if (!cloned->hasCreateCl) return cloned;

    auto const onClosure = [&] (LSString& closureName) {
      auto const& cls = index.cls(closureName);
      assertx(is_closure(cls));

      // CreateCls are allowed to refer to the same closure within the
      // same func. If this is a duplicate, use the already cloned
      // closure name.
      if (auto const it = clonedClosures.find(&cls);
          it != clonedClosures.end()) {
        closureName = it->second->name;
        return true;
      }

      // Otherwise clone the closure (which gives it a new name), and
      // update the name in the CreateCl to match.
      auto closure = clone_closure(
        index,
        cls,
        cloned->cls->closureContextCls
          ? index.cls(cloned->cls->closureContextCls)
          : *cloned->cls,
        requiresFromOriginalModule,
        clonedClosures
      );
      if (!closure) return false;
      closureName = closure->name;
      always_assert(clonedClosures.emplace(&cls, std::move(closure)).second);
      return true;
    };

    auto mf = php::WideFunc::mut(cloned.get());
    assertx(!mf.blocks().empty());
    for (size_t bid = 0; bid < mf.blocks().size(); bid++) {
      auto const b = mf.blocks()[bid].mutate();
      for (size_t ix = 0; ix < b->hhbcs.size(); ix++) {
        auto& bc = b->hhbcs[ix];
        switch (bc.op) {
          case Op::CreateCl: {
            if (!onClosure(bc.CreateCl.str2)) return nullptr;
            break;
          }
          default:
            break;
        }
      }
    }

    return cloned;
  }

  static std::unique_ptr<php::Func> clone(const LocalIndex& index,
                                          const php::Func& orig,
                                          SString name,
                                          Attr attrs,
                                          const php::Class& dstCls,
                                          ClonedClosures& clonedClosures,
                                          bool internal = false) {
    auto cloned = std::make_unique<php::Func>(orig);
    cloned->name = name;
    cloned->attrs = attrs;
    if (!internal) cloned->attrs |= AttrTrait;
    cloned->cls = const_cast<php::Class*>(&dstCls);
    cloned->unit = dstCls.unit;

    if (!cloned->originalUnit)     cloned->originalUnit = orig.unit;
    cloned->originalClass = orig.originalClass
      ? orig.originalClass
      : orig.cls->name;
    cloned->originalModuleName = orig.originalModuleName;

    // If the "module level traits" semantics is enabled, whenever HHBBC
    // inlines a method from a trait defined in module A into a trait/class
    // defined in module B, it sets the requiresFromOriginalModule flag of the
    // method to true. This flag causes the originalModuleName field to be
    // copied in the HHVM extendedSharedData section of the method, so that
    // HHVM is able to resolve correctly the original module of the method.
    // Preserving the original module of a method is also needed when a
    // method is defined in an internal trait that is used by a module level
    // trait.
    const bool requiresFromOriginalModule = [&] () {
      bool copyFromModuleLevelTrait =
        orig.fromModuleLevelTrait && !orig.requiresFromOriginalModule &&
        orig.originalModuleName != dstCls.moduleName;
      bool copyFromInternal =
        (orig.cls->attrs & AttrInternal)
        && dstCls.userAttributes.contains(s___ModuleLevelTrait.get());

      if (Cfg::Eval::ModuleLevelTraits &&
          (copyFromModuleLevelTrait || copyFromInternal)) {
        return true;
      } else {
        return orig.requiresFromOriginalModule;
      }
    }();
    cloned->requiresFromOriginalModule = requiresFromOriginalModule;

    // cloned method isn't in any method table yet, so trash its
    // index.
    cloned->clsIdx = std::numeric_limits<uint32_t>::max();
    return clone_closures(index, std::move(cloned), requiresFromOriginalModule, clonedClosures);
  }

  static bool merge_inits(const LocalIndex& index,
                          const php::Class& cls,
                          const ClassInfo2& cinfo,
                          SString name,
                          std::vector<std::unique_ptr<php::Func>>& clones) {
    auto const existing = [&] () -> const php::Func* {
      for (auto const& m : cls.methods) {
        if (m->name == name) return m.get();
      }
      return nullptr;
    }();

    std::unique_ptr<php::Func> cloned;

    auto const merge = [&] (const php::Func& f) {
      if (!cloned) {
        ClonedClosures clonedClosures;
        if (existing) {
          cloned = clone(
            index,
            *existing,
            existing->name,
            existing->attrs,
            cls,
            clonedClosures,
            true
          );
          assertx(clonedClosures.empty());
          if (!cloned) return false;
        } else {
          ITRACE(4, "- cloning {}::{} as {}::{}\n",
                 f.cls->name, f.name, cls.name, name);
          cloned = clone(index, f, f.name, f.attrs, cls, clonedClosures, true);
          assertx(clonedClosures.empty());
          return (bool)cloned;
        }
      }

      ITRACE(4, "- appending {}::{} into {}::{}\n",
             f.cls->name, f.name, cls.name, name);
      if (name == s_86cinit.get()) return append_86cinit(cloned.get(), f);
      return append_func(cloned.get(), f);
    };

    for (auto const tname : cls.usedTraitNames) {
      auto const& trait = index.classInfo(tname);
      auto const it = trait.methods.find(name);
      if (it == trait.methods.end()) continue;
      auto const& meth = index.meth(it->second);
      if (!merge(meth)) {
        ITRACE(4, "merge_inits: failed to merge {}::{}\n",
               meth.cls->name, name);
        return false;
      }
    }

    if (cloned) {
      ITRACE(4, "merge_inits: adding {}::{} to method table\n",
             cloned->cls->name, cloned->name);
      clones.emplace_back(std::move(cloned));
    }

    return true;
  }

  static bool merge_xinits(const LocalIndex& index,
                           const php::Class& cls,
                           const ClassInfo2& cinfo,
                           const State& state,
                           std::vector<std::unique_ptr<php::Func>>& clones) {
    auto const merge_one = [&] (SString name, Attr attr) {
      auto const unnecessary = std::all_of(
        cinfo.traitProps.begin(),
        cinfo.traitProps.end(),
        [&] (const php::Prop& p) {
         if ((p.attrs & (AttrStatic | AttrLSB)) != attr) return true;
         if (p.val.m_type != KindOfUninit) return true;
         if (p.attrs & AttrLateInit) return true;
         return false;
       }
      );
      if (unnecessary) return true;
      return merge_inits(index, cls, cinfo, name, clones);
    };

    if (!merge_one(s_86pinit.get(), AttrNone))             return false;
    if (!merge_one(s_86sinit.get(), AttrStatic))           return false;
    if (!merge_one(s_86linit.get(), AttrStatic | AttrLSB)) return false;

    auto const unnecessary = std::all_of(
      state.m_traitCns.begin(),
      state.m_traitCns.end(),
      [&] (const php::Const& c) {
        return !c.val || c.val->m_type != KindOfUninit;
      }
    );
    if (unnecessary) return true;
    return merge_inits(index, cls, cinfo, s_86cinit.get(), clones);
  }

  static std::vector<std::unique_ptr<ClassInfo2>>
  flatten_traits(const LocalIndex& index,
                 php::Class& cls,
                 ClassInfo2& cinfo,
                 State& state) {
    if (cls.attrs & AttrNoExpandTrait) return {};
    if (cls.usedTraitNames.empty()) {
      cls.attrs |= AttrNoExpandTrait;
      return {};
    }

    ITRACE(4, "flatten traits: {}\n", cls.name);
    Trace::Indent indent;

    assertx(!is_closure(cls));

    auto traitHasConstProp = cls.hasConstProp;
    for (auto const tname : cls.usedTraitNames) {
      auto const& trait = index.cls(tname);
      auto const& tinfo = index.classInfo(tname);
      if (!(trait.attrs & AttrNoExpandTrait)) {
        ITRACE(4, "Not flattening {} because of {}\n", cls.name, trait.name);
        return {};
      }
      if (is_noflatten_trait(&trait)) {
        ITRACE(
          4, "Not flattening {} because {} is annotated with __NoFlatten\n",
          cls.name, trait.name
        );
        return {};
      }
      if (tinfo.hasConstProp) traitHasConstProp = true;
    }

    std::vector<std::pair<SString, MethTabEntry*>> toAdd;
    for (auto& [name, mte] : cinfo.methods) {
      if (!mte.topLevel()) continue;
      if (mte.meth().cls->tsame(cls.name)) continue;
      assertx(index.cls(mte.meth().cls).attrs & AttrTrait);
      toAdd.emplace_back(name, &mte);
    }

    if (!toAdd.empty()) {
      assertx(!cinfo.extraMethods.empty());
      std::sort(
        toAdd.begin(), toAdd.end(),
        [&] (auto const& a, auto const& b) {
          return
            state.methodIdx(index.m_ctx->name, cinfo.name, a.first) <
            state.methodIdx(index.m_ctx->name, cinfo.name, b.first);
        }
      );
    } else if constexpr (debug) {
      // When building the ClassInfos, we proactively added all
      // closures from usedTraits to the extraMethods map; but now
      // we're going to start from the used methods, and deduce which
      // closures actually get pulled in. Its possible *none* of the
      // methods got used, in which case, we won't need their closures
      // either. To be safe, verify that the only things in the map
      // are closures.
      for (auto const& mte : cinfo.extraMethods) {
        auto const& meth = index.meth(mte);
        always_assert(meth.isClosureBody);
      }
    }

    std::vector<std::unique_ptr<php::Func>> clones;
    ClonedClosures clonedClosures;

    for (auto const& [name, mte] : toAdd) {
      auto const& meth = index.meth(*mte);
      auto cloned = clone(
        index,
        meth,
        name,
        mte->attrs,
        cls,
        clonedClosures
      );
      if (!cloned) {
        ITRACE(4, "Not flattening {} because {}::{} could not be cloned\n",
               cls.name, mte->meth().cls, name);
        return {};
      }
      assertx(cloned->attrs & AttrTrait);
      clones.emplace_back(std::move(cloned));
    }

    if (!merge_xinits(index, cls, cinfo, state, clones)) {
      ITRACE(4, "Not flattening {} because we couldn't merge the 86xinits\n",
             cls.name);
      return {};
    }

    // We're now committed to flattening.
    ITRACE(3, "Flattening {}\n", cls.name);

    if (traitHasConstProp) {
      assertx(cinfo.hasConstProp);
      cls.hasConstProp = true;
    }
    cinfo.extraMethods.clear();

    for (auto [_, mte] : toAdd) mte->attrs |= AttrTrait;

    for (auto& p : cinfo.traitProps) {
      ITRACE(4, "- prop {}\n", p.name);
      auto& info = cinfo.propDeclInfo.at(p.name);
      info.decl = cinfo.name;
      cls.properties.emplace_back(std::move(p));
      cls.properties.back().attrs |= AttrTrait;
    }
    cinfo.traitProps.clear();

    // Type and context constants are explicitly allowed to be overridden,
    // see the logic in `add_constant`. We build a set of existing cns indexes
    // here to check non-value constants against and override in the case
    // of conflicts. Only populate this map if there may be conflicts to resolve.
    SStringToOneT<size_t> existingCnsIndexes;
    if (state.m_traitCns.size()) {
      for (size_t i = 0, size = cls.constants.size(); i < size; ++i) {
        auto const& cns = cls.constants[i];
        if (!cns.val) {
          // Only add non-value constants.
          auto DEBUG_ONLY succeeded =
            existingCnsIndexes.emplace(cns.name, i).second;
          assertx(succeeded);
        }
      }
    }

    for (auto& c : state.m_traitCns) {
      ITRACE(4, "- const {}\n", c.name);

      auto it = cinfo.clsConstants.find(c.name);
      assertx(it != cinfo.clsConstants.end());
      auto& cnsIdx = it->second;

      c.cls = cls.name;
      state.m_cnsFromTrait.erase(c.name);
      cnsIdx.idx.cls = cls.name;

      if (auto sinfo = ClsCnsSubInfo::fromCns(cls, c); !sinfo.isMissing()) {
        cinfo.cnsSubInfo.insert_or_assign(c.name, std::move(sinfo));
      } else {
        cinfo.cnsSubInfo.erase(c.name);
      }

      bool replacedExisting = false;
      if (!c.val) {
        auto it = existingCnsIndexes.find(c.name);
        if (it != existingCnsIndexes.end()) {
          size_t i = it->second;
          auto const& DEBUG_ONLY existingCns = cls.constants[i];
          assertx(existingCns.kind == c.kind);
          replacedExisting = true;
          cnsIdx.idx.idx = i;
          cls.constants[i] = std::move(c);
        }
      }
      if (!replacedExisting) {
        cnsIdx.idx.idx = cls.constants.size();
        cls.constants.emplace_back(std::move(c));
      }
    }
    state.m_traitCns.clear();

    // A class should inherit any declared interfaces of any traits
    // that are flattened into it.
    for (auto const tname : cls.usedTraitNames) {
      auto const& tinfo = index.classInfo(tname);
      cinfo.classGraph.flattenTraitInto(tinfo.classGraph);
      state.m_flattenedInto.emplace(tname);
    }

    // If we flatten the traits into us, they're no longer actual
    // parents.
    state.m_parents.erase(
      std::remove_if(
        begin(state.m_parents),
        end(state.m_parents),
        [] (const php::Class* c) { return bool(c->attrs & AttrTrait); }
      ),
      end(state.m_parents)
    );

    for (auto const tname : cls.usedTraitNames) {
      auto const& traitState = index.state(tname);
      state.m_parents.insert(
        end(state.m_parents),
        begin(traitState.m_parents),
        end(traitState.m_parents)
      );
    }
    std::sort(
      begin(state.m_parents),
      end(state.m_parents),
      [] (const php::Class* a, const php::Class* b) {
        return string_data_lt_type{}(a->name, b->name);
      }
    );
    state.m_parents.erase(
      std::unique(begin(state.m_parents), end(state.m_parents)),
      end(state.m_parents)
    );

    std::vector<std::unique_ptr<ClassInfo2>> newClosures;
    if (!clones.empty()) {
      auto const add = [&] (std::unique_ptr<php::Func> clone) {
        assertx(clone->cls == &cls);
        clone->clsIdx = cls.methods.size();

        if (!is_special_method_name(clone->name)) {
          auto it = cinfo.methods.find(clone->name);
          assertx(it != cinfo.methods.end());
          assertx(!it->second.meth().cls->tsame(cls.name));
          it->second.setMeth(MethRef { cls.name, clone->clsIdx });
        } else {
          auto const [existing, emplaced] =
            cinfo.methods.emplace(clone->name, MethTabEntry { *clone });
          if (!emplaced) {
            assertx(existing->second.meth().cls->tsame(cls.name));
            if (clone->name != s_86cinit.get()) {
              auto const idx = existing->second.meth().idx;
              clone->clsIdx = idx;
              cls.methods[idx] = std::move(clone);
              return;
            } else {
              existing->second.setMeth(MethRef { cls.name, clone->clsIdx });
            }
          }
        }

        cls.methods.emplace_back(std::move(clone));
      };

      auto cinit = [&] () -> std::unique_ptr<php::Func> {
        if (cls.methods.empty()) return nullptr;
        if (cls.methods.back()->name != s_86cinit.get()) return nullptr;
        auto init = std::move(cls.methods.back());
        cls.methods.pop_back();
        return init;
      }();

      for (auto& clone : clones) {
        ITRACE(4, "- meth {}\n", clone->name);
        if (clone->name == s_86cinit.get()) {
          cinit = std::move(clone);
          continue;
        }
        add(std::move(clone));
      }
      if (cinit) add(std::move(cinit));

      for (auto& [orig, clo] : clonedClosures) {
        ITRACE(4, "- closure {} as {}\n", orig->name, clo->name);
        assertx(is_closure(*orig));
        assertx(is_closure(*clo));
        assertx(clo->closureContextCls->tsame(cls.name));
        assertx(clo->unit == cls.unit);

        assertx(clo->usedTraitNames.empty());
        State cloState;
        auto cloinfo = make_info(index, *clo, cloState);
        assertx(cloinfo);
        assertx(cloState.m_traitCns.empty());
        assertx(cloState.m_cnsFromTrait.empty());
        assertx(cloState.m_parents.size() == 1);
        assertx(cloState.m_parents[0]->name->tsame(s_Closure.get()));

        cls.closures.emplace_back(std::move(clo));
        newClosures.emplace_back(std::move(cloinfo));
      }
    }

    // Flattening methods into traits can turn methods from not "first
    // name" to "first name", so recalculate that here.
    for (auto& [name, mte] : cinfo.methods) {
      if (mte.firstName()) continue;
      auto const firstName = [&, name=name] {
        if (cls.parentName) {
          auto const& parentInfo = index.classInfo(cls.parentName);
          if (parentInfo.methods.contains(name)) return false;
          if (parentInfo.missingMethods.contains(name)) return false;
        }
        for (auto const iname : cinfo.classGraph.interfaces()) {
          auto const& iface = index.classInfo(iname.name());
          if (iface.methods.contains(name)) return false;
          if (iface.missingMethods.contains(name)) return false;
        }
        return true;
      }();
      if (firstName) mte.setFirstName();
    }

    struct EqHash {
      bool operator()(const PreClass::ClassRequirement& a,
                      const PreClass::ClassRequirement& b) const {
        return a.is_same(&b);
      }
      size_t operator()(const PreClass::ClassRequirement& a) const {
        return a.hash();
      }
    };
    hphp_fast_set<PreClass::ClassRequirement, EqHash, EqHash> reqs{
      cls.requirements.begin(),
      cls.requirements.end()
    };

    for (auto const tname : cls.usedTraitNames) {
      auto const& trait = index.cls(tname);
      for (auto const& req : trait.requirements) {
        if (reqs.emplace(req).second) cls.requirements.emplace_back(req);
      }
    }

    cls.attrs |= AttrNoExpandTrait;
    return newClosures;
  }

  static std::unique_ptr<FuncInfo2> make_func_info(const LocalIndex& index,
                                                   const php::Func& f) {
    auto finfo = std::make_unique<FuncInfo2>();
    finfo->name = f.name;
    return finfo;
  }

  static bool resolve_one(TypeConstraint& tc,
                          const TypeConstraint& tv,
                          TSStringSet& uses,
                          bool isProp,
                          bool isUnion) {
    assertx(!tv.isUnion());
    // Whatever it's an alias of isn't valid, so leave unresolved.
    if (tv.isUnresolved()) return false;
    if (isProp && !propSupportsAnnot(tv.type())) return false;
    auto const value = [&] () -> SString {
      if (tv.isSubObject()) {
        auto clsName = tv.clsName();
        assertx(clsName);
        return clsName;
      }
      return nullptr;
    }();
    if (isUnion) tc.unresolve();
    tc.resolveType(tv.type(), tv.isNullable(), value);
    assertx(IMPLIES(isProp, tc.validForProp()));
    if (value) uses.emplace(value);
    return true;
  }

  // Update a type constraint to it's ultimate type, or leave it as
  // unresolved if it resolves to nothing valid. Record the new type
  // in case it needs to be fixed up later.
  static void update_type_constraint(const LocalIndex& index,
                                     TypeConstraint& tc,
                                     bool isProp,
                                     TSStringSet& uses,
                                     SStringSet& predeps) {
    always_assert(IMPLIES(isProp, tc.validForProp()));

    if (!tc.isUnresolved()) {
      // Any TC already resolved is assumed to be correct.
      for (auto& part : eachTypeConstraintInUnion(tc)) {
        if (auto clsName = part.clsName()) uses.emplace(clsName);
      }
      return;
    }
    auto const name = tc.typeName();
    predeps.emplace(name);

    if (tc.isUnion()) {
      // This is a union that contains unresolved names.
      not_implemented(); // TODO(T151885113)
    }

    // This is an unresolved name that can resolve to either a single type or
    // a union.

    // Is this name a type-alias or enum?
    if (auto const tm = index.typeMapping(name)) {
      predeps.emplace(tm->name);

      if (tm->value.isUnion()) {
        auto flags =
          tc.flags() & (TypeConstraintFlags::Nullable
                        | TypeConstraintFlags::TypeVar
                        | TypeConstraintFlags::Soft
                        | TypeConstraintFlags::TypeConstant
                        | TypeConstraintFlags::DisplayNullable
                        | TypeConstraintFlags::UpperBound);
        std::vector<TypeConstraint> members;
        for (auto& tv : eachTypeConstraintInUnion(tm->value)) {
          TypeConstraint copy = tv;
          copy.addFlags(flags);
          if (!resolve_one(copy, tv, uses, isProp, true)) {
            return;
          }
          members.emplace_back(std::move(copy));
        }
        tc = TypeConstraint::makeUnion(name, members);
        if (tm->typeStructure) {
          type_structure_references(tm->typeStructure, predeps);
        }
        return;
      }

      // This unresolved name resolves to a single type.
      resolve_one(tc, tm->value, uses, isProp, false);
      if (tm->typeStructure) {
        type_structure_references(tm->typeStructure, predeps);
      }
      return;
    }

    // Not a type-alias or enum. If it's explicitly marked as missing,
    // leave it unresolved. Otherwise assume it's an object with that
    // name.
    if (index.missingType(name)) return;
    tc.resolveType(AnnotType::SubObject, tc.isNullable(), name);
    uses.emplace(name);
  }

  static void update_type_constraints(const LocalIndex& index,
                                      php::Func& func,
                                      TSStringSet& uses,
                                      OutputMeta::NewPredeps& predeps) {
    auto& pre = is_86init_func(func) ? predeps.cinitPredeps : predeps.predeps;

    for (auto& p : func.params) {
      p.typeConstraints.forEachMutable([&](TypeConstraint& tc) {
          update_type_constraint(index, tc, false, uses, pre);
      });
    }

    func.retTypeConstraints.forEachMutable(
      [&](TypeConstraint& tc) {
        update_type_constraint(index, tc, false, uses, pre);
      }
    );
  }

  static void update_type_constraints(const LocalIndex& index,
                                      php::Class& cls,
                                      TSStringSet& uses,
                                      OutputMeta::NewPredeps& predeps) {
    if (cls.attrs & AttrEnum) {
      update_type_constraint(
        index,
        cls.enumBaseTy,
        false,
        uses,
        predeps.predeps
      );
    }
    for (auto& meth : cls.methods) {
      update_type_constraints(index, *meth, uses, predeps);
    }
    for (auto& prop : cls.properties) {
      prop.typeConstraints.forEachMutable([&](TypeConstraint& tc) {
        update_type_constraint(index, tc, true, uses, predeps.predeps);
      });
    }
  }

  /*
   * Mark any properties in cls that definitely do not redeclare a
   * property in the parent with an inequivalent type-hint.
   *
   * Rewrite the initial values for any AttrSystemInitialValue
   * properties. If the properties' type-hint does not admit null
   * values, change the initial value to one that is not null
   * (if possible). This is only safe to do so if the property is not
   * redeclared in a derived class or if the redeclaration does not
   * have a null system provided default value. Otherwise, a property
   * can have a null value (even if its type-hint doesn't allow it)
   * without the JIT realizing that its possible.
   *
   * Note that this ignores any unflattened traits. This is okay
   * because properties pulled in from traits which match an already
   * existing property can't change the initial value. The runtime
   * will clear AttrNoImplicitNullable on any property pulled from the
   * trait if it doesn't match an existing property.
   */
  static void optimize_properties(const LocalIndex& index,
                                  php::Class& cls,
                                  ClassInfo2& cinfo) {
    assertx(cinfo.hasBadRedeclareProp);

    auto const isClosure = is_closure(cls);

    cinfo.hasBadRedeclareProp = false;
    for (auto& prop : cls.properties) {
      assertx(!(prop.attrs & AttrNoBadRedeclare));
      assertx(!(prop.attrs & AttrNoImplicitNullable));

      auto const noBadRedeclare = [&] {
        // Closures should never have redeclared properties.
        if (isClosure) return true;
        // Static and private properties never redeclare anything so
        // need not be considered.
        if (prop.attrs & (AttrStatic | AttrPrivate)) return true;

        for (auto const base : cinfo.classGraph.bases()) {
          if (base.name()->tsame(cls.name)) continue;

          auto& baseCInfo = index.classInfo(base.name());
          auto& baseCls = index.cls(base.name());

          auto const parentProp = [&] () -> php::Prop* {
            for (auto& p : baseCls.properties) {
              if (p.name == prop.name) return const_cast<php::Prop*>(&p);
            }
            for (auto& p : baseCInfo.traitProps) {
              if (p.name == prop.name) return const_cast<php::Prop*>(&p);
            }
            return nullptr;
          }();
          if (!parentProp) continue;
          if (parentProp->attrs & (AttrStatic | AttrPrivate)) continue;

          // This property's type-constraint might not have been
          // resolved (if the parent is not on the output list for
          // this job), so do so here.
          TSStringSet uses;
          SStringSet predeps;
          parentProp->typeConstraints.forEachMutable([&](TypeConstraint& tc) {
            update_type_constraint(index, tc, true, uses, predeps);
          });

          // This check is safe, but conservative. It might miss a few
          // rare cases, but it's sufficient and doesn't require class
          // hierarchies.
          auto tcs = prop.typeConstraints.range();
          for (auto it = tcs.begin(); it != tcs.end(); ++it) {
            for (auto ptc : parentProp->typeConstraints.range()) {
              if (it->maybeInequivalentForProp(ptc)) return false;
            }
          }
        }

        return true;
      }();

      if (noBadRedeclare) {
        attribute_setter(prop.attrs, true, AttrNoBadRedeclare);
      } else {
        cinfo.hasBadRedeclareProp = true;
      }

      auto const dv = prop.typeConstraints.defaultValue();
      auto const nullable = [&] {
        if (isClosure) return true;
        if (!(prop.attrs & AttrSystemInitialValue)) return false;
        return dv && dv->m_type == KindOfNull;
      }();

      attribute_setter(prop.attrs, dv && !nullable, AttrNoImplicitNullable);
      if (!(prop.attrs & AttrSystemInitialValue)) continue;
      if (prop.val.m_type == KindOfUninit) {
        assertx(isClosure || bool(prop.attrs & AttrLateInit));
        continue;
      }

      prop.val = [&] {
        if (nullable) return make_tv<KindOfNull>();
        // Give the 86reified_prop a special default value to avoid
        // pessimizing the inferred type (we want it to always be a
        // vec of a specific size).
        if (prop.name == s_86reified_prop.get()) {
          return get_default_value_of_reified_list(cls.userAttributes);
        }

        return dv ? dv.value() : make_tv<KindOfNull>();
      }();
    }
  }
};

Job<FlattenJob> s_flattenJob;

/*
 * For efficiency reasons, we want to do class flattening all in one
 * pass. So, we use assign_hierarchical_work (described above) to
 * calculate work buckets to allow us to do this.
 *
 * - The "root" classes are the leaf classes in the hierarchy. These are
 *   the buckets which are not dependencies of anything.
 *
 * - The dependencies of a class are all of the (transitive) parent
 *   classes of that class (up to the top classes with no parents).
 *
 * - Each job takes two kinds of input. The first is the set of
 *   classes which are actually to be flattened. These will have the
 *   flattening results returned as output from the job. The second is
 *   the set of dependencies that are required to perform flattening
 *   on the first set of inputs. These will have the same flattening
 *   algorithm applied to them, but only to obtain intermediate state
 *   to calculate the output for the first set of inputs. Their
 *   results will be thrown away.
 *
 * - When we run the jobs, we'll take the outputs and turn that into a set of
 *   updates, which we then apply to the Index data structures. Some
 *   of these updates require changes to the php::Unit, which we do a
 *   in separate set of "fixup" jobs at the end.
 */

// Input class metadata to be turned into work buckets.
struct IndexFlattenMetadata {
  struct ClassMeta {
    TSStringSet deps;
    // All types mentioned in type-constraints in this class.
    std::vector<SString> unresolvedTypes;
    size_t idx; // Index into allCls vector
    bool isClosure{false};
    bool uninstantiable{false};
  };
  TSStringToOneT<ClassMeta> cls;
  // All classes to be flattened
  std::vector<SString> allCls;
  // Mapping of units to classes which should be deleted from that
  // unit. This is typically from duplicate meth callers. This is
  // performed as part of "fixing up" the unit after flattening
  // because it's convenient to do so there.
  SStringToOneT<std::vector<SString>> unitDeletions;
  struct FuncMeta {
    // All types mentioned in type-constraints in this func.
    std::vector<SString> unresolvedTypes;
  };
  FSStringToOneT<FuncMeta> func;
  std::vector<SString> allFuncs;
  TSStringToOneT<TypeMapping> typeMappings;
};

//////////////////////////////////////////////////////////////////////

constexpr size_t kNumTypeMappingRounds = 20;

/*
 * Update the type-mappings in the program so they all point to their
 * ultimate type. After this step, every type-mapping that still has
 * an unresolved type points to an invalid type.
 */
void flatten_type_mappings(IndexData& index,
                           IndexFlattenMetadata& meta) {
  trace_time tracer{"flatten type mappings"};
  tracer.ignore_client_stats();

  std::vector<const TypeMapping*> work;
  work.reserve(meta.typeMappings.size());
  for (auto const& [_, tm] : meta.typeMappings) work.emplace_back(&tm);

  auto resolved = parallel::map(
    work,
    [&] (const TypeMapping* typeMapping) {
      Trace::Bump bump{
        Trace::hhbbc_index,
        trace_bump_for(typeMapping->name, nullptr, typeMapping->unit)
      };

      Optional<TSStringSet> seen;
      TypeConstraintFlags flags =
        typeMapping->value.flags() & (TypeConstraintFlags::Nullable
                                      | TypeConstraintFlags::TypeVar
                                      | TypeConstraintFlags::Soft
                                      | TypeConstraintFlags::TypeConstant
                                      | TypeConstraintFlags::DisplayNullable
                                      | TypeConstraintFlags::UpperBound);
      auto const isUnion = typeMapping->value.isUnion();
      FTRACE(4, "Flattening type mapping {}\n", typeMapping->name);
      bool anyUnresolved = false;

      auto enumMeta = folly::get_ptr(meta.cls, typeMapping->name);

      std::vector<TypeConstraint> tvu;
      Optional<SArray> lastTS;

      for (auto const& tc : eachTypeConstraintInUnion(typeMapping->value)) {
        const auto type = tc.type();
        const auto value = tc.typeName();
        auto name = value;
        auto const inEnum = typeMapping->isEnum;

        if (type != AnnotType::Unresolved) {
          // If the type-mapping is already resolved, we mainly take it
          // as is. The exception is if it's an enum, in which case we
          // validate the underlying base type.
          assertx(type != AnnotType::SubObject);
          if (enumMeta) {
            if (!enumSupportsAnnot(type)) {
              FTRACE(
                2, "Type-mapping '{}' is invalid because it resolves to "
                "invalid enum type {}\n",
                typeMapping->name,
                annotName(type)
              );
              tvu.emplace_back(AnnotType::Unresolved, tc.flags(), value);
              lastTS = nullptr;
              continue;
            }
            tvu.emplace_back(type, tc.flags() | flags, value);
            anyUnresolved = true;
          } else {
            tvu.emplace_back(tc);
          }

          if (!lastTS) {
            lastTS = typeMapping->typeStructure;
          } else if (*lastTS != typeMapping->typeStructure) {
            lastTS = nullptr;
          }
          continue;
        }

        std::queue<std::tuple<LSString, bool>> queue; // item, inEnum
        FTRACE(5, "Pushing ({} {}) onto queue\n", name, inEnum);
        queue.push(std::make_tuple(name, inEnum));

        for (size_t rounds = 0;; ++rounds) {
          if (queue.empty()) break;
          auto [name, inEnum] = queue.front();
          name = normalizeNS(name);
          queue.pop();

          FTRACE(5, "Popping ({} {})\n", name, inEnum);

          if (auto const next = folly::get_ptr(meta.typeMappings, name)) {
            flags |= next->value.flags() & (TypeConstraintFlags::Nullable
                                            | TypeConstraintFlags::TypeVar
                                            | TypeConstraintFlags::Soft
                                            | TypeConstraintFlags::TypeConstant
                                            | TypeConstraintFlags::DisplayNullable
                                            | TypeConstraintFlags::UpperBound);
            assertx(IMPLIES(next->value.isUnion(), !next->isEnum));
            if (!inEnum) inEnum = next->isEnum;
            if (enumMeta && next->isEnum) enumMeta->deps.emplace(name);

            for (auto const& next_tc : eachTypeConstraintInUnion(next->value)) {
              auto next_type = next_tc.type();
              auto next_value = next_tc.typeName();
              if (next_type == AnnotType::Unresolved) {
                FTRACE(5, "Pushing ({} {}) onto queue\n", next_value, inEnum);
                queue.push(std::make_tuple(next_value, inEnum));
                continue;
              }
              assertx(next_type != AnnotType::SubObject);
              if (inEnum && !enumSupportsAnnot(next_type)) {
                FTRACE(
                  2, "Type-mapping '{}' is invalid because it resolves to "
                  "invalid enum type {}\n",
                  typeMapping->name,
                  annotName(next_type)
                );
                tvu.emplace_back(AnnotType::Unresolved, tc.flags() | flags, name);
                anyUnresolved = true;
                lastTS = nullptr;
                continue;
              }
              tvu.emplace_back(next_type, tc.flags() | flags, next_value);
            }

            if (!lastTS) {
              lastTS = next->typeStructure;
            } else if (*lastTS != next->typeStructure) {
              lastTS = nullptr;
            }
          } else if (index.classRefs.contains(name)) {
            if (inEnum) {
              FTRACE(
                2, "Type-mapping '{}' is invalid because it resolves to "
                "invalid object '{}' for enum\n",
                typeMapping->name,
                name
              );
            }

            tvu.emplace_back(
              inEnum ? AnnotType::Unresolved : AnnotType::SubObject,
              tc.flags() | flags,
              name
            );
            if (inEnum) anyUnresolved = true;
            lastTS = nullptr;
            continue;
          } else {
            FTRACE(
              2, "Type-mapping '{}' is invalid because it involves "
              "non-existent type '{}'\n",
              typeMapping->name,
              name
            );
            tvu.emplace_back(AnnotType::Unresolved, tc.flags() | flags, name);
            anyUnresolved = true;
            lastTS = nullptr;
            continue;
          }

          // Deal with cycles. Since we don't expect to encounter them, just
          // use a counter until we hit a chain length of kNumTypeMappingRounds,
          // then start tracking the names we resolve.
          if (rounds == kNumTypeMappingRounds) {
            seen.emplace();
            seen->insert(name);
          } else if (rounds > kNumTypeMappingRounds) {
            if (!seen->insert(name).second) {
              FTRACE(
                2, "Type-mapping '{}' is invalid because it's definition "
                "is circular with '{}'\n",
                typeMapping->name,
                name
              );
              return TypeMapping {
                typeMapping->name,
                TypeConstraint{AnnotType::Unresolved, flags, name},
                false,
                inEnum,
                typeMapping->unit,
                typeMapping->typeStructure
              };
            }
          }
        }
      }
      if (isUnion && anyUnresolved) {
        // Unions cannot contain a mix of resolved an unresolved class names so
        // if one of the names failed to resolve we must mark all of them as
        // unresolved.
        for (auto& tc : tvu) if (tc.isSubObject()) tc.unresolve();
      }
      assertx(!tvu.empty());
      // If any of the subtypes end up unresolved then the final union will also
      // be unresolved. But it's important to try the `makeUnion` anyway because
      // it will deal with some of the canonicalizations like `bool`.
      auto value = TypeConstraint::makeUnion(typeMapping->name, std::move(tvu));
      // Should no longer be a type alias.
      return TypeMapping {
        typeMapping->name,
        value,
        false,
        typeMapping->isEnum,
        typeMapping->unit,
        lastTS.value_or(nullptr)
      };
    }
  );

  for (auto& after : resolved) {
    auto const name = after.name;
    Trace::Bump bump{
      Trace::hhbbc_index, trace_bump_for(name, nullptr, after.unit)
    };
    using namespace folly::gen;
    FTRACE(
      4, "Type-mapping '{}' flattened to {}\n",
      name,
      after.value.debugName()
    );
    if (after.value.isUnresolved() && meta.cls.contains(name)) {
      FTRACE(4, "  Marking enum '{}' as uninstantiable\n", name);
      meta.cls.at(name).uninstantiable = true;
    }
    meta.typeMappings.at(name) = std::move(after);
  }
}

//////////////////////////////////////////////////////////////////////

struct FlattenClassesWork {
  std::vector<SString> classes;
  std::vector<SString> deps;
  std::vector<SString> funcs;
  std::vector<SString> uninstantiable;
};

std::vector<FlattenClassesWork>
flatten_classes_assign(IndexFlattenMetadata& meta) {
  trace_time trace{"flatten classes assign"};
  trace.ignore_client_stats();

  // First calculate the classes which *aren't* leafs. A class is a
  // leaf if it is not depended on by another class. The sense is
  // inverted because we want to default construct the atomics.
  std::vector<std::atomic<bool>> isNotLeaf(meta.allCls.size());
  parallel::for_each(
    meta.allCls,
    [&] (SString cls) {
      auto const& clsMeta = meta.cls.at(cls);
      for (auto const d : clsMeta.deps) {
        auto const it = meta.cls.find(d);
        if (it == meta.cls.end()) continue;
        assertx(it->second.idx < isNotLeaf.size());
        isNotLeaf[it->second.idx] = true;
      }
    }
  );

  // Store all of the (transitive) dependencies for every class,
  // calculated lazily. LockFreeLazy ensures that multiple classes can
  // access this concurrently and safely calculate it on demand.
  struct DepLookup {
    TSStringSet deps;
    // Whether this class is instantiable
    bool instantiable{false};
  };
  std::vector<LockFreeLazy<DepLookup>> allDeps{meta.allCls.size()};

  // Look up all of the transitive dependencies for the given class.
  auto const findAllDeps = [&] (SString cls,
                                TSStringSet& visited,
                                auto const& self) -> const DepLookup& {
    static const DepLookup empty;

    auto const it = meta.cls.find(cls);
    if (it == meta.cls.end()) {
      FTRACE(
        4, "{} is not instantiable because it is missing\n",
        cls
      );
      return empty;
    }

    // The class exists, so look up it's dependency information.
    auto const idx = it->second.idx;
    auto const& deps = it->second.deps;

    // Check for cycles. A class involved in cyclic inheritance is not
    // instantiable (and has no dependencies). This needs to be done
    // before accessing the LockFreeLazy below, because if we are in a
    // cycle, we'll deadlock when we do so.
    auto const emplaced = visited.emplace(cls).second;
    if (!emplaced) {
      FTRACE(
        4, "{} is not instantiable because it forms a dependency "
        "cycle with itself\n", cls
      );
      it->second.uninstantiable = true;
      return empty;
    }
    SCOPE_EXIT { visited.erase(cls); };

    assertx(idx < allDeps.size());
    return allDeps[idx].get(
      [&] {
        // Otherwise get all of the transitive dependencies of it's
        // dependencies and combine them.
        DepLookup out;
        out.instantiable = !it->second.uninstantiable;

        for (auto const d : deps) {
          auto const& lookup = self(d, visited, self);
          if (lookup.instantiable || meta.cls.contains(d)) {
            out.deps.emplace(d);
          }
          out.deps.insert(begin(lookup.deps), end(lookup.deps));
          if (lookup.instantiable) continue;
          // If the dependency is not instantiable, this isn't
          // either. Note, however, we still need to preserve the
          // already gathered dependencies, since they'll have to be
          // placed in some bucket.
          if (out.instantiable) {
            FTRACE(
              4, "{} is not instantiable because it depends on {}, "
              "which is not instantiable\n",
              cls, d
            );
            it->second.uninstantiable = true;
          }
          out.instantiable = false;
        }

        return out;
      }
    );
  };

  constexpr size_t kBucketSize = 2000;
  constexpr size_t kMaxBucketSize = 30000;

  auto assignments = assign_hierarchical_work(
    [&] {
      std::vector<SString> l;
      auto const size = meta.allCls.size();
      assertx(size == isNotLeaf.size());
      l.reserve(size);
      for (size_t i = 0; i < size; ++i) {
        if (!isNotLeaf[i]) l.emplace_back(meta.allCls[i]);
      }
      return l;
    }(),
    meta.allCls.size(),
    kBucketSize,
    kMaxBucketSize,
    [&] (SString c, TSStringSet& deps) {
      TSStringSet visited;
      auto const& lookup = findAllDeps(c, visited, findAllDeps);
      deps.insert(begin(lookup.deps), end(lookup.deps));
    },
    [&] (SString c) {
      TSStringSet visited;
      return findAllDeps(c, visited, findAllDeps).instantiable;
    },
    [&] (const TSStringSet&, size_t, SString c) -> Optional<size_t> {
      return meta.cls.at(c).idx;
    }
  );

  // Bucketize functions separately

  constexpr size_t kFuncBucketSize = 5000;

  auto funcBuckets = consistently_bucketize(meta.allFuncs, kFuncBucketSize);

  std::vector<FlattenClassesWork> work;
  // If both the class and func assignments map to a single bucket,
  // combine them both together. This is an optimization for things
  // like unit tests, where the total amount of work is low and we
  // want to run it all in a single job if possible.
  if (assignments.size() == 1 && funcBuckets.size() == 1) {
    work.emplace_back(
      FlattenClassesWork{
        std::move(assignments[0].classes),
        std::move(assignments[0].deps),
        std::move(funcBuckets[0]),
        std::move(assignments[0].uninstantiable)
      }
    );
  } else {
    // Otherwise split the classes and func work.
    work.reserve(assignments.size() + funcBuckets.size());
    for (auto& assignment : assignments) {
      work.emplace_back(
        FlattenClassesWork{
          std::move(assignment.classes),
          std::move(assignment.deps),
          {},
          std::move(assignment.uninstantiable)
        }
      );
    }
    for (auto& bucket : funcBuckets) {
      work.emplace_back(
        FlattenClassesWork{ {}, {}, std::move(bucket), {} }
      );
    }
  }

  if (Trace::moduleEnabled(Trace::hhbbc_index, 5)) {
    for (size_t i = 0; i < work.size(); ++i) {
      auto const& [classes, deps, funcs, uninstantiable] = work[i];
      FTRACE(5, "flatten work item #{}:\n", i);
      FTRACE(5, "  classes ({}):\n", classes.size());
      for (auto const DEBUG_ONLY c : classes) FTRACE(5, "    {}\n", c);
      FTRACE(5, "  deps ({}):\n", deps.size());
      for (auto const DEBUG_ONLY d : deps) FTRACE(5, "    {}\n", d);
      FTRACE(5, "  funcs ({}):\n", funcs.size());
      for (auto const DEBUG_ONLY f : funcs) FTRACE(5, "    {}\n", f);
      FTRACE(5, "  uninstantiable classes ({}):\n", uninstantiable.size());
      for (auto const DEBUG_ONLY c : uninstantiable) FTRACE(5, "    {}\n", c);
    }
  }

  return work;
}

// Metadata used to assign work buckets for building subclasses. This
// is produced from flattening classes. We don't put closures (or
// Closure base class) into here. There's a lot of them, but we can
// predict their results without running build subclass pass on them.
struct SubclassMetadata {
  // Immediate children and parents of class (not transitive!).
  struct Meta {
    std::vector<SString> children;
    std::vector<SString> parents;
    size_t idx; // Index into all classes vector.
  };
  TSStringToOneT<Meta> meta;
  // All classes to be processed
  std::vector<SString> all;
};

// Metadata used to drive the init-types pass. This is produced from
// flattening classes and added to when building subclasses.
struct InitTypesMetadata {
  struct ClsMeta {
    // Dependencies of the class. A dependency is a class in a
    // property/param/return type-hint.
    TSStringSet deps;
    TSStringSet candidateRegOnlyEquivs;
  };
  struct FuncMeta {
    // Same as ClsMeta, but for the func
    TSStringSet deps;
  };
  // Modifications to make to an unit
  struct Fixup {
    std::vector<SString> addClass;
    std::vector<SString> removeFunc;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(addClass)(removeFunc);
    }
  };
  TSStringToOneT<ClsMeta> classes;
  FSStringToOneT<FuncMeta> funcs;
  SStringToOneT<Fixup> fixups;
  SStringToOneT<std::vector<FuncFamilyEntry>> nameOnlyFF;
};

std::tuple<SubclassMetadata, InitTypesMetadata, std::vector<InterfaceConflicts>>
flatten_classes(IndexData& index, IndexFlattenMetadata meta) {
  trace_time trace("flatten classes", index.sample);
  trace.ignore_client_stats();

  using namespace folly::gen;

  struct ClassUpdate {
    SString name;
    UniquePtrRef<php::Class> cls;
    UniquePtrRef<php::ClassBytecode> bytecode;
    UniquePtrRef<ClassInfo2> cinfo;
    TSStringSet typeUses;
    MethRefSet extraMethods;
    TSStringSet flattenedInto;
    FlattenJob::OutputMeta::NewPredeps newPredeps;
    bool isInterface{false};
    bool has86init{false};
    CompactVector<SString> parents;
  };
  struct FuncUpdate {
    SString name;
    UniquePtrRef<php::Func> func;
    UniquePtrRef<FuncInfo2> finfo;
    TSStringSet typeUses;
    FlattenJob::OutputMeta::NewPredeps newPredeps;
  };
  struct ClosureUpdate {
    SString name;
    SString context;
    SString unit;
  };
  struct MethodUpdate {
    SString name;
    UniquePtrRef<MethodsWithoutCInfo> methods;
  };
  struct UnitUpdate {
    SStringToOneT<SStringSet> originalUnits;
  };
  using Update = std::variant<
    ClassUpdate,
    FuncUpdate,
    ClosureUpdate,
    MethodUpdate,
    UnitUpdate
  >;
  using UpdateVec = std::vector<Update>;

  tbb::concurrent_hash_map<
    SString,
    InterfaceConflicts,
    string_data_hash_tsame
  > ifaceConflicts;

  auto const run = [&] (FlattenClassesWork work) -> coro::Task<UpdateVec> {
    co_await coro::co_reschedule_on_current_executor;

    if (work.classes.empty() &&
        work.funcs.empty() &&
        work.uninstantiable.empty()) {
      assertx(work.deps.empty());
      co_return UpdateVec{};
    }

    auto metadata = make_exec_metadata(
      "flatten classes",
      work.classes.empty()
        ? (work.uninstantiable.empty()
           ? work.funcs[0]->toCppString()
           : work.uninstantiable[0]->toCppString())
        : work.classes[0]->toCppString()
    );
    auto classes = from(work.classes)
      | map([&] (SString c) { return index.classRefs.at(c); })
      | as<std::vector>();
    auto deps = from(work.deps)
      | map([&] (SString c) { return index.classRefs.at(c); })
      | as<std::vector>();
    auto bytecode = (from(work.classes) + from(work.deps))
      | map([&] (SString c) { return index.classBytecodeRefs.at(c); })
      | as<std::vector>();
    auto funcs = from(work.funcs)
      | map([&] (SString f) { return index.funcRefs.at(f); })
      | as<std::vector>();
    auto uninstantiableRefs = from(work.uninstantiable)
      | map([&] (SString c) { return index.classRefs.at(c); })
      | as<std::vector>();

    // Gather any type-mappings or missing types referenced by these
    // classes or funcs.
    std::vector<TypeMapping> typeMappings;
    std::vector<SString> missingTypes;
    {
      TSStringSet seen;

      auto const addUnresolved = [&] (SString u) {
        if (!seen.emplace(u).second) return;
        if (auto const m = folly::get_ptr(meta.typeMappings, u)) {
          typeMappings.emplace_back(*m);
        } else if (!index.classRefs.contains(u) ||
                   meta.cls.at(u).uninstantiable) {
          missingTypes.emplace_back(u);
        }
      };

      auto const addClass = [&] (SString c) {
        for (auto const u : meta.cls.at(c).unresolvedTypes) addUnresolved(u);
      };
      auto const addFunc = [&] (SString f) {
        for (auto const u : meta.func.at(f).unresolvedTypes) addUnresolved(u);
      };

      for (auto const c : work.classes) addClass(c);
      for (auto const d : work.deps)    addClass(d);
      for (auto const f : work.funcs)   addFunc(f);

      std::sort(begin(typeMappings), end(typeMappings));
      std::sort(begin(missingTypes), end(missingTypes), string_data_lt_type{});
    }

    auto [typeMappingsRef, missingTypesRef, config] = co_await
      coro::collectAll(
        index.client->store(std::move(typeMappings)),
        index.client->store(std::move(missingTypes)),
        index.configRef->getCopy()
      );

    auto results = co_await
      index.client->exec(
        s_flattenJob,
        std::move(config),
        singleton_vec(
          std::make_tuple(
            std::move(classes),
            std::move(deps),
            std::move(bytecode),
            std::move(funcs),
            std::move(uninstantiableRefs),
            std::move(typeMappingsRef),
            std::move(missingTypesRef)
          )
        ),
        std::move(metadata)
      );
    // Every flattening job is a single work-unit, so we should only
    // ever get one result for each one.
    assertx(results.size() == 1);
    auto& [clsRefs, bytecodeRefs, cinfoRefs, funcRefs,
           finfoRefs, methodRefs, classMetaRef] = results[0];
    assertx(clsRefs.size() == cinfoRefs.size());
    assertx(clsRefs.size() == bytecodeRefs.size());
    assertx(funcRefs.size() == work.funcs.size());
    assertx(funcRefs.size() == finfoRefs.size());

    // We need the output metadata, but everything else stays
    // uploaded.
    auto clsMeta = co_await index.client->load(std::move(classMetaRef));
    assertx(methodRefs.size() == clsMeta.uninstantiable.size());

    // Create the updates by combining the job output (but skipping
    // over uninstantiable classes).
    UpdateVec updates;
    updates.reserve(work.classes.size() * 3);

    size_t outputIdx = 0;
    size_t parentIdx = 0;
    size_t methodIdx = 0;
    for (auto const name : work.classes) {
      if (clsMeta.uninstantiable.contains(name)) {
        assertx(methodIdx < methodRefs.size());
        updates.emplace_back(
          MethodUpdate{ name, std::move(methodRefs[methodIdx]) }
        );
        ++methodIdx;
        continue;
      }
      assertx(outputIdx < clsRefs.size());
      assertx(outputIdx < clsMeta.classTypeUses.size());
      assertx(outputIdx < clsMeta.extraMethods.size());
      assertx(outputIdx < clsMeta.flattenedInto.size());
      assertx(outputIdx < clsMeta.newClassPredeps.size());

      auto const& flattenMeta = meta.cls.at(name);
      updates.emplace_back(
        ClassUpdate{
          name,
          std::move(clsRefs[outputIdx]),
          std::move(bytecodeRefs[outputIdx]),
          std::move(cinfoRefs[outputIdx]),
          std::move(clsMeta.classTypeUses[outputIdx]),
          std::move(clsMeta.extraMethods[outputIdx]),
          std::move(clsMeta.flattenedInto[outputIdx]),
          std::move(clsMeta.newClassPredeps[outputIdx]),
          (bool)clsMeta.interfaces.contains(name),
          (bool)clsMeta.with86init.contains(name)
        }
      );

      // Ignore closures. We don't run the build subclass pass for
      // closures, so we don't need information for them.
      if (!flattenMeta.isClosure) {
        assertx(parentIdx < clsMeta.parents.size());
        auto const& parents = clsMeta.parents[parentIdx].names;
        auto& update = std::get<ClassUpdate>(updates.back());
        update.parents.insert(
          end(update.parents), begin(parents), end(parents)
        );
        ++parentIdx;
      }

      ++outputIdx;
    }
    assertx(outputIdx == clsRefs.size());
    assertx(outputIdx == clsMeta.classTypeUses.size());

    for (auto const& [unit, name, context] : clsMeta.newClosures) {
      updates.emplace_back(ClosureUpdate{ name, context, unit });
    }

    for (auto const name : work.uninstantiable) {
      assertx(clsMeta.uninstantiable.contains(name));
      assertx(methodIdx < methodRefs.size());
      updates.emplace_back(
        MethodUpdate{ name, std::move(methodRefs[methodIdx]) }
      );
      ++methodIdx;
    }
    assertx(methodIdx == methodRefs.size());

    assertx(work.funcs.size() == clsMeta.funcTypeUses.size());
    assertx(work.funcs.size() == clsMeta.newFuncPredeps.size());
    for (size_t i = 0, size = work.funcs.size(); i < size; ++i) {
      updates.emplace_back(
        FuncUpdate{
          work.funcs[i],
          std::move(funcRefs[i]),
          std::move(finfoRefs[i]),
          std::move(clsMeta.funcTypeUses[i]),
          std::move(clsMeta.newFuncPredeps[i])
        }
      );
    }

    if (!clsMeta.originalUnits.empty()) {
      updates.emplace_back(UnitUpdate{ std::move(clsMeta.originalUnits) });
    }

    for (auto const& c : clsMeta.interfaceConflicts) {
      decltype(ifaceConflicts)::accessor acc;
      ifaceConflicts.insert(acc, c.name);
      acc->second.name = c.name;
      acc->second.usage += c.usage;
      acc->second.conflicts.insert(begin(c.conflicts), end(c.conflicts));
    }

    co_return updates;
  };

  // Calculate the grouping of classes into work units for flattening,
  // perform the flattening, and gather all updates from the jobs.
  auto allUpdates = [&] {
    auto assignments = flatten_classes_assign(meta);

    trace_time trace2("flatten classes work", index.sample);
    return coro::blockingWait(coro::collectAllRange(
      from(assignments)
        | move
        | map([&] (FlattenClassesWork w) {
            return co_withExecutor(index.executor->sticky(), run(std::move(w)));
          })
        | as<std::vector>()
    ));
  }();

  // Now take the updates and apply them to the Index tables. This
  // needs to be done in a single threaded context (per data
  // structure). This also gathers up all the fixups needed.

  SubclassMetadata subclassMeta;
  InitTypesMetadata initTypesMeta;

  {
    trace_time trace2("flatten classes update");
    trace2.ignore_client_stats();

    parallel::parallel(
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<ClassUpdate>(&update);
            if (!u) continue;
            index.classRefs.insert_or_assign(
              u->name,
              std::move(u->cls)
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<ClassUpdate>(&update);
            if (!u) continue;
            always_assert(
              index.classInfoRefs.emplace(
                u->name,
                std::move(u->cinfo)
              ).second
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<ClassUpdate>(&update);
            if (!u) continue;
            index.classBytecodeRefs.insert_or_assign(
              u->name,
              std::move(u->bytecode)
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<FuncUpdate>(&update);
            if (!u) continue;
            index.funcRefs.at(u->name) = std::move(u->func);
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<FuncUpdate>(&update);
            if (!u) continue;
            always_assert(
              index.funcInfoRefs.emplace(
                u->name,
                std::move(u->finfo)
              ).second
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            // Keep closure mappings up to date.
            auto u = std::get_if<ClosureUpdate>(&update);
            if (!u) continue;
            initTypesMeta.fixups[u->unit].addClass.emplace_back(u->name);
            assertx(u->context);
            always_assert(
              index.closureToClass.emplace(u->name, u->context).second
            );
            index.classToClosures[u->context].emplace(u->name);
          }
        }
        for (auto& [unit, deletions] : meta.unitDeletions) {
          initTypesMeta.fixups[unit].removeFunc = std::move(deletions);
        }
      },
      [&] {
        // A class which didn't have an 86*init function previously
        // can gain one due to trait flattening. Update that here.
        for (auto const& updates : allUpdates) {
          for (auto const& update : updates) {
            auto u = std::get_if<ClassUpdate>(&update);
            if (!u || !u->has86init) continue;
            index.classesWith86Inits.emplace(u->name);
          }
        }
      },
      [&] {
        // Build metadata for the next build subclass pass.
        auto& all = subclassMeta.all;
        auto& meta = subclassMeta.meta;
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<ClassUpdate>(&update);
            if (!u) continue;

            // We shouldn't have parents for closures because we
            // special case those explicitly.
            if (is_closure_name(u->name) || is_closure_base(u->name)) {
              assertx(u->parents.empty());
              continue;
            }
            // Otherwise build the children lists from the parents.
            all.emplace_back(u->name);
            for (auto const p : u->parents) {
              meta[p].children.emplace_back(u->name);
            }
            auto& parents = meta[u->name].parents;
            assertx(parents.empty());
            parents.insert(
              end(parents),
              begin(u->parents), end(u->parents)
            );
          }
        }

        std::sort(begin(all), end(all), string_data_lt_type{});
        // Make sure there's no duplicates:
        assertx(std::adjacent_find(begin(all), end(all)) == end(all));

        for (size_t i = 0; i < all.size(); ++i) meta[all[i]].idx = i;
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto u = std::get_if<MethodUpdate>(&update);
            if (!u) continue;
            always_assert(
              index.uninstantiableClsMethRefs.emplace(
                u->name,
                std::move(u->methods)
              ).second
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            if (auto const u = std::get_if<ClassUpdate>(&update)) {
              auto& meta = initTypesMeta.classes[u->name];
              assertx(meta.deps.empty());
              meta.deps.insert(begin(u->typeUses), end(u->typeUses));
            } else if (auto const u = std::get_if<FuncUpdate>(&update)) {
              auto& meta = initTypesMeta.funcs[u->name];
              assertx(meta.deps.empty());
              meta.deps.insert(begin(u->typeUses), end(u->typeUses));
            }
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto const u = std::get_if<ClassUpdate>(&update);
            if (!u || u->extraMethods.empty()) continue;
            always_assert(
              index.extraMethods.emplace(
                u->name,
                std::move(u->extraMethods)
              ).second
            );
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            if (auto const u = std::get_if<ClassUpdate>(&update)) {
              auto const unit = index.classToUnit.at(u->name);
              auto& toPredeps = index.unitPredeps.at(unit);
              auto& toCInitPredeps = index.unitCInitPredeps.at(unit);
              toPredeps.insert(begin(u->typeUses), end(u->typeUses));
              toPredeps.insert(
                begin(u->newPredeps.predeps),
                end(u->newPredeps.predeps)
              );
              toCInitPredeps.insert(
                begin(u->newPredeps.cinitPredeps),
                end(u->newPredeps.cinitPredeps)
              );
            } else if (auto const u = std::get_if<FuncUpdate>(&update)) {
              auto const unit = index.funcToUnit.at(u->name);
              auto& toPredeps = index.unitPredeps.at(unit);
              auto& toCInitPredeps = index.unitCInitPredeps.at(unit);
              toPredeps.insert(begin(u->typeUses), end(u->typeUses));
              toPredeps.insert(
                begin(u->newPredeps.predeps),
                end(u->newPredeps.predeps)
              );
              toCInitPredeps.insert(
                begin(u->newPredeps.cinitPredeps),
                end(u->newPredeps.cinitPredeps)
              );
            }
          }
        }

        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto const u = std::get_if<ClassUpdate>(&update);
            if (!u) continue;
            auto const unit = index.classToUnit.at(u->name);
            auto& toPredeps = index.unitPredeps.at(unit);
            auto& toCInitPredeps = index.unitCInitPredeps.at(unit);
            for (auto const t : u->flattenedInto) {
              auto const fromUnit = index.classToUnit.at(t);
              auto const& fromPredeps = index.unitPredeps.at(fromUnit);
              auto const& fromCInitPredeps =
                index.unitCInitPredeps.at(fromUnit);
              toPredeps.insert(begin(fromPredeps), end(fromPredeps));
              toCInitPredeps.insert(
                begin(fromCInitPredeps),
                end(fromCInitPredeps)
              );
            }
          }
        }
      },
      [&] {
        for (auto& updates : allUpdates) {
          for (auto& update : updates) {
            auto const u = std::get_if<UnitUpdate>(&update);
            if (!u) continue;
            for (auto const& [n, orig] : u->originalUnits) {
              index.unitToOriginalUnits[n].insert(begin(orig), end(orig));
            }
          }
        }
      }
    );
  }

  return std::make_tuple(
    std::move(subclassMeta),
    std::move(initTypesMeta),
    [&] {
      std::vector<InterfaceConflicts> out;
      out.reserve(ifaceConflicts.size());
      for (auto& [_, c] : ifaceConflicts) out.emplace_back(std::move(c));
      return out;
    }()
  );
}

//////////////////////////////////////////////////////////////////////
// Subclass list

/*
 * Subclass lists are built in a similar manner as flattening classes,
 * except the order is reversed.
 *
 * However, there is one complication: the transitive children of each
 * class can be huge. In fact, for large hierarchies, they can easily
 * be too large to (efficiently) handle in a single job.
 *
 * Rather than (always) processing everything in a single pass, we
 * might need to use multiple passes to keep the fan-in down. When
 * calculating the work buckets, we keep the size of each bucket into
 * account and don't allow any bucket to grow too large. If it does,
 * we'll just process that bucket, and process any dependencies in the
 * next pass.
 *
 * This isn't sufficient. A single class have (far) more direct
 * children than we want in a single bucket. Multiple passes don't
 * help here because there's no intermediate classes to use as an
 * output. To fix this, we insert "splits", which serve to "summarize"
 * some subset of a class' direct children.
 *
 * For example, suppose a class has 10k direct children, and our
 * maximum bucket size is 1k. On the first pass we'll process all of
 * the children in ~10 different jobs, each one processing 1k of the
 * children, and producing a single split node. The second pass will
 * actually process the class and take all of the splits as inputs
 * (not the actual children). The inputs to the job has been reduced
 * from 10k to 10. This is a simplification. In reality a job can
 * produce multiple splits, and inputs can be a mix of splits and
 * actual classes. In extreme cases, you might need multiple rounds of
 * splits before processing the class.
 *
 * There is one other difference between this and the flatten classes
 * pass. Unlike in flatten classes, every class (except leafs) are
 * "roots" here. We do not promote any dependencies. This causes more
 * work overall, but it lets us process more classes in parallel.
 */

/*
 * Extern-worker job to build ClassInfo2 subclass lists, and calculate
 * various properties on the ClassInfo2 from it.
 */
struct BuildSubclassListJob {
  static std::string name() { return "hhbbc-build-subclass"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
    ClassGraph::init();
  }
  static void fini() { ClassGraph::destroy(); }

  // Aggregated data for some group of classes. The data can either
  // come from a split node, or inferred from a group of classes.
  struct Data {
    // Information about all of the methods with a particular name
    // between all of the classes in this Data.
    struct MethInfo {
      // Methods which are present on at least one regular class.
      MethRefSet regularMeths;
      // Methods which are only present on non-regular classes, but is
      // private on at least one class. These are sometimes treated
      // like a regular class.
      MethRefSet nonRegularPrivateMeths;
      // Methods which are only present on non-regular classes (and
      // never private). These three sets are always disjoint.
      MethRefSet nonRegularMeths;

      Optional<FuncFamily2::StaticInfo> allStatic;
      Optional<FuncFamily2::StaticInfo> regularStatic;

      // Whether all classes in this Data have a method with this
      // name.
      bool complete{true};
      // Whether all regular classes in this Data have a method with
      // this name.
      bool regularComplete{true};
      // Whether any of the methods has a private ancestor.
      bool privateAncestor{false};

      template <typename SerDe> void serde(SerDe& sd) {
        sd(regularMeths, std::less<MethRef>{})
          (nonRegularPrivateMeths, std::less<MethRef>{})
          (nonRegularMeths, std::less<MethRef>{})
          (allStatic)
          (regularStatic)
          (complete)
          (regularComplete)
          (privateAncestor)
          ;
      }
    };
    SStringToOneT<MethInfo> methods;

    // The name of properties which might have null values even if the
    // type-constraint doesn't allow it (due to system provided
    // initial values).
    SStringSet propsWithImplicitNullable;

    // For a given property name, all subclasses which have a
    // declaration for the item. If an entry is present, but the set
    // is empty, it means the property is declared in a subclass, but
    // there's no declaration in this class or parents. We don't track
    // the subclasses precisely in that case because the amount of
    // properties can be huge.
    SStringToOneT<TSStringSet> propDeclInfo;

    SStringToOneT<ClsCnsSubInfo> cnsSubInfo;

    // The classes for whom isMocked would be true due to one of the
    // classes making up this Data. The classes in this set may not
    // necessarily be also part of this Data.
    TSStringSet mockedClasses;

    bool hasConstProp{false};
    bool hasReifiedGeneric{false};

    bool isSubMocked{false};

    // The meaning of these differ depending on whether the ClassInfo
    // contains just it's info, or all of it's subclass info.
    bool hasRegularClass{false};
    bool hasRegularClassFull{false};

    template <typename SerDe> void serde(SerDe& sd) {
      sd(methods, string_data_lt{})
        (propsWithImplicitNullable, string_data_lt{})
        (propDeclInfo, string_data_lt{}, string_data_lt_type{})
        (cnsSubInfo, string_data_lt{})
        (mockedClasses, string_data_lt_type{})
        (hasConstProp)
        (hasReifiedGeneric)
        (isSubMocked)
        (hasRegularClass)
        (hasRegularClassFull)
        ;
    }
  };

  // Split node. Used to wrap a Data when summarizing some subset of a
  // class' children.
  struct Split {
    Split() = default;
    Split(SString name, SString cls) : name{name}, cls{cls} {}

    SString name;
    SString cls;
    CompactVector<SString> children;
    CompactVector<ClassGraph> classGraphs;
    Data data;

    template <typename SerDe> void serde(SerDe& sd) {
      ScopedStringDataIndexer _;
      ClassGraph::ScopedSerdeState _2;
      sd(name)
        (cls)
        (children)
        (classGraphs, nullptr)
        (data)
        ;
    }
  };

  // Mark a dependency on a class to a split node. Since the splits
  // are not actually part of the hierarchy, the relationship between
  // classes and splits cannot be inferred otherwise.
  struct EdgeToSplit {
    SString cls;
    SString split;
    template <typename SerDe> void serde(SerDe& sd) {
      sd(cls)
        (split)
        ;
    }
  };

  // Job output meant to be downloaded and drive the next round.
  struct OutputMeta {
    // For every input ClassInfo, the set of func families present in
    // that ClassInfo's method family table. If the ClassInfo is used
    // as a dep later, these func families need to be provided as
    // well.
    std::vector<hphp_fast_set<FuncFamily2::Id>> funcFamilyDeps;
    // The ids of new (not provided as an input) func families
    // produced. The ids are grouped together to become
    // FuncFamilyGroups.
    std::vector<std::vector<FuncFamily2::Id>> newFuncFamilyIds;
    // Func family entries corresponding to all methods with a
    // particular name encountered in this job. Multiple jobs will
    // generally produce func family entries for the same name, so
    // they must be aggregated together afterwards.
    std::vector<std::pair<SString, FuncFamilyEntry>> nameOnly;
    std::vector<std::vector<SString>> regOnlyEquivCandidates;

    // For every output class, the set of classes which that class has
    // inherited class constants from.
    TSStringToOneT<TSStringSet> cnsBases;

    template <typename SerDe> void serde(SerDe& sd) {
      ScopedStringDataIndexer _;
      sd(funcFamilyDeps, std::less<FuncFamily2::Id>{})
        (newFuncFamilyIds)
        (nameOnly)
        (regOnlyEquivCandidates)
        (cnsBases, string_data_lt_type{}, string_data_lt_type{})
        ;
    }
  };
  using Output = Multi<
    Variadic<std::unique_ptr<ClassInfo2>>,
    Variadic<std::unique_ptr<Split>>,
    Variadic<std::unique_ptr<php::Class>>,
    Variadic<FuncFamilyGroup>,
    Variadic<std::unique_ptr<ClassInfo2>>,
    OutputMeta
  >;

  // Each job takes the list of classes and splits which should be
  // produced, dependency classes and splits (which are not updated),
  // edges between classes and splits, and func families (needed by
  // dependency classes). Leafs are like deps, except they'll be
  // considered as part of calculating the name-only func families
  // because its possible for them to be disjoint from classes.
  // (normal deps are not considered as their data is guaranteed to
  // be included by a class).
  static Output
  run(Variadic<std::unique_ptr<ClassInfo2>> classes,
      Variadic<std::unique_ptr<ClassInfo2>> deps,
      Variadic<std::unique_ptr<ClassInfo2>> leafs,
      Variadic<std::unique_ptr<Split>> splits,
      Variadic<std::unique_ptr<Split>> splitDeps,
      Variadic<std::unique_ptr<php::Class>> phpClasses,
      Variadic<EdgeToSplit> edges,
      Variadic<FuncFamilyGroup> funcFamilies) {
    // Store mappings of names to classes and edges.
    LocalIndex index;

    if constexpr (debug) {
      for (auto const& cinfo : classes.vals) {
        always_assert(!cinfo->classGraph.isMissing());
        always_assert(!cinfo->classGraph.hasCompleteChildren());
        always_assert(!cinfo->classGraph.isConservative());
      }
      for (auto const& cinfo : leafs.vals) {
        always_assert(!cinfo->classGraph.isMissing());
        always_assert(!cinfo->classGraph.hasCompleteChildren());
        always_assert(!cinfo->classGraph.isConservative());
      }
      for (auto const& cinfo : deps.vals) {
        always_assert(!cinfo->classGraph.isMissing());
      }
      for (auto const& split : splits.vals) {
        for (auto const child : split->classGraphs) {
          always_assert(!child.isMissing());
          always_assert(!child.hasCompleteChildren());
          always_assert(!child.isConservative());
        }
      }
      for (auto const& split : splitDeps.vals) {
        for (auto const child : split->classGraphs) {
          always_assert(!child.isMissing());
          always_assert(child.hasCompleteChildren() ||
                        child.isConservative());
        }
      }
    }

    for (auto& cinfo : classes.vals) {
      assertx(!is_closure_name(cinfo->name));
      always_assert(
        index.classInfos.emplace(cinfo->name, cinfo.get()).second
      );
      index.top.emplace(cinfo->name);
    }
    for (auto& cinfo : deps.vals) {
      assertx(!is_closure_name(cinfo->name));
      always_assert(
        index.classInfos.emplace(cinfo->name, cinfo.get()).second
      );
    }
    for (auto& cinfo : leafs.vals) {
      assertx(!is_closure_name(cinfo->name));
      always_assert(
        index.classInfos.emplace(cinfo->name, cinfo.get()).second
      );
    }
    for (auto& split : splits.vals) {
      always_assert(
        index.splits.emplace(split->name, split.get()).second
      );
      index.top.emplace(split->name);
    }
    for (auto& split : splitDeps.vals) {
      assertx(split->children.empty());
      always_assert(
        index.splits.emplace(split->name, split.get()).second
      );
    }
    for (auto& cls : phpClasses.vals) {
      assertx(!is_closure(*cls));
      always_assert(
        index.classes.emplace(cls->name, cls.get()).second
      );
    }
    for (auto& group : funcFamilies.vals) {
      for (auto& ff : group.m_ffs) {
        auto const id = ff->m_id;
        // We could have multiple groups which contain the same
        // FuncFamily, so don't assert uniqueness here. We'll just
        // take the first one we see (they should all be equivalent).
        index.funcFamilies.emplace(id, std::move(ff));
      }
    }

    index.aggregateData.reserve(index.top.size());

    OutputMeta meta;

    // Mark all of the classes (including leafs) as being complete
    // since their subclass lists are correct.
    for (auto& cinfo : leafs.vals) {
      if (cinfo->classGraph.hasCompleteChildren()) continue;
      cinfo->classGraph.setComplete();
    }
    for (auto& cinfo : classes.vals) {
      if (cinfo->classGraph.hasCompleteChildren() ||
          cinfo->classGraph.isConservative()) {
        continue;
      }
      cinfo->classGraph.setComplete();
    }
    for (auto& cinfo : deps.vals) {
      if (cinfo->classGraph.hasCompleteChildren() ||
          cinfo->classGraph.isConservative()) {
        continue;
      }
      cinfo->classGraph.setComplete();
    }
    for (auto& split : splits.vals) {
      for (auto child : split->classGraphs) {
        if (child.hasCompleteChildren() ||
            child.isConservative()) {
          continue;
        }
        child.setComplete();
      }
    }

    // Store the regular-only equivalent classes in the output
    // metadata. This will be used in the init-types pass.
    meta.regOnlyEquivCandidates.reserve(classes.vals.size());
    for (auto& cinfo : classes.vals) {
      meta.regOnlyEquivCandidates.emplace_back();
      auto& candidates = meta.regOnlyEquivCandidates.back();
      for (auto const g : cinfo->classGraph.candidateRegOnlyEquivs()) {
        candidates.emplace_back(g.name());
      }
    }

    // If there's no classes or splits, this job is doing nothing but
    // calculating name only func family entries (so should have at
    // least one leaf).
    if (!index.top.empty()) {
      build_children(index, edges.vals);
      process_roots(index, classes.vals, splits.vals);
    } else {
      assertx(classes.vals.empty());
      assertx(splits.vals.empty());
      assertx(index.splits.empty());
      assertx(index.funcFamilies.empty());
      assertx(!leafs.vals.empty());
      assertx(!index.classInfos.empty());
    }

    meta.nameOnly =
      make_name_only_method_entries(index, classes.vals, leafs.vals);

    // Record dependencies for each input class. A func family is a
    // dependency of the class if it appears in the method families
    // table.
    meta.funcFamilyDeps.reserve(classes.vals.size());
    for (auto const& cinfo : classes.vals) {
      meta.funcFamilyDeps.emplace_back();
      auto& deps = meta.funcFamilyDeps.back();
      for (auto const& [_, entry] : cinfo->methodFamilies) {
        match(
          entry.m_meths,
          [&] (const FuncFamilyEntry::BothFF& e)      { deps.emplace(e.m_ff); },
          [&] (const FuncFamilyEntry::FFAndSingle& e) { deps.emplace(e.m_ff); },
          [&] (const FuncFamilyEntry::FFAndNone& e)   { deps.emplace(e.m_ff); },
          [&] (const FuncFamilyEntry::BothSingle&)    {},
          [&] (const FuncFamilyEntry::SingleAndNone&) {},
          [&] (const FuncFamilyEntry::None&)          {}
        );
      }
    }

    Variadic<FuncFamilyGroup> funcFamilyGroups;
    group_func_families(index, funcFamilyGroups.vals, meta.newFuncFamilyIds);

    auto const addCnsBase = [&] (const ClassInfo2& cinfo) {
      auto& bases = meta.cnsBases[cinfo.name];
      for (auto const& [_, idx] : cinfo.clsConstants) {
        if (!cinfo.name->tsame(idx.idx.cls)) bases.emplace(idx.idx.cls);
      }
    };
    for (auto const& cinfo : classes.vals) addCnsBase(*cinfo);
    for (auto const& cinfo : leafs.vals)   addCnsBase(*cinfo);

    // We only need to provide php::Class which correspond to a class
    // which wasn't a dep.
    phpClasses.vals.erase(
      std::remove_if(
        begin(phpClasses.vals),
        end(phpClasses.vals),
        [&] (const std::unique_ptr<php::Class>& c) {
          return !index.top.contains(c->name);
        }
      ),
      end(phpClasses.vals)
    );

    return std::make_tuple(
      std::move(classes),
      std::move(splits),
      std::move(phpClasses),
      std::move(funcFamilyGroups),
      std::move(leafs),
      std::move(meta)
    );
  }

protected:

  /*
   * MethInfo caching
   *
   * When building a func family, MethRefSets must be sorted, then
   * hashed in order to generate the unique id. Once we do so, we can
   * then check if that func family already exists. For large func
   * families, this can very expensive and we might have to do this
   * (wasted) work multiple times.
   *
   * To avoid this, we add a cache before the sorting/hashing
   * step. Instead of using a func family id (which is the expensive
   * thing to generate), the cache is keyed by the set of methods
   * directly. A commutative hash is used so that we don't actually
   * have to sort the MethRefSets, and equality is just equality of
   * the MethRefSets. Moreover, we make use of hetereogenous lookup to
   * avoid having to copy any MethRefSets (again they can be large)
   * when doing the lookup.
   */

  // What is actually stored in the cache. Keeps a copy of the
  // MethRefSets.
  struct MethInfoTuple {
    MethRefSet regular;
    MethRefSet nonRegularPrivate;
    MethRefSet nonRegular;
  };
  // Used for lookups. Just has pointers to the MethRefSets, so we
  // don't have to do any copying for the lookup.
  struct MethInfoTupleProxy {
    const MethRefSet* regular;
    const MethRefSet* nonRegularPrivate;
    const MethRefSet* nonRegular;
  };

  struct MethInfoTupleHasher {
    using is_transparent = void;

    size_t operator()(const MethInfoTuple& t) const {
      auto const h1 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(t.regular), end(t.regular)
      );
      auto const h2 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(t.nonRegularPrivate), end(t.nonRegularPrivate)
      );
      auto const h3 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(t.nonRegular), end(t.nonRegular)
      );
      return folly::hash::hash_combine(h1, h2, h3);
    }
    size_t operator()(const MethInfoTupleProxy& t) const {
      auto const h1 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(*t.regular), end(*t.regular)
      );
      auto const h2 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(*t.nonRegularPrivate), end(*t.nonRegularPrivate)
      );
      auto const h3 = folly::hash::commutative_hash_combine_range_generic(
        0, MethRef::Hash{}, begin(*t.nonRegular), end(*t.nonRegular)
      );
      return folly::hash::hash_combine(h1, h2, h3);
    }
  };
  struct MethInfoTupleEquals {
    using is_transparent = void;

    bool operator()(const MethInfoTuple& t1, const MethInfoTuple& t2) const {
      return
        t1.regular == t2.regular &&
        t1.nonRegularPrivate == t2.nonRegularPrivate &&
        t1.nonRegular == t2.nonRegular;
    }
    bool operator()(const MethInfoTupleProxy& t1,
                    const MethInfoTuple& t2) const {
      return
        *t1.regular == t2.regular &&
        *t1.nonRegularPrivate == t2.nonRegularPrivate &&
        *t1.nonRegular == t2.nonRegular;
    }
  };

  struct LocalIndex {
    // All ClassInfos, whether inputs or dependencies.
    TSStringToOneT<ClassInfo2*> classInfos;
    // All splits, whether inputs or dependencies.
    TSStringToOneT<Split*> splits;
    // All php::Class, whether inputs or dependencies.
    TSStringToOneT<php::Class*> classes;

    // ClassInfos and splits which are inputs (IE, we want to
    // calculate data for).
    TSStringSet top;

    // Aggregated data for an input
    TSStringToOneT<Data> aggregateData;

    // Mapping of input ClassInfos/splits to all of their subclasses
    // present in this Job. Some of the children may be splits, which
    // means some subset of the children were processed in another
    // Job.
    TSStringToOneT<std::vector<SString>> children;

    // The leafs in this job. This isn't necessarily an actual leaf,
    // but one whose children haven't been provided in this job
    // (because they've already been processed). Classes which are
    // leafs have information in their ClassInfo2 which reflect all of
    // their subclasses, otherwise just their own information.
    TSStringSet leafs;

    // All func families available in this Job, either from inputs, or
    // created during processing.
    hphp_fast_map<FuncFamily2::Id, std::unique_ptr<FuncFamily2>> funcFamilies;

    // Above mentioned func family cache. If an entry is present here,
    // we know the func family already exists and don't need to do
    // expensive sorting/hashing.
    hphp_fast_map<
      MethInfoTuple,
      FuncFamily2::Id,
      MethInfoTupleHasher,
      MethInfoTupleEquals
    > funcFamilyCache;

    // funcFamilies contains all func families. If a func family is
    // created during processing, it will be inserted here (used to
    // determine outputs).
    std::vector<FuncFamily2::Id> newFuncFamilies;

    php::Class& cls(SString name) {
      auto const it = classes.find(name);
      always_assert(it != end(classes));
      return *it->second;
    }
  };

  // Take all of the func families produced by this job and group them
  // together into FuncFamilyGroups. We produce both the
  // FuncFamilyGroups themselves, but also the associated ids in each
  // group (which will be output as metadata).
  static void group_func_families(
    LocalIndex& index,
    std::vector<FuncFamilyGroup>& groups,
    std::vector<std::vector<FuncFamily2::Id>>& ids
  ) {
    constexpr size_t kGroupSize = 5000;

    // The grouping algorithm is very simple. First we sort all of the
    // func families by size. We then just group adjacent families
    // until their total size exceeds some threshold. Once it does, we
    // start a new group.
    std::sort(
      begin(index.newFuncFamilies),
      end(index.newFuncFamilies),
      [&] (const FuncFamily2::Id& id1, const FuncFamily2::Id& id2) {
        auto const& ff1 = index.funcFamilies.at(id1);
        auto const& ff2 = index.funcFamilies.at(id2);
        auto const size1 =
          ff1->m_regular.size() +
          ff1->m_nonRegularPrivate.size() +
          ff1->m_nonRegular.size();
        auto const size2 =
          ff2->m_regular.size() +
          ff2->m_nonRegularPrivate.size() +
          ff2->m_nonRegular.size();
        if (size1 != size2) return size1 < size2;
        return id1 < id2;
      }
    );

    size_t current = 0;
    for (auto const& id : index.newFuncFamilies) {
      auto& ff = index.funcFamilies.at(id);
      auto const size =
        ff->m_regular.size() +
        ff->m_nonRegularPrivate.size() +
        ff->m_nonRegular.size();
      if (groups.empty() || current > kGroupSize) {
        groups.emplace_back();
        ids.emplace_back();
        current = 0;
      }
      groups.back().m_ffs.emplace_back(std::move(ff));
      ids.back().emplace_back(id);
      current += size;
    }
  }

  // Produce a set of name-only func families from the given set of
  // roots and leafs. It is assumed that any roots have already been
  // processed by process_roots, so that they'll have any appropriate
  // method families on them. Only entries which are "first name" are
  // processed.
  static std::vector<std::pair<SString, FuncFamilyEntry>>
  make_name_only_method_entries(
    LocalIndex& index,
    const std::vector<std::unique_ptr<ClassInfo2>>& roots,
    const std::vector<std::unique_ptr<ClassInfo2>>& leafs
  ) {
    SStringToOneT<Data::MethInfo> infos;

    // Use the already calculated method family and merge
    // it's contents into what we already have.
    auto const process = [&] (const ClassInfo2* cinfo,
                              SString name) {
      auto const it = cinfo->methodFamilies.find(name);
      always_assert(it != end(cinfo->methodFamilies));
      auto entryInfo = meth_info_from_func_family_entry(index, it->second);

      auto& info = infos[name];
      info.complete = false;
      info.regularComplete = false;

      for (auto const& meth : entryInfo.regularMeths) {
        if (info.regularMeths.contains(meth)) continue;
        info.regularMeths.emplace(meth);
        info.nonRegularPrivateMeths.erase(meth);
        info.nonRegularMeths.erase(meth);
      }
      for (auto const& meth : entryInfo.nonRegularPrivateMeths) {
        if (info.regularMeths.contains(meth) ||
            info.nonRegularPrivateMeths.contains(meth)) {
          continue;
        }
        info.nonRegularPrivateMeths.emplace(meth);
        info.nonRegularMeths.erase(meth);
      }
      for (auto const& meth : entryInfo.nonRegularMeths) {
        if (info.regularMeths.contains(meth) ||
            info.nonRegularPrivateMeths.contains(meth) ||
            info.nonRegularMeths.contains(meth)) {
          continue;
        }
        info.nonRegularMeths.emplace(meth);
      }

      // Merge any StaticInfo entries we have for this method.
      if (entryInfo.allStatic) {
        if (!info.allStatic) {
          info.allStatic = std::move(*entryInfo.allStatic);
        } else {
          *info.allStatic |= *entryInfo.allStatic;
        }
      }
      if (entryInfo.regularStatic) {
        if (!info.regularStatic) {
          info.regularStatic = std::move(*entryInfo.regularStatic);
        } else {
          *info.regularStatic |= *entryInfo.regularStatic;
        }
      }
    };

    // First process the roots. These methods might be overridden or
    // not.
    for (auto const& cinfo : roots) {
      for (auto const& [name, mte] : cinfo->methods) {
        if (!mte.firstName()) continue;
        if (!has_name_only_func_family(name)) continue;
        process(cinfo.get(), name);
      }
    }

    // Leafs are by definition always AttrNoOverride.
    for (auto const& cinfo : leafs) {
      for (auto const& [name, mte] : cinfo->methods) {
        if (!mte.firstName()) continue;
        if (!has_name_only_func_family(name)) continue;
        process(cinfo.get(), name);
      }
    }

    // Make the MethInfo order deterministic
    std::vector<SString> sorted;
    sorted.reserve(infos.size());
    for (auto const& [name, _] : infos) sorted.emplace_back(name);
    std::sort(begin(sorted), end(sorted), string_data_lt{});

    std::vector<std::pair<SString, FuncFamilyEntry>> entries;
    entries.reserve(infos.size());

    // Turn the MethInfos into FuncFamilyEntries
    for (auto const name : sorted) {
      auto& info = infos.at(name);
      entries.emplace_back(
        name,
        make_method_family_entry(index, name, std::move(info))
      );
    }
    return entries;
  }

  // From the information present in the inputs, calculate a mapping
  // of classes and splits to their children (which can be other
  // classes or split nodes). This is not just direct children, but
  // all transitive subclasses.
  static void build_children(LocalIndex& index,
                             const std::vector<EdgeToSplit>& edges) {
    TSStringToOneT<TSStringSet> children;
    // First record direct children. This can be inferred from the
    // parents of all present ClassInfos:

    // Everything starts out as a leaf.
    index.leafs.reserve(index.classInfos.size());
    for (auto const [name, _] : index.classInfos) {
      index.leafs.emplace(name);
    }

    auto const onParent = [&] (SString parent, const ClassInfo2* child) {
      // Due to how work is divided, a class might have parents not
      // present in this job. Ignore those.
      if (!index.classInfos.contains(parent)) return;
      children[parent].emplace(child->name);
      // If you're a parent, you're not a leaf.
      index.leafs.erase(parent);
    };

    for (auto const [name, cinfo] : index.classInfos) {
      if (cinfo->parent) onParent(cinfo->parent, cinfo);
      for (auto const iface : cinfo->classGraph.declInterfaces()) {
        onParent(iface.name(), cinfo);
      }
      for (auto const trait : cinfo->classGraph.usedTraits()) {
        onParent(trait.name(), cinfo);
      }
    }

    // Use the edges provided to the Job to know the mapping from
    // ClassInfo to split (it cannot be inferred otherwise).
    for (auto const& edge : edges) {
      SCOPE_ASSERT_DETAIL("Edge not present in job") {
        return folly::sformat("{} -> {}", edge.cls, edge.split);
      };
      assertx(index.classInfos.contains(edge.cls));
      assertx(index.splits.contains(edge.split));
      children[edge.cls].emplace(edge.split);
    }

    // Every "top" ClassInfo also has itself as a subclass (this
    // matches the semantics of the subclass list and simplifies the
    // processing).
    for (auto const name : index.top) {
      if (auto const split = folly::get_default(index.splits, name)) {
        // Copy the children list out of the split and add it to the
        // map.
        auto& c = children[name];
        for (auto const child : split->children) {
          assertx(index.classInfos.contains(child) ||
                  index.splits.contains(child));
          c.emplace(child);
        }
      }
    }

    // Calculate the indegree for all children. The indegree for a given node
    // differs depending on the top used, so these are calculated separately.
    auto const getIndegree = [&](SString root) {
      TSStringSet visited;
      TSStringSet toExplore{root};
      TSStringSet toExploreNext;
      TSStringToOneT<uint32_t> indegree;

      while (!toExplore.empty()) {
        toExploreNext.clear();
        for (auto const child : toExplore) {
          if (visited.contains(child)) continue;
          visited.emplace(child);
          auto const it = children.find(child);
          // May not exist in children if processed in earlier round.
          if (it == end(children)) continue;
          for (auto const c : it->second) {
            indegree[c]++;
            toExploreNext.emplace(c);
          }
        }
        std::swap(toExplore, toExploreNext);
      }
      return indegree;
    };

    // Topological sort the transitive children for each node.
    for (auto& [name, _] : children) {
      auto indegree = getIndegree(name);
      std::vector<SString> sorted{name};

      int sortedBegin = 0;
      int sortedEnd = sorted.size();

      while (sortedBegin != sortedEnd) {
        for (int i = sortedBegin; i < sortedEnd; i++) {
          auto const cls = sorted[i];
          auto const it = children.find(cls);
          if (it == end(children)) continue;
          std::vector<SString> clsChildren{begin(it->second), end(it->second)};
          std::sort(
            begin(clsChildren),
            end(clsChildren),
            string_data_lt_type{}
          );
          for (auto const c : clsChildren) {
            indegree[c]--;
            if (indegree[c] == 0) sorted.emplace_back(c);
          }
        }
        sortedBegin = sortedEnd;
        sortedEnd = sorted.size();
      }
      assertx(indegree.size() + 1 == sorted.size());
      index.children[name] = std::move(sorted);
    }
  }

  static FuncFamily2::StaticInfo static_info_from_meth_meta(
    const FuncFamilyEntry::MethMetadata& meta
  ) {
    FuncFamily2::StaticInfo info;
    info.m_numInOut = meta.m_numInOut;
    info.m_requiredCoeffects = meta.m_requiredCoeffects;
    info.m_coeffectRules = meta.m_coeffectRules;
    info.m_paramPreps = meta.m_prepKinds;
    info.m_minNonVariadicParams = info.m_maxNonVariadicParams =
      meta.m_nonVariadicParams;
    info.m_isReadonlyReturn = yesOrNo(meta.m_isReadonlyReturn);
    info.m_isReadonlyThis = yesOrNo(meta.m_isReadonlyThis);
    info.m_supportsAER = yesOrNo(meta.m_supportsAER);
    info.m_maybeReified = meta.m_isReified;
    info.m_maybeCaresAboutDynCalls = meta.m_caresAboutDyncalls;
    info.m_maybeBuiltin = meta.m_builtin;
    return info;
  }

  // Turn a FuncFamilyEntry into an equivalent Data::MethInfo.
  static Data::MethInfo
  meth_info_from_func_family_entry(LocalIndex& index,
                                   const FuncFamilyEntry& entry) {
    Data::MethInfo info;
    info.complete = !entry.m_allIncomplete;
    info.regularComplete = !entry.m_regularIncomplete;
    info.privateAncestor = entry.m_privateAncestor;

    auto const getFF = [&] (const FuncFamily2::Id& id)
      -> const FuncFamily2& {
      auto const it = index.funcFamilies.find(id);
      always_assert_flog(
        it != end(index.funcFamilies),
        "Tried to access non-existent func-family '{}'",
        id.toString()
      );
      return *it->second;
    };

    match(
      entry.m_meths,
      [&] (const FuncFamilyEntry::BothFF& e) {
        auto const& ff = getFF(e.m_ff);
        info.regularMeths.insert(
          begin(ff.m_regular),
          end(ff.m_regular)
        );
        info.nonRegularPrivateMeths.insert(
          begin(ff.m_nonRegularPrivate),
          end(ff.m_nonRegularPrivate)
        );
        info.nonRegularMeths.insert(
          begin(ff.m_nonRegular),
          end(ff.m_nonRegular)
        );
        assertx(ff.m_allStatic);
        assertx(ff.m_regularStatic);
        info.allStatic = ff.m_allStatic;
        info.regularStatic = ff.m_regularStatic;
      },
      [&] (const FuncFamilyEntry::FFAndSingle& e) {
        auto const& ff = getFF(e.m_ff);
        info.nonRegularMeths.insert(
          begin(ff.m_nonRegular),
          end(ff.m_nonRegular)
        );
        if (e.m_nonRegularPrivate) {
          assertx(ff.m_nonRegularPrivate.size() == 1);
          assertx(ff.m_nonRegularPrivate[0] == e.m_regular);
          info.nonRegularPrivateMeths.emplace(e.m_regular);
        } else {
          assertx(ff.m_regular.size() == 1);
          assertx(ff.m_regular[0] == e.m_regular);
          info.regularMeths.emplace(e.m_regular);
        }
        assertx(ff.m_allStatic);
        assertx(ff.m_regularStatic);
        info.allStatic = ff.m_allStatic;
        info.regularStatic = ff.m_regularStatic;
      },
      [&] (const FuncFamilyEntry::FFAndNone& e) {
        auto const& ff = getFF(e.m_ff);
        assertx(ff.m_regular.empty());
        info.nonRegularMeths.insert(
          begin(ff.m_nonRegular),
          end(ff.m_nonRegular)
        );
        assertx(ff.m_allStatic);
        assertx(!ff.m_regularStatic);
        info.allStatic = ff.m_allStatic;
      },
      [&] (const FuncFamilyEntry::BothSingle& e) {
        if (e.m_nonRegularPrivate) {
          info.nonRegularPrivateMeths.emplace(e.m_all);
        } else {
          info.regularMeths.emplace(e.m_all);
        }
        info.allStatic = info.regularStatic =
          static_info_from_meth_meta(e.m_meta);
      },
      [&] (const FuncFamilyEntry::SingleAndNone& e) {
        info.nonRegularMeths.emplace(e.m_all);
        info.allStatic = static_info_from_meth_meta(e.m_meta);
      },
      [&] (const FuncFamilyEntry::None&) {
        assertx(!info.complete);
      }
    );

    return info;
  }

  // Create a Data representing the single ClassInfo or split with the
  // name "clsname".
  static Data build_data(LocalIndex& index, SString clsname) {
    // Does this name represent a class?
    if (auto const cinfo = folly::get_default(index.classInfos, clsname)) {
      // It's a class. We need to build a Data from what's in the
      // ClassInfo. If the ClassInfo hasn't been processed already
      // (it's a leaf or its the first round), the data will reflect
      // just that class. However if the ClassInfo has been processed
      // (it's a dependencies and it's past the first round), it will
      // reflect any subclasses of that ClassInfo as well.
      Data data;

      // Use the method family table to build initial MethInfos (if
      // the ClassInfo hasn't been processed this will be empty).
      for (auto const& [name, entry] : cinfo->methodFamilies) {
        data.methods.emplace(
          name,
          meth_info_from_func_family_entry(index, entry)
        );
      }

      auto const& cls = index.cls(cinfo->name);

      if constexpr (debug) {
        for (auto const& [name, mte] : cinfo->methods) {
          if (is_special_method_name(name)) continue;

          // Every method should have a methodFamilies entry. If this
          // method is AttrNoOverride, it shouldn't have a FuncFamily
          // associated with it.
          auto const it = cinfo->methodFamilies.find(name);
          always_assert(it != end(cinfo->methodFamilies));

          if (mte.attrs & AttrNoOverride) {
            always_assert(
              std::get_if<FuncFamilyEntry::BothSingle>(&it->second.m_meths) ||
              std::get_if<FuncFamilyEntry::SingleAndNone>(&it->second.m_meths)
            );
          }
        }
      }

      // Create a MethInfo for any missing methods as well.
      for (auto const name : cinfo->missingMethods) {
        assertx(!cinfo->methods.contains(name));
        if (data.methods.contains(name)) continue;
        // The MethInfo will be empty, and be marked as incomplete.
        auto& info = data.methods[name];
        info.complete = false;
        if (cinfo->isRegularClass) info.regularComplete = false;
      }

      data.hasConstProp = cinfo->subHasConstProp;
      data.hasReifiedGeneric = cinfo->subHasReifiedGeneric;

      // If this is a mock class, any direct parent of this class
      // should be marked as mocked.
      if (cinfo->isMockClass) {
        for (auto const p : cinfo->classGraph.directParents()) {
          data.mockedClasses.emplace(p.name());
        }
      }
      data.isSubMocked = cinfo->isMocked || cinfo->isSubMocked;

      data.hasRegularClass = cinfo->isRegularClass;
      data.hasRegularClassFull =
        data.hasRegularClass || cinfo->classGraph.mightHaveRegularSubclass();
      if (!data.hasRegularClass && index.leafs.contains(clsname)) {
        data.hasRegularClass = data.hasRegularClassFull;
      }

      for (auto const& prop : cls.properties) {
        if (!(prop.attrs & (AttrStatic|AttrPrivate|AttrNoImplicitNullable))) {
          data.propsWithImplicitNullable.emplace(prop.name);
        }
      }

      for (auto const& [n, info] : cinfo->propDeclInfo) {
        auto& decls = data.propDeclInfo[n];
        decls = info.subDecls;
        if (info.decl) decls.emplace(info.decl);
      }

      data.cnsSubInfo = cinfo->cnsSubInfo;
      return data;
    }

    // It doesn't represent a class. It should represent a
    // split.

    // A split cannot be both a root and a dependency due to how we
    // set up the buckets.
    assertx(!index.top.contains(clsname));
    auto const split = folly::get_default(index.splits, clsname);
    always_assert(split != nullptr);
    assertx(split->children.empty());
    // Split already contains the Data, so nothing to do but return
    // it.
    return split->data;
  }

  static void update_data(Data& data, Data childData) {
    // Combine MethInfos for each method name:
    folly::erase_if(
      data.methods,
      [&] (std::pair<const SString, Data::MethInfo>& p) {
        auto const name = p.first;
        auto& info = p.second;

        if (auto const childInfo =
            folly::get_ptr(childData.methods, name)) {
          // There's a MethInfo with that name in the
          // child. "Promote" the MethRefs if they're in a superior
          // status in the child.
          for (auto const& meth : childInfo->regularMeths) {
            if (info.regularMeths.contains(meth)) continue;
            info.regularMeths.emplace(meth);
            info.nonRegularPrivateMeths.erase(meth);
            info.nonRegularMeths.erase(meth);
          }
          for (auto const& meth : childInfo->nonRegularPrivateMeths) {
            if (info.regularMeths.contains(meth) ||
                info.nonRegularPrivateMeths.contains(meth)) {
              continue;
            }
            info.nonRegularPrivateMeths.emplace(meth);
            info.nonRegularMeths.erase(meth);
          }
          for (auto const& meth : childInfo->nonRegularMeths) {
            if (info.regularMeths.contains(meth) ||
                info.nonRegularPrivateMeths.contains(meth) ||
                info.nonRegularMeths.contains(meth)) {
              continue;
            }
            info.nonRegularMeths.emplace(meth);
          }
          info.complete &= childInfo->complete;
          if (childData.hasRegularClassFull) {
            info.regularComplete &= childInfo->regularComplete;
            info.privateAncestor |= childInfo->privateAncestor;
          } else {
            assertx(childInfo->regularComplete);
            assertx(!childInfo->privateAncestor);
          }

          if (childInfo->allStatic) {
            if (!info.allStatic) {
              info.allStatic = std::move(*childInfo->allStatic);
            } else {
              *info.allStatic |= *childInfo->allStatic;
            }
          }
          if (childInfo->regularStatic) {
            if (!info.regularStatic) {
              info.regularStatic = std::move(*childInfo->regularStatic);
            } else {
              *info.regularStatic |= *childInfo->regularStatic;
            }
          }

          return false;
        }

        // There's no MethInfo with that name in the child. We might
        // still want to keep the MethInfo because it will be needed
        // for expanding abstract class/interface method
        // families. If the child has a regular class, we can remove
        // it (it won't be part of the expansion).
        return
          childData.hasRegularClass ||
          !info.regularComplete ||
          is_special_method_name(name) ||
          name == s_construct.get();
      }
    );

    // Since we drop non-matching method names only if the class has
    // a regular class, it introduces an ordering dependency. If the
    // first class we encounter has a regular class, everything
    // works fine. However, if the first class we encounter does not
    // have a regular class, the Data will have its methods. If we
    // eventually process a class which does have a regular class,
    // we'll never process it's non-matching methods (because we
    // iterate over data.methods). They won't end up in data.methods
    // whereas they would if a class with regular class was
    // processed first. Detect this condition and manually add such
    // methods to data.methods.
    if (!data.hasRegularClass) {
      for (auto& [name, info] : childData.methods) {
        if (!info.regularComplete) continue;
        if (is_special_method_name(name)) continue;
        if (name == s_construct.get()) continue;
        if (data.methods.contains(name)) continue;
        auto& newInfo = data.methods[name];
        newInfo.regularMeths = std::move(info.regularMeths);
        newInfo.nonRegularPrivateMeths =
          std::move(info.nonRegularPrivateMeths);
        newInfo.nonRegularMeths = std::move(info.nonRegularMeths);
        newInfo.allStatic = std::move(info.allStatic);
        newInfo.regularStatic = std::move(info.regularStatic);
        newInfo.complete = false;
        newInfo.regularComplete = true;
        newInfo.privateAncestor = info.privateAncestor;
      }
    }

    data.propsWithImplicitNullable.insert(
      begin(childData.propsWithImplicitNullable),
      end(childData.propsWithImplicitNullable)
    );

    for (auto const& [n, d] : childData.propDeclInfo) {
      data.propDeclInfo[n].insert(begin(d), end(d));
    }

    for (auto& [n, i] : data.cnsSubInfo) {
      if (childData.cnsSubInfo.contains(n)) continue;
      i |= ClsCnsSubInfo::missing();
    }
    for (auto& [n, i] : childData.cnsSubInfo) {
      if (auto old = folly::get_ptr(data.cnsSubInfo, n)) {
        *old |= i;
        if (old->dynamic.size() > options.preciseSubclassDynamicCNSLimit) {
          *old = ClsCnsSubInfo::conservative();
        }
      } else {
        i |= ClsCnsSubInfo::missing();
        data.cnsSubInfo.emplace(n, std::move(i));
      }
    }

    data.mockedClasses.insert(
      begin(childData.mockedClasses),
      end(childData.mockedClasses)
    );

    // The rest are booleans which can just be unioned together.
    data.hasConstProp |= childData.hasConstProp;
    data.hasReifiedGeneric |= childData.hasReifiedGeneric;
    data.isSubMocked |= childData.isSubMocked;
    data.hasRegularClass |= childData.hasRegularClass;
    data.hasRegularClassFull |= childData.hasRegularClassFull;
  }

  // Obtain a Data for the given class/split named "top".
  // @param calculatedAcc: an accumulator passed in to track nodes we process
  // while processing children recursively
  static Data aggregate_data(LocalIndex& index,
                             SString top,
                             TSStringSet& calculatedAcc) {
    assertx(index.top.contains(top));

    auto const& children = [&]() -> const std::vector<SString>& {
      auto const it = index.children.find(top);
      always_assert(it != end(index.children));
      assertx(!it->second.empty());
      return it->second;
    }();

    auto const it = index.aggregateData.find(top);
    if (it != end(index.aggregateData)) {
      for (auto const child : children) calculatedAcc.emplace(child);
      return it->second;
    }

    Data data;
    auto first = true;
    // Set of children calculated for current top to ensure we don't
    // duplicate work.
    TSStringSet calculatedForTop;

    // For each child of the class/split (for classes this includes
    // the top class itself), we create a Data, then union it together
    // with the rest.
    size_t childIdx = 0;
    while (calculatedForTop.size() < children.size()) {
      auto const child = children[childIdx++];
      if (calculatedForTop.contains(child)) continue;
      // Top Splits have no associated data yet.
      if (index.top.contains(child) && index.splits.contains(child)) {
        calculatedForTop.emplace(child);
        continue;
      }

      auto childData = [&]() {
        if (index.top.contains(child) && !child->tsame(top)) {
          return aggregate_data(index, child, calculatedForTop);
        } else {
          calculatedForTop.emplace(child);
          return build_data(index, child);
        }
      }();

      // The first Data has nothing to union with, so just use it as is.
      if (first) {
        data = std::move(childData);
        first = false;
        continue;
      }
      update_data(data, std::move(childData));
    }

    for (auto const cls : calculatedForTop) calculatedAcc.emplace(cls);
    always_assert(index.aggregateData.emplace(top, data).second);
    return data;
  }

   // Obtain a Data for the given class/split named "top".
  static Data aggregate_data(LocalIndex& index, SString top) {
    TSStringSet calculated;
    return aggregate_data(index, top, calculated);
  }

  // Create (or re-use an existing) FuncFamily for the given MethInfo.
  static FuncFamily2::Id make_func_family(
    LocalIndex& index,
    SString name,
    Data::MethInfo info
  ) {
    // We should have more than one method because otherwise we
    // shouldn't be trying to create a FuncFamily for it.
    assertx(
      info.regularMeths.size() +
      info.nonRegularPrivateMeths.size() +
      info.nonRegularMeths.size() > 1
    );

    // Before doing the expensive sorting and hashing, see if this
    // FuncFamily already exists. If so, just return the id.
    if (auto const id = folly::get_ptr(
        index.funcFamilyCache,
        MethInfoTupleProxy{
          &info.regularMeths,
          &info.nonRegularPrivateMeths,
          &info.nonRegularMeths
        }
      )) {
      return *id;
    }

    // Nothing in the cache. We need to do the expensive step of
    // actually creating the FuncFamily.

    // First sort the methods so they're in deterministic order.
    std::vector<MethRef> regular{
      begin(info.regularMeths), end(info.regularMeths)
    };
    std::vector<MethRef> nonRegularPrivate{
      begin(info.nonRegularPrivateMeths), end(info.nonRegularPrivateMeths)
    };
    std::vector<MethRef> nonRegular{
      begin(info.nonRegularMeths), end(info.nonRegularMeths)
    };
    std::sort(begin(regular), end(regular));
    std::sort(begin(nonRegularPrivate), end(nonRegularPrivate));
    std::sort(begin(nonRegular), end(nonRegular));

    // Create the id by hashing the methods:
    SHA1Hasher hasher;
    {
      auto const size1 = regular.size();
      auto const size2 = nonRegularPrivate.size();
      auto const size3 = nonRegular.size();
      hasher.update((const char*)&size1, sizeof(size1));
      hasher.update((const char*)&size2, sizeof(size2));
      hasher.update((const char*)&size3, sizeof(size3));
    }
    for (auto const& m : regular) {
      hasher.update(m.cls->data(), m.cls->size());
      hasher.update((const char*)&m.idx, sizeof(m.idx));
    }
    for (auto const& m : nonRegularPrivate) {
      hasher.update(m.cls->data(), m.cls->size());
      hasher.update((const char*)&m.idx, sizeof(m.idx));
    }
    for (auto const& m : nonRegular) {
      hasher.update(m.cls->data(), m.cls->size());
      hasher.update((const char*)&m.idx, sizeof(m.idx));
    }
    auto const id = hasher.finish();

    // See if this id exists already. If so, record it in the cache
    // and we're done.
    if (index.funcFamilies.contains(id)) {
      index.funcFamilyCache.emplace(
        MethInfoTuple{
          std::move(info.regularMeths),
          std::move(info.nonRegularPrivateMeths),
          std::move(info.nonRegularMeths)
        },
        id
      );
      return id;
    }

    // It's a new id. Create the actual FuncFamily:

    regular.shrink_to_fit();
    nonRegularPrivate.shrink_to_fit();
    nonRegular.shrink_to_fit();

    auto ff = std::make_unique<FuncFamily2>();
    ff->m_id = id;
    ff->m_name = name;
    ff->m_regular = std::move(regular);
    ff->m_nonRegularPrivate = std::move(nonRegularPrivate);
    ff->m_nonRegular = std::move(nonRegular);
    ff->m_allStatic = std::move(info.allStatic);
    ff->m_regularStatic = std::move(info.regularStatic);

    always_assert(
      index.funcFamilies.emplace(id, std::move(ff)).second
    );
    index.newFuncFamilies.emplace_back(id);
    index.funcFamilyCache.emplace(
      MethInfoTuple{
        std::move(info.regularMeths),
        std::move(info.nonRegularPrivateMeths),
        std::move(info.nonRegularMeths)
      },
      id
    );

    return id;
  }

  // Turn a FuncFamily::StaticInfo into an equivalent
  // FuncFamilyEntry::MethMetadata. The StaticInfo must be valid for a
  // single method.
  static FuncFamilyEntry::MethMetadata single_meth_meta_from_static_info(
    const FuncFamily2::StaticInfo& info
  ) {
    assertx(info.m_numInOut);
    assertx(info.m_requiredCoeffects);
    assertx(info.m_coeffectRules);
    assertx(info.m_minNonVariadicParams == info.m_maxNonVariadicParams);
    assertx(info.m_isReadonlyReturn != TriBool::Maybe);
    assertx(info.m_isReadonlyThis != TriBool::Maybe);
    assertx(info.m_supportsAER != TriBool::Maybe);

    FuncFamilyEntry::MethMetadata meta;
    meta.m_prepKinds = info.m_paramPreps;
    meta.m_coeffectRules = *info.m_coeffectRules;
    meta.m_numInOut = *info.m_numInOut;
    meta.m_requiredCoeffects = *info.m_requiredCoeffects;
    meta.m_nonVariadicParams = info.m_minNonVariadicParams;
    meta.m_isReadonlyReturn = info.m_isReadonlyReturn == TriBool::Yes;
    meta.m_isReadonlyThis = info.m_isReadonlyThis == TriBool::Yes;
    meta.m_supportsAER = info.m_supportsAER == TriBool::Yes;
    meta.m_isReified = info.m_maybeReified;
    meta.m_caresAboutDyncalls = info.m_maybeCaresAboutDynCalls;
    meta.m_builtin = info.m_maybeBuiltin;
    return meta;
  }

  // Translate a MethInfo into the appropriate FuncFamilyEntry
  static FuncFamilyEntry make_method_family_entry(
    LocalIndex& index,
    SString name,
    Data::MethInfo info
  ) {
    FuncFamilyEntry entry;
    entry.m_allIncomplete = !info.complete;
    entry.m_regularIncomplete = !info.regularComplete;
    entry.m_privateAncestor = info.privateAncestor;

    if (info.regularMeths.size() + info.nonRegularPrivateMeths.size() > 1) {
      // There's either multiple regularMeths, multiple
      // nonRegularPrivateMeths, or one of each (remember they are
      // disjoint). In either case, there's more than one method, so
      // we need a func family.
      assertx(info.allStatic);
      assertx(info.regularStatic);
      auto const ff = make_func_family(index, name, std::move(info));
      entry.m_meths = FuncFamilyEntry::BothFF{ff};
    } else if (!info.regularMeths.empty() ||
               !info.nonRegularPrivateMeths.empty()) {
      // We know their sum isn't greater than one, so only one of them
      // can be non-empty (and the one that is has only a single
      // method).
      assertx(info.allStatic);
      assertx(info.regularStatic);
      auto const r = !info.regularMeths.empty()
        ? *begin(info.regularMeths)
        : *begin(info.nonRegularPrivateMeths);
      if (info.nonRegularMeths.empty()) {
        // There's only one method and it covers both variants.
        entry.m_meths = FuncFamilyEntry::BothSingle{
          r,
          single_meth_meta_from_static_info(*info.allStatic),
          info.regularMeths.empty()
        };
      } else {
        // nonRegularMeths is non-empty. Since the MethRefSets are
        // disjoint, overall there's more than one method so need a
        // func family.
        auto const nonRegularPrivate = info.regularMeths.empty();
        auto const ff = make_func_family(index, name, std::move(info));
        entry.m_meths = FuncFamilyEntry::FFAndSingle{ff, r, nonRegularPrivate};
      }
    } else if (info.nonRegularMeths.size() > 1) {
      // Both regularMeths and nonRegularPrivateMeths is empty. If
      // there's multiple nonRegularMeths, we need a func family for
      // the non-regular variant, but the regular variant is empty.
      assertx(info.allStatic);
      assertx(!info.regularStatic);
      auto const ff = make_func_family(index, name, std::move(info));
      entry.m_meths = FuncFamilyEntry::FFAndNone{ff};
    } else if (!info.nonRegularMeths.empty()) {
      // There's exactly one nonRegularMeths method (and nothing for
      // the regular variant).
      assertx(info.allStatic);
      assertx(!info.regularStatic);
      entry.m_meths = FuncFamilyEntry::SingleAndNone{
        *begin(info.nonRegularMeths),
        single_meth_meta_from_static_info(*info.allStatic)
      };
    } else {
      // No methods at all
      assertx(!info.complete);
      assertx(!info.allStatic);
      assertx(!info.regularStatic);
      entry.m_meths = FuncFamilyEntry::None{};
    }

    return entry;
  }

  // Calculate the data for each root (those which will we'll provide
  // outputs for) and update the ClassInfo or Split as appropriate.
  static void process_roots(
    LocalIndex& index,
    const std::vector<std::unique_ptr<ClassInfo2>>& roots,
    const std::vector<std::unique_ptr<Split>>& splits
  ) {
    for (auto const& cinfo : roots) {
      assertx(index.top.contains(cinfo->name));
      // Process the children of this class and build a unified Data
      // for it.
      auto data = aggregate_data(index, cinfo->name);

      // These are just copied directly from Data.
      cinfo->subHasConstProp = data.hasConstProp;
      cinfo->subHasReifiedGeneric = data.hasReifiedGeneric;

      auto& cls = index.cls(cinfo->name);

      // This class is mocked if its on the mocked classes list.
      cinfo->isMocked = (bool)data.mockedClasses.contains(cinfo->name);
      cinfo->isSubMocked = data.isSubMocked || cinfo->isMocked;
      attribute_setter(cls.attrs, !cinfo->isSubMocked, AttrNoMock);

      // We can use whether we saw regular/non-regular subclasses to
      // infer if this class is overridden.
      if (cinfo->classGraph.mightHaveRegularSubclass()) {
        attribute_setter(cls.attrs, false, AttrNoOverrideRegular);
        attribute_setter(cls.attrs, false, AttrNoOverride);
      } else if (cinfo->classGraph.mightHaveNonRegularSubclass()) {
        attribute_setter(cls.attrs, true, AttrNoOverrideRegular);
        attribute_setter(cls.attrs, false, AttrNoOverride);
      } else {
        attribute_setter(cls.attrs, true, AttrNoOverrideRegular);
        attribute_setter(cls.attrs, true, AttrNoOverride);
      }

      assertx(
        IMPLIES(
          cinfo->initialNoReifiedInit,
          cls.attrs & AttrNoReifiedInit
        )
      );

      attribute_setter(
        cls.attrs,
        [&] {
          if (cinfo->initialNoReifiedInit) return true;
          if (cinfo->parent) return false;
          if (cls.attrs & AttrInterface) return true;
          return !data.hasReifiedGeneric;
        }(),
        AttrNoReifiedInit
      );

      for (auto const& [n, decls] : data.propDeclInfo) {
        auto& info = cinfo->propDeclInfo[n];
        assertx(info.subDecls.empty());
        // If there's no higher declaration, keep the entry (which we
        // might have just created), but don't track subclasses any
        // longer.
        if (!info.decl) continue;
        info.subDecls = decls;
        info.subDecls.erase(info.decl);
      }

      cinfo->cnsSubInfo = std::move(data.cnsSubInfo);

      for (auto& [name, mte] : cinfo->methods) {
        if (is_special_method_name(name)) continue;

        // Since this is the first time we're processing this class,
        // all of the methods should be marked as AttrNoOverride.
        assertx(mte.attrs & AttrNoOverride);
        assertx(mte.noOverrideRegular());

        auto& info = [&, name=name] () -> Data::MethInfo& {
          auto it = data.methods.find(name);
          always_assert(it != end(data.methods));
          return it->second;
        }();

        auto const meth = mte.meth();

        // Is this method overridden?
        auto const noOverride = [&] {
          // An incomplete method family is always overridden because
          // the call could fail.
          if (!info.complete) return false;
          // If more than one method then no.
          if (info.regularMeths.size() +
              info.nonRegularPrivateMeths.size() +
              info.nonRegularMeths.size() > 1) {
            return false;
          }
          // NB: All of the below checks all return true. The
          // different conditions are just for checking the right
          // invariants.
          if (info.regularMeths.empty()) {
            // The (single) method isn't on a regular class. This
            // class shouldn't have any regular classes (the set is
            // complete so if we did, the method would have been on
            // it). The (single) method must be on nonRegularMeths or
            // nonRegularPrivateMeths.
            assertx(!cinfo->isRegularClass);
            if (info.nonRegularPrivateMeths.empty()) {
              assertx(info.nonRegularMeths.contains(meth));
              return true;
            }
            assertx(info.nonRegularMeths.empty());
            assertx(info.nonRegularPrivateMeths.contains(meth));
            return true;
          }
          assertx(info.nonRegularPrivateMeths.empty());
          assertx(info.nonRegularMeths.empty());
          assertx(info.regularMeths.contains(meth));
          return true;
        };

        // Is this method overridden in a regular class? (weaker
        // condition)
        auto const noOverrideRegular = [&] {
          // An incomplete method family is always overridden because
          // the call could fail.
          if (!info.regularComplete) return false;
          // If more than one method then no. For the purposes of this
          // check, non-regular but private methods are included.
          if (info.regularMeths.size() +
              info.nonRegularPrivateMeths.size() > 1) {
            return false;
          }
          if (info.regularMeths.empty()) {
            // The method isn't on a regular class. Like in
            // noOverride(), the class shouldn't have any regular
            // classes. If nonRegularPrivateMethos is empty, this
            // means any possible override is non-regular, so we're
            // good.
            assertx(!cinfo->isRegularClass);
            if (info.nonRegularPrivateMeths.empty()) return true;
            return (bool)info.nonRegularPrivateMeths.contains(meth);
          }
          if (cinfo->isRegularClass) {
            // If this class is regular, the method on this class
            // should be marked as regular.
            assertx(info.regularMeths.contains(meth));
            return true;
          }
          // We know regularMeths is non-empty, and the size is at
          // most one. If this method is the (only) one in
          // regularMeths, it's not overridden by anything.
          return (bool)info.regularMeths.contains(meth);
        };

        if (!noOverrideRegular()) {
          mte.clearNoOverrideRegular();
          attribute_setter(mte.attrs, false, AttrNoOverride);
        } else if (!noOverride()) {
          attribute_setter(mte.attrs, false, AttrNoOverride);
        }

        // Keep func attrs in sync
        if (meth.cls->tsame(cinfo->name)) {
          assertx(meth.idx < cls.methods.size());
          auto& m = cls.methods[meth.idx];
          attribute_setter(
            m->attrs,
            (bool)(mte.attrs & AttrNoOverride),
            AttrNoOverride
          );
        }

        auto& entry = cinfo->methodFamilies.at(name);
        assertx(
          std::get_if<FuncFamilyEntry::BothSingle>(&entry.m_meths) ||
          std::get_if<FuncFamilyEntry::SingleAndNone>(&entry.m_meths)
        );

        if constexpr (debug) {
          if (mte.attrs & AttrNoOverride) {
            always_assert(info.complete);
            always_assert(info.regularComplete);

            if (cinfo->isRegularClass ||
                cinfo->classGraph.mightHaveRegularSubclass()) {
              always_assert(info.regularMeths.size() == 1);
              always_assert(info.regularMeths.contains(meth));
              always_assert(info.nonRegularPrivateMeths.empty());
              always_assert(info.nonRegularMeths.empty());
            } else {
              // If this class isn't regular, it could still have a
              // regular method which it inherited from a (regular)
              // parent. There should only be one method across all the
              // sets though.
              always_assert(
                info.regularMeths.size() +
                info.nonRegularPrivateMeths.size() +
                info.nonRegularMeths.size() == 1
              );
              always_assert(
                info.regularMeths.contains(meth) ||
                info.nonRegularPrivateMeths.contains(meth) ||
                info.nonRegularMeths.contains(meth)
              );
            }

            if (mte.hasPrivateAncestor() &&
                (cinfo->isRegularClass ||
                 cinfo->classGraph.mightHaveRegularSubclass())) {
              always_assert(info.privateAncestor);
            } else {
              always_assert(!info.privateAncestor);
            }
          } else {
            always_assert(!(cls.attrs & AttrNoOverride));
          }
        }

        // NB: Even if the method is AttrNoOverride, we might need to
        // change the FuncFamilyEntry. This class could be non-regular
        // and a child class could be regular. Even if the child class
        // doesn't override the method, it changes it from non-regular
        // to regular.
        entry = make_method_family_entry(index, name, std::move(info));

        if (mte.attrs & AttrNoOverride) {
          // However, even if the entry changes with AttrNoOverride,
          // it can only be these two cases.
          always_assert(
            std::get_if<FuncFamilyEntry::BothSingle>(&entry.m_meths) ||
            std::get_if<FuncFamilyEntry::SingleAndNone>(&entry.m_meths)
          );
        }
      }

      /*
       * Interfaces can cause monotonicity violations. Suppose we have two
       * interfaces: I2 and I2. I1 declares a method named Foo. Every
       * class which implements I2 also implements I1 (therefore I2
       * implies I1). During analysis, a type is initially Obj<=I1 and we
       * resolve a call to Foo using I1's func families. After further
       * optimization, we narrow the type to Obj<=I2. Now when we go to
       * resolve a call to Foo using I2's func families, we find
       * nothing. Foo is declared in I1, not in I2, and interface methods
       * are not inherited. We use the fall back name-only tables, which
       * might give us a worse type than before. This is a monotonicity
       * violation because refining the object type gave us worse
       * analysis.
       *
       * To avoid this, we expand an interface's (and abstract class'
       * which has similar issues) func families to include all methods
       * defined by *all* of it's (regular) implementations. So, in the
       * example above, we'd expand I2's func families to include Foo,
       * since all of I2's implements should define a Foo method (since
       * they also all implement I1).
       *
       * Any MethInfos which are part of the abstract class/interface
       * method table has already been processed above. Any ones which
       * haven't are candidates for the above expansion and must also
       * be placed in the method families table. Note: we do not just
       * restrict this to just abstract classes or interfaces. This
       * class may be a child of an abstract class or interfaces and
       * we need to propagate these "expanded" methods so they're
       * available in the dependency when we actually process the
       * abstract class/interface in a later round.
       */
      for (auto& [name, info] : data.methods) {
        if (cinfo->methods.contains(name)) continue;
        assertx(!is_special_method_name(name));
        auto entry = make_method_family_entry(index, name, std::move(info));
        always_assert(
          cinfo->methodFamilies.emplace(name, std::move(entry)).second
        );
      }

      for (auto& prop : cls.properties) {
        if (bool(prop.attrs & AttrNoImplicitNullable) &&
            !(prop.attrs & (AttrStatic | AttrPrivate))) {
          attribute_setter(
            prop.attrs,
            !data.propsWithImplicitNullable.contains(prop.name),
            AttrNoImplicitNullable
          );
        }

        if (!(prop.attrs & AttrSystemInitialValue)) continue;
        if (prop.val.m_type == KindOfUninit) {
          assertx(prop.attrs & AttrLateInit);
          continue;
        }

        prop.val = [&] {
          if (!(prop.attrs & AttrNoImplicitNullable)) {
            return make_tv<KindOfNull>();
          }
          // Give the 86reified_prop a special default value to
          // avoid pessimizing the inferred type (we want it to
          // always be a vec of a specific size).
          if (prop.name == s_86reified_prop.get()) {
            return get_default_value_of_reified_list(cls.userAttributes);
          }
          auto dv = prop.typeConstraints.defaultValue();
          return dv ? dv.value() : make_tv<KindOfNull>();
        }();
      }
    }

    // Splits just store the data directly. Since this split hasn't
    // been processed yet (and no other job should process it), all of
    // the fields should be their default settings.
    for (auto& split : splits) {
      assertx(index.top.contains(split->name));
      split->data = aggregate_data(index, split->name);
      // This split inherits all of the splits of their children.
      for (auto const child : split->children) {
        if (auto const c = folly::get_default(index.classInfos, child)) {
          split->classGraphs.emplace_back(c->classGraph);
          continue;
        }
        auto const s = folly::get_default(index.splits, child);
        always_assert(s);
        split->classGraphs.insert(
          end(split->classGraphs),
          begin(s->classGraphs),
          end(s->classGraphs)
        );
      }
      std::sort(begin(split->classGraphs), end(split->classGraphs));
      split->classGraphs.erase(
        std::unique(begin(split->classGraphs), end(split->classGraphs)),
        end(split->classGraphs)
      );
      split->children.clear();
    }
  }
};

Job<BuildSubclassListJob> s_buildSubclassJob;

struct SubclassWork {
  TSStringToOneT<std::unique_ptr<BuildSubclassListJob::Split>> allSplits;
  struct Bucket {
    std::vector<SString> classes;
    std::vector<SString> deps;
    std::vector<SString> splits;
    std::vector<SString> splitDeps;
    std::vector<SString> leafs;
    std::vector<BuildSubclassListJob::EdgeToSplit> edges;
    size_t cost{0};
  };
  std::vector<std::vector<Bucket>> buckets;
};

/*
 * Algorithm for assigning work for building subclass lists:
 *
 * - Keep track of which classes have been processed and which ones
 *   have not yet been.
 *
 * - Keep looping until all classes have been processed. Each round of
 *   the algorithm becomes a round of output.
 *
 * - Iterate over all classes which haven't been
 *   processed. Distinguish classes which are eligible for processing
 *   or not. A class is eligible for processing if its transitive
 *   dependencies are below the maximum size.
 *
 * - Non-eligible classes are ignored and will be processed again next
 *   round. However, if the class has more eligible direct children
 *   than the bucket size, the class' children will be turned into
 *   split nodes.
 *
 * - Create split nodes. For each class (who we're splitting), use the
 *   typical consistent hashing algorithm to assign each child to a
 *   split node. Change the class' child list to contain the split
 *   nodes instead of the children (this should shrink it
 *   considerably). Each new split becomes a root.
 *
 * - Assign each eligible class to a bucket. Use
 *   assign_hierachial_work to map each eligible class to a bucket.
 *
 * - Update the processed set. Any class which hasn't been processed
 *   that round should have their dependency set shrunken. Processing
 *   a class makes its dependency set be empty. So if a class wasn't
 *   eligible, it should have a dependency which was. Therefore the
 *   class' transitive dependencies should shrink. It should continue
 *   to shrink until its eventually becomes eligible. The same happens
 *   if the class' children are turned into split nodes. Each N
 *   children is replaced with a single split (with no other
 *   dependencies), so the class' dependencies should shrink. Thus,
 *   the algorithm eventually terminates.
*/

// Dependency information for a class or split node.
struct DepData {
  // Transitive dependencies (children) for this class.
  TSStringSet deps;
  // Any split nodes which are dependencies of this class.
  TSStringSet edges;
  // The number of direct children of this class which will be
  // processed this round.
  size_t processChildren{0};
};


// Given a set of roots, greedily add roots and their children to buckets
// via DFS traversal.
template <typename GetDeps>
std::vector<HierarchicalWorkBucket>
dfs_bucketize(SubclassMetadata& subclassMeta,
              std::vector<SString> roots,
              const TSStringToOneT<std::vector<SString>>& splitImmDeps,
              size_t kMaxBucketSize,
              size_t maxClassIdx,
              bool alwaysCreateNew,
              const TSStringSet& leafs,
              const TSStringSet& processed, // already processed
              const GetDeps& getDeps) {
  TSStringSet visited;
  std::vector<std::vector<SString>> rootsToProcess;
  rootsToProcess.emplace_back();
  std::vector<size_t> rootsCost;

  auto const depsSize = [&] (SString cls) {
    return getDeps(cls, getDeps).deps.size();
  };

  size_t cost = 0;

  auto const finishBucket = [&]() {
    if (!cost) return;
    rootsToProcess.emplace_back();
    rootsCost.emplace_back(cost);
    cost = 0;
  };

  auto const addRoot = [&](SString c) {
    rootsToProcess.back().emplace_back(c);
    cost += depsSize(c);
  };

  auto const processSubgraph = [&](SString cls) {
    assertx(!processed.contains(cls));

    addRoot(cls);
    for (auto const& child : getDeps(cls, getDeps).deps) {
      if (processed.contains(child)) continue;
      if (visited.contains(child)) continue;
      visited.insert(child);
      // Leaves use special leaf-promotion logic in assign_hierarchial_work
      if (leafs.contains(child)) continue;
      addRoot(child);
    }
    if (cost < kMaxBucketSize) return;
    finishBucket();
  };

  // Visit immediate children. Recurse until you find a node that has small
  // enough transitive deps.
  auto const visitSubgraph = [&](SString root, auto const& self) {
    if (processed.contains(root) || visited.contains(root)) return false;
    if (!depsSize(root)) return false;
    auto progress = false;
    visited.insert(root);

    assertx(IMPLIES(splitImmDeps.contains(root),
                    depsSize(root) <= kMaxBucketSize));
    if (depsSize(root) <= kMaxBucketSize) {
      processSubgraph(root);
      progress = true;
    } else {
      auto const immChildren = [&] {
        auto const it = subclassMeta.meta.find(root);
        assertx(it != end(subclassMeta.meta));
        return it->second.children;
      }();
      for (auto const& child : immChildren) progress |= self(child, self);
    }
    return progress;
  };

  // Sort the roots to keep it deterministic
  std::sort(
    begin(roots), end(roots),
    [&] (SString a, SString b) {
      auto const s1 = getDeps(a, getDeps).deps.size();
      auto const s2 = getDeps(b, getDeps).deps.size();
      if (s1 != s2) return s1 > s2;
      return string_data_lt_type{}(a, b);
    }
  );

  auto progress = false;
  for (auto const r : roots) {
    assertx(depsSize(r)); // Should never be processing one leaf
    progress |= visitSubgraph(r, visitSubgraph);
  }
  assertx(progress);
  finishBucket();

  if (rootsToProcess.back().empty()) rootsToProcess.pop_back();

  auto const buckets = parallel::gen(
    rootsToProcess.size(),
    [&] (size_t bucketIdx) {
      auto numBuckets =
        (rootsCost[bucketIdx] + (kMaxBucketSize/2)) / kMaxBucketSize;
      if (!numBuckets) numBuckets = 1;
      return consistently_bucketize_by_num_buckets(rootsToProcess[bucketIdx],
        alwaysCreateNew ? rootsToProcess[bucketIdx].size() : numBuckets);
    }
  );

  std::vector<std::vector<SString>> flattened;
  for (auto const& b : buckets) {
    flattened.insert(flattened.end(), b.begin(), b.end());
  }

  auto const work = build_hierarchical_work(
    flattened,
    maxClassIdx,
    [&] (SString c, TSStringSet& out) {
      auto const& deps = getDeps(c, getDeps).deps;
      out.insert(begin(deps), end(deps));
    },
    [] (SString) { return true; },
    [&] (const TSStringSet&, size_t, SString c) -> Optional<size_t> {
      if (!leafs.contains(c)) return std::nullopt;
      return subclassMeta.meta.at(c).idx;
    }
  );
  return work;
}

// For each round:
// While toProcess is not empty:
// 1. Find transitive dep counts
// 2. For each class, calculate splits, find roots, find rootLeafs
// 3. For rootLeafs, consistently hash to make buckets
// 4. For roots, assign subgraphs to buckets via greedy DFS. If buckets get too big,
//    split them via consistent hashing.
SubclassWork build_subclass_lists_assign(SubclassMetadata subclassMeta) {
  trace_time trace{"build subclass lists assign"};
  trace.ignore_client_stats();

  constexpr size_t kBucketSize = 2000;
  constexpr size_t kMaxBucketSize = 25000;

  SubclassWork out;

  auto const maxClassIdx = subclassMeta.all.size();

  // A processed class/split is considered processed once it's
  // assigned to a bucket in a round. Once considered processed, it
  // will have no dependencies.
  TSStringSet processed;

  TSStringToOneT<std::unique_ptr<DepData>> splitDeps;
  TSStringToOneT<std::unique_ptr<BuildSubclassListJob::Split>> splitPtrs;
  TSStringSet leafs;
  TSStringToOneT<std::vector<SString>> splitImmDeps;

  // Keep creating rounds until all of the classes are assigned to a
  // bucket in a round.
  auto toProcess = std::move(subclassMeta.all);
  TSStringSet tp;
  if constexpr (debug) tp.insert(toProcess.begin(), toProcess.end());

  for (size_t round = 0; !toProcess.empty(); ++round) {
    // If we have this many rounds, something has gone wrong, because
    // it should require an astronomical amount of classes.
    always_assert_flog(
      round < 10,
      "Worklist still has {} items after {} rounds. "
      "This almost certainly means it's stuck in an infinite loop",
      toProcess.size(),
      round
    );

    // The dependency information for every class, for just this
    // round. The information is calculated lazily and recursively by
    // findDeps.
    std::vector<LockFreeLazy<DepData>> deps{maxClassIdx};

    auto const findDeps = [&] (SString cls,
                               auto const& self) -> const DepData& {
      // If it's processed, there's implicitly no dependencies
      static DepData empty;
      if (processed.contains(cls)) return empty;

      // Look up the metadata for this class. If we don't find any,
      // assume that it's for a split.
      auto const it = subclassMeta.meta.find(cls);
      if (it == end(subclassMeta.meta)) {
        auto const it2 = splitDeps.find(cls);
        always_assert(it2 != end(splitDeps));
        return *it2->second;
      }
      auto const& meta = it->second;
      auto const idx = meta.idx;
      assertx(idx < deps.size());

      // Now that we have the index into the dependency vector, look
      // it up, calculating it if it hasn't been already.
      return deps[idx].get(
        [&] {
          DepData out;
          for (auto const c : meta.children) {
            // At a minimum, we need the immediate deps in order to
            // construct the subclass lists for the parent.
            out.deps.emplace(c);
            if (splitDeps.contains(c)) out.edges.emplace(c);
            auto const& childDeps = self(c, self);
            if (childDeps.deps.size() <= kMaxBucketSize) ++out.processChildren;
            out.deps.insert(begin(childDeps.deps), end(childDeps.deps));
          }
          return out;
        }
      );
    };

    auto const depsSize = [&] (SString cls) {
      return findDeps(cls, findDeps).deps.size();
    };
    // If this class' children needs to be split into split nodes this
    // round. This happens if the number of direct children of this
    // class which are eligible for processing exceeds the bucket
    // size.
    auto const willSplitChildren = [&] (SString cls) {
      return findDeps(cls, findDeps).processChildren > kBucketSize;
    };
    // If this class will be processed this round. A class will be
    // processed if it's dependencies are less than the maximum bucket
    // size.
    auto const willProcess = [&] (SString cls) {
      // NB: Not <=. When calculating splits, a class is included
      // among it's own dependencies so we need to leave space for one
      // more.
      return depsSize(cls) < kMaxBucketSize;
    };

    // Process every remaining class in parallel and assign an action
    // to each:

    // This class will be processed this round and is a root.
    struct Root { SString cls; };
    struct RootLeaf { SString cls; };
    struct Child { SString cls; };
    // This class' children should be split. The class' child list
    // will be replaced with the new child list and splits created.
    struct Split {
      SString cls;
      std::vector<SString> children;
      struct Data {
        SString name;
        std::unique_ptr<DepData> deps;
        std::unique_ptr<BuildSubclassListJob::Split> ptr;
        std::vector<SString> children;
      };
      std::vector<Data> splits;
    };
    using Action = std::variant<Root, Split, Child, RootLeaf>;

    auto const actions = parallel::map(
      toProcess,
      [&] (SString cls) {
        auto const& meta = subclassMeta.meta.at(cls);

        if (!willSplitChildren(cls)) {
          if (!meta.parents.empty()) return Action{ Child{cls} };
          if (meta.children.empty()) return Action{ RootLeaf{cls} };
          return Action{ Root{cls} };
        }

        // Otherwise we're going to split some/all of this class'
        // children. Once we process those in this round, this class'
        // dependencies should be smaller and be able to be processed.
        Split split;
        split.cls = cls;
        split.splits = [&] {
          // Group all of the eligible children into buckets, and
          // split the buckets to ensure they remain below the maximum
          // size.
          auto const buckets = split_buckets(
            [&] {
              auto const numChildren = findDeps(cls, findDeps).processChildren;
              auto const numBuckets =
                (numChildren + kMaxBucketSize - 1) / kMaxBucketSize;
              assertx(numBuckets > 0);

              std::vector<std::vector<SString>> buckets;
              buckets.resize(numBuckets);
              for (auto const child : meta.children) {
                if (!willProcess(child)) continue;
                auto const idx =
                  consistent_hash(child->hashStatic(), numBuckets);
                assertx(idx < numBuckets);
                buckets[idx].emplace_back(child);
              }

              buckets.erase(
                std::remove_if(
                  begin(buckets),
                  end(buckets),
                  [] (const std::vector<SString>& b) { return b.empty(); }
                ),
                end(buckets)
              );

              assertx(!buckets.empty());
              return buckets;
            }(),
            kMaxBucketSize,
            [&] (SString child, TSStringSet& deps) {
              auto const& d = findDeps(child, findDeps).deps;
              deps.insert(begin(d), end(d));
            }
          );
          // Each bucket corresponds to a new split node, which will
          // contain the results for the children in that bucket.
          auto const numSplits = buckets.size();

          // Actually make the splits and fill their children list.
          std::vector<Split::Data> splits;
          splits.reserve(numSplits);
          for (size_t i = 0; i < numSplits; ++i) {
            // The names of a split node are arbitrary, but must be
            // unique and not collide with any actual classes.
            auto const name = makeStaticString(
              folly::sformat("{}_{}_split;{}", round, i, cls)
            );

            auto deps = std::make_unique<DepData>();
            auto split =
              std::make_unique<BuildSubclassListJob::Split>(name, cls);
            std::vector<SString> children;

            for (auto const child : buckets[i]) {
              split->children.emplace_back(child);
              children.emplace_back(child);
              auto const& childDeps = findDeps(child, findDeps).deps;
              deps->deps.insert(begin(childDeps), end(childDeps));
              deps->deps.emplace(child);
            }
            assertx(deps->deps.size() <= kMaxBucketSize);

            std::sort(
              begin(split->children),
              end(split->children),
              string_data_lt_type{}
            );

            splits.emplace_back(
              Split::Data{
                name,
                std::move(deps),
                std::move(split),
                std::move(children)
              }
            );
          }
          return splits;
        }();

        // Create the new children list for this class. The new
        // children list are any children which won't be processed,
        // and the new splits.
        for (auto const child : meta.children) {
          if (willProcess(child)) continue;
          split.children.emplace_back(child);
        }
        for (auto const& [name, _, _2, _3] : split.splits) {
          split.children.emplace_back(name);
        }

        return Action{ std::move(split) };
      }
    );

    assertx(actions.size() == toProcess.size());
    std::vector<SString> roots;
    roots.reserve(actions.size());
    std::vector<SString> rootLeafs;

    for (auto const& action : actions) {
      match(
        action,
        [&] (Root r) {
          assertx(!subclassMeta.meta.at(r.cls).children.empty());
          roots.emplace_back(r.cls);
        },
        [&] (RootLeaf r) {
          assertx(subclassMeta.meta.at(r.cls).children.empty());
          rootLeafs.emplace_back(r.cls);
          leafs.emplace(r.cls);
        },
        [&] (Child n) {
          auto const& meta = subclassMeta.meta.at(n.cls);
          if (meta.children.empty()) leafs.emplace(n.cls);
        },
        [&] (const Split& s) {
          auto& meta = subclassMeta.meta.at(s.cls);
          meta.children = s.children;
          if (meta.parents.empty()) {
            roots.emplace_back(s.cls);
          }
          auto& splits = const_cast<std::vector<Split::Data>&>(s.splits);
          for (auto& [name, deps, ptr, children] : splits) {
            splitImmDeps.emplace(name, children);
            roots.emplace_back(name);
            splitDeps.emplace(name, std::move(deps));
            splitPtrs.emplace(name, std::move(ptr));
          }
        }
      );
    }

    auto work = dfs_bucketize(
      subclassMeta,
      std::move(roots),
      splitImmDeps,
      kMaxBucketSize,
      maxClassIdx,
      round > 0,
      leafs,
      processed,
      findDeps
    );

    // Bucketize root leafs.
    // These are cheaper since we will only be calculating
    // name-only func family entries.
    for (auto& b : consistently_bucketize(rootLeafs, kMaxBucketSize)) {
      work.emplace_back(HierarchicalWorkBucket{ std::move(b) });
    }

    std::vector<SString> markProcessed;
    markProcessed.reserve(actions.size());

    // The output of assign_hierarchical_work is just buckets with the
    // names. We need to map those to classes or edge nodes and put
    // them in the correct data structure in the output. If there's a
    // class dependency on a split node, we also need to record an
    // edge between them.
    auto const add = [&] (SString cls, auto& clsList,
                          auto& splitList, auto& edgeList) {
      auto const it = splitPtrs.find(cls);
      if (it == end(splitPtrs)) {
        clsList.emplace_back(cls);
        for (auto const s : findDeps(cls, findDeps).edges) {
          edgeList.emplace_back(BuildSubclassListJob::EdgeToSplit{cls, s});
        }
      } else {
        splitList.emplace_back(it->second->name);
      }
    };

    out.buckets.emplace_back();
    for (auto const& w : work) {
      assertx(w.uninstantiable.empty());
      out.buckets.back().emplace_back();
      auto& bucket = out.buckets.back().back();
      // Separate out any of the "roots" which are actually leafs.
      for (auto const cls : w.classes) {
        bucket.cost += depsSize(cls);
        markProcessed.emplace_back(cls);
        if (leafs.contains(cls)) {
          leafs.erase(cls);
          bucket.leafs.emplace_back(cls);
        } else {
          add(cls, bucket.classes, bucket.splits, bucket.edges);
        }
      }
      for (auto const cls : w.deps) {
        add(cls, bucket.deps, bucket.splitDeps, bucket.edges);
      }

      std::sort(
        begin(bucket.edges), end(bucket.edges),
        [] (const BuildSubclassListJob::EdgeToSplit& a,
            const BuildSubclassListJob::EdgeToSplit& b) {
          if (string_data_lt_type{}(a.cls, b.cls)) return true;
          if (string_data_lt_type{}(b.cls, a.cls)) return false;
          return string_data_lt_type{}(a.split, b.split);
        }
      );
      std::sort(begin(bucket.leafs), end(bucket.leafs), string_data_lt_type{});
    }

    std::sort(
      begin(out.buckets.back()), end(out.buckets.back()),
      [] (const SubclassWork::Bucket& a,
          const SubclassWork::Bucket& b) {
        return a.cost > b.cost;
      }
    );

    // Update the processed set. We have to defer that until here
    // because we'd check it when building the buckets.
    processed.insert(begin(markProcessed), end(markProcessed));

    auto const before = toProcess.size();
    toProcess.erase(
      std::remove_if(
        begin(toProcess), end(toProcess),
        [&] (SString c) { return processed.contains(c); }
      ),
      end(toProcess)
    );
    always_assert(toProcess.size() < before);
  }

  // Keep all split nodes created in the output
  for (auto& [name, p] : splitPtrs) out.allSplits.emplace(name, std::move(p));

  // Ensure we create an output for everything exactly once
  if constexpr (debug) {
    for (size_t round = 0; round < out.buckets.size(); ++round) {
      auto const& r = out.buckets[round];
      for (size_t i = 0; i < r.size(); ++i) {
        auto const& bucket = r[i];
        for (auto const c : bucket.classes) always_assert(tp.erase(c));
        for (auto const l : bucket.leafs) always_assert(tp.erase(l));
      }
    }
    assertx(tp.empty());
  }

  if (Trace::moduleEnabled(Trace::hhbbc_index, 5)) {
    for (size_t round = 0; round < out.buckets.size(); ++round) {
      auto const& r = out.buckets[round];
      for (size_t i = 0; i < r.size(); ++i) {
        auto const& bucket = r[i];
        FTRACE(5, "build subclass lists round #{} work item #{}:\n", round, i);
        FTRACE(5, "  classes ({}):\n", bucket.classes.size());
        for (auto const DEBUG_ONLY c : bucket.classes) FTRACE(6, "    {}\n", c);
        FTRACE(5, "  splits ({}):\n", bucket.splits.size());
        for (auto const DEBUG_ONLY s : bucket.splits) FTRACE(6, "    {}\n", s);
        FTRACE(5, "  deps ({}):\n", bucket.deps.size());
        for (auto const DEBUG_ONLY d : bucket.deps) FTRACE(6, "    {}\n", d);
        FTRACE(5, "  split deps ({}):\n", bucket.splitDeps.size());
        for (auto const DEBUG_ONLY s : bucket.splitDeps) {
          FTRACE(6, "    {}\n", s);
        }
        FTRACE(5, "  leafs ({}):\n", bucket.leafs.size());
        for (auto const DEBUG_ONLY c : bucket.leafs) FTRACE(6, "    {}\n", c);
        FTRACE(5, "  edges ({}):\n", bucket.edges.size());
        for (DEBUG_ONLY auto const& e : bucket.edges) {
          FTRACE(6, "    {} -> {}\n", e.cls, e.split);
        }
      }
    }
  }

  return out;
}

void build_subclass_lists(IndexData& index,
                          SubclassMetadata meta,
                          InitTypesMetadata& initTypesMeta) {
  trace_time tracer{"build subclass lists", index.sample};
  tracer.ignore_client_stats();

  using namespace folly::gen;

  // Mapping of splits to their Ref. We only upload a split when we're
  // going to run a job which it is part of the output.
  TSStringToOneT<UniquePtrRef<BuildSubclassListJob::Split>> splitsToRefs;

  FSStringToOneT<hphp_fast_set<FuncFamily2::Id>> funcFamilyDeps;

  // Use the metadata to assign to rounds and buckets.
  auto work = build_subclass_lists_assign(std::move(meta));

  // We need to defer updates to data structures until after all the
  // jobs in a round have completed. Otherwise we could update a ref
  // to a class at the same time another thread is reading it.
  struct Updates {
    std::vector<
      std::tuple<SString, UniquePtrRef<ClassInfo2>, UniquePtrRef<php::Class>>
    > classes;
    std::vector<
      std::pair<SString, UniquePtrRef<BuildSubclassListJob::Split>>
    > splits;
    std::vector<
      std::pair<FuncFamily2::Id, Ref<FuncFamilyGroup>>
    > funcFamilies;
    std::vector<
      std::pair<SString, hphp_fast_set<FuncFamily2::Id>>
    > funcFamilyDeps;
    std::vector<std::pair<SString, UniquePtrRef<ClassInfo2>>> leafs;
    std::vector<std::pair<SString, FuncFamilyEntry>> nameOnly;
    std::vector<std::pair<SString, SString>> candidateRegOnlyEquivs;
    TSStringToOneT<TSStringSet> cnsBases;
  };

  auto const run = [&] (SubclassWork::Bucket bucket, size_t round)
    -> coro::Task<Updates> {
    co_await coro::co_reschedule_on_current_executor;

    if (bucket.classes.empty() &&
        bucket.splits.empty() &&
        bucket.leafs.empty()) {
      assertx(bucket.splitDeps.empty());
      co_return Updates{};
    }

    // We shouldn't get closures or Closure in any of this.
    if constexpr (debug) {
      for (auto const c : bucket.classes) {
        always_assert(!c->tsame(s_Closure.get()));
        always_assert(!is_closure_name(c));
      }
      for (auto const c : bucket.deps) {
        always_assert(!c->tsame(s_Closure.get()));
        always_assert(!is_closure_name(c));
      }
    }

    auto classes = from(bucket.classes)
      | map([&] (SString c) { return index.classInfoRefs.at(c); })
      | as<std::vector>();
    auto deps = from(bucket.deps)
      | map([&] (SString c) { return index.classInfoRefs.at(c); })
      | as<std::vector>();
    auto leafs = from(bucket.leafs)
      | map([&] (SString c) { return index.classInfoRefs.at(c); })
      | as<std::vector>();
    auto splits = from(bucket.splits)
      | map([&] (SString s) {
          std::unique_ptr<BuildSubclassListJob::Split> split =
            std::move(work.allSplits.at(s));
          assertx(split);
          return split;
        })
      | as<std::vector>();
    auto splitDeps = from(bucket.splitDeps)
      | map([&] (SString s) { return splitsToRefs.at(s); })
      | as<std::vector>();
    auto phpClasses =
      (from(bucket.classes) + from(bucket.deps) + from(bucket.leafs))
      | map([&] (SString c) { return index.classRefs.at(c); })
      | as<std::vector>();

    std::vector<Ref<FuncFamilyGroup>> funcFamilies;
    if (round > 0) {
      // Provide the func families associated with any dependency
      // classes going into this job. We only need to do this after
      // the first round because in the first round all dependencies
      // are leafs and won't have any func families.
      for (auto const c : bucket.deps) {
        if (auto const deps = folly::get_ptr(funcFamilyDeps, c)) {
          for (auto const& d : *deps) {
            funcFamilies.emplace_back(index.funcFamilyRefs.at(d));
          }
        }
      }
      // Keep the func families in deterministic order and avoid
      // duplicates.
      std::sort(begin(funcFamilies), end(funcFamilies));
      funcFamilies.erase(
        std::unique(begin(funcFamilies), end(funcFamilies)),
        end(funcFamilies)
      );
    } else {
      assertx(funcFamilyDeps.empty());
      assertx(index.funcFamilyRefs.empty());
    }

    // ClassInfos and any dependency splits should already be
    // stored. Any splits as output of the job, or edges need to be
    // uploaded, however.
    auto [splitRefs, edges, config] = co_await coro::collectAll(
      index.client->storeMulti(std::move(splits)),
      index.client->storeMulti(std::move(bucket.edges)),
      index.configRef->getCopy()
    );

    auto metadata = make_exec_metadata(
      "build subclass list",
      round,
      [&] {
        if (!bucket.classes.empty()) return bucket.classes[0];
        if (!bucket.splits.empty()) return bucket.splits[0];
        assertx(!bucket.leafs.empty());
        return bucket.leafs[0];
      }()->toCppString()
    );

    auto results = co_await
      index.client->exec(
        s_buildSubclassJob,
        std::move(config),
        singleton_vec(
          std::make_tuple(
            std::move(classes),
            std::move(deps),
            std::move(leafs),
            std::move(splitRefs),
            std::move(splitDeps),
            std::move(phpClasses),
            std::move(edges),
            std::move(funcFamilies)
          )
        ),
        std::move(metadata)
      );
    // Every job is a single work-unit, so we should only ever get one
    // result for each one.
    assertx(results.size() == 1);
    auto& [cinfoRefs, outSplitRefs, clsRefs, ffRefs, leafRefs, outMetaRef]
      = results[0];
    assertx(cinfoRefs.size() == bucket.classes.size());
    assertx(outSplitRefs.size() == bucket.splits.size());
    assertx(clsRefs.size() == bucket.classes.size());
    assertx(leafRefs.size() == bucket.leafs.size());

    auto outMeta = co_await index.client->load(std::move(outMetaRef));
    assertx(outMeta.newFuncFamilyIds.size() == ffRefs.size());
    assertx(outMeta.funcFamilyDeps.size() == cinfoRefs.size());
    assertx(outMeta.regOnlyEquivCandidates.size() == cinfoRefs.size());

    Updates updates;
    updates.classes.reserve(bucket.classes.size());
    updates.splits.reserve(bucket.splits.size());
    updates.funcFamilies.reserve(outMeta.newFuncFamilyIds.size());
    updates.funcFamilyDeps.reserve(outMeta.funcFamilyDeps.size());
    updates.nameOnly.reserve(outMeta.nameOnly.size());
    updates.leafs.reserve(bucket.leafs.size());

    for (size_t i = 0, size = bucket.classes.size(); i < size; ++i) {
      updates.classes.emplace_back(bucket.classes[i], cinfoRefs[i], clsRefs[i]);
    }
    for (size_t i = 0, size = bucket.splits.size(); i < size; ++i) {
      updates.splits.emplace_back(bucket.splits[i], outSplitRefs[i]);
    }
    for (size_t i = 0, size = bucket.leafs.size(); i < size; ++i) {
      updates.leafs.emplace_back(bucket.leafs[i], leafRefs[i]);
    }
    for (size_t i = 0, size = outMeta.newFuncFamilyIds.size(); i < size; ++i) {
      auto const ref = ffRefs[i];
      for (auto const& id : outMeta.newFuncFamilyIds[i]) {
        updates.funcFamilies.emplace_back(id, ref);
      }
    }
    for (size_t i = 0, size = outMeta.funcFamilyDeps.size(); i < size; ++i) {
      updates.funcFamilyDeps.emplace_back(
        bucket.classes[i],
        std::move(outMeta.funcFamilyDeps[i])
      );
    }
    updates.nameOnly = std::move(outMeta.nameOnly);
    for (size_t i = 0, size = outMeta.regOnlyEquivCandidates.size();
         i < size; ++i) {
      auto const name = bucket.classes[i];
      for (auto const c : outMeta.regOnlyEquivCandidates[i]) {
        updates.candidateRegOnlyEquivs.emplace_back(name, c);
      }
    }
    updates.cnsBases = std::move(outMeta.cnsBases);

    co_return updates;
  };

  {
    trace_time tracer2{"build subclass lists work", index.sample};

    for (size_t roundNum = 0; roundNum < work.buckets.size(); ++roundNum) {
      auto& round = work.buckets[roundNum];
      // In each round, run all of the work for each bucket
      // simultaneously, gathering up updates from each job.
      auto const updates = coro::blockingWait(coro::collectAllRange(
        from(round)
          | move
          | map([&] (SubclassWork::Bucket&& b) {
              return co_withExecutor(index.executor->sticky(), run(std::move(b), roundNum)
                );
            })
          | as<std::vector>()
      ));

      // Apply the updates to ClassInfo refs. We can do this
      // concurrently because every ClassInfo is already in the map, so
      // we can update in place (without mutating the map).
      parallel::for_each(
        updates,
        [&] (const Updates& u) {
          for (auto const& [name, cinfo, cls] : u.classes) {
            index.classInfoRefs.at(name) = cinfo;
            index.classRefs.at(name) = cls;
          }
          for (auto const& [name, cinfo] : u.leafs) {
            index.classInfoRefs.at(name) = cinfo;
          }
        }
      );

      // However updating splitsToRefs cannot be, because we're mutating
      // the map by inserting into it. However there's a relatively
      // small number of splits, so this should be fine.
      parallel::parallel(
        [&] {
          for (auto const& u : updates) {
            for (auto const& [name, ref] : u.splits) {
              always_assert(splitsToRefs.emplace(name, ref).second);
            }
          }
        },
        [&] {
          for (auto const& u : updates) {
            for (auto const& [id, ref] : u.funcFamilies) {
              // The same FuncFamily can be grouped into multiple
              // different groups. Prefer the group that's smaller and
              // if they're the same size, use the one with the lowest
              // id to keep determinism.
              auto const& [existing, inserted] =
                index.funcFamilyRefs.emplace(id, ref);
              if (inserted) continue;
              if (existing->second.id().m_size < ref.id().m_size) continue;
              if (ref.id().m_size < existing->second.id().m_size) {
                existing->second = ref;
                continue;
              }
              if (existing->second.id() <= ref.id()) continue;
              existing->second = ref;
            }
          }
        },
        [&] {
          for (auto& u : updates) {
            for (auto& [name, ids] : u.funcFamilyDeps) {
              always_assert(
                funcFamilyDeps.emplace(name, std::move(ids)).second
              );
            }
          }
        },
        [&] {
          for (auto& u : updates) {
            for (auto& [name, entry] : u.nameOnly) {
              initTypesMeta.nameOnlyFF[name].emplace_back(std::move(entry));
            }
            for (auto [name, candidate] : u.candidateRegOnlyEquivs) {
              initTypesMeta.classes[name]
                .candidateRegOnlyEquivs.emplace(candidate);
            }
          }
        },
        [&] {
          for (auto& u : updates) {
            for (auto& [n, o] : u.cnsBases) {
              always_assert(
                index.classToCnsBases.emplace(n, std::move(o)).second
              );
            }
          }
        }
      );
    }
  }

  splitsToRefs.clear();
  funcFamilyDeps.clear();
  work.buckets.clear();
  work.allSplits.clear();
}

//////////////////////////////////////////////////////////////////////

/*
 * Initialize the return-types of functions and methods from their
 * type-hints. Also set AttrInitialSatisfiesTC on properties if
 * appropriate (which must be done after types are initialized).
 */
struct InitTypesJob {
  static std::string name() { return "hhbbc-init-types"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
    ClassGraph::init();
  }
  static void fini() { ClassGraph::destroy(); }

  using Output = Multi<
    Variadic<std::unique_ptr<php::Class>>,
    Variadic<std::unique_ptr<ClassInfo2>>,
    Variadic<std::unique_ptr<php::Func>>,
    Variadic<std::unique_ptr<FuncInfo2>>
  >;
  static Output run(Variadic<std::unique_ptr<php::Class>> classes,
                    Variadic<std::unique_ptr<ClassInfo2>> cinfos,
                    Variadic<std::unique_ptr<php::Func>> funcs,
                    Variadic<std::unique_ptr<FuncInfo2>> finfos,
                    Variadic<std::unique_ptr<ClassInfo2>> cinfoDeps) {
    LocalIndex index;

    for (auto const& cls : classes.vals) {
      always_assert(index.classes.emplace(cls->name, cls.get()).second);
      for (auto const& clo : cls->closures) {
        always_assert(index.classes.emplace(clo->name, clo.get()).second);
      }
    }

    // All of the classes which might be a regular only equivalent
    // have been provided to the job. So, we can now definitely set
    // the regular only equivalent (if necessary). We need to do this
    // before setting the initial types because we need that
    // information to canonicalize.
    for (auto const& cinfo : cinfos.vals) {
      always_assert(index.classInfos.emplace(cinfo->name, cinfo.get()).second);
      cinfo->classGraph.setRegOnlyEquivs();
      // If this is a regular class, we don't need the "expanded"
      // method family information anymore, so clear it here to save
      // memory.
      if (cinfo->isRegularClass) {
        folly::erase_if(
          cinfo->methodFamilies,
          [&] (auto const& e) { return !cinfo->methods.contains(e.first); }
        );
      }

      for (auto const& clo : cinfo->closures) {
        always_assert(index.classInfos.emplace(clo->name, clo.get()).second);
        clo->classGraph.setRegOnlyEquivs();
      }
    }
    for (auto const& cinfo : cinfoDeps.vals) {
      always_assert(index.classInfos.emplace(cinfo->name, cinfo.get()).second);
      cinfo->classGraph.setRegOnlyEquivs();
      for (auto const& clo : cinfo->closures) {
        always_assert(index.classInfos.emplace(clo->name, clo.get()).second);
        clo->classGraph.setRegOnlyEquivs();
      }
    }

    for (auto const& finfo : finfos.vals) {
      always_assert(index.funcInfos.emplace(finfo->name, finfo.get()).second);
    }

    auto const onCls = [&] (php::Class& cls, ClassInfo2& cinfo) {
      assertx(cls.name->tsame(cinfo.name));
      assertx(cinfo.funcInfos.size() == cls.methods.size());

      unresolve_missing(index, cls);
      set_bad_initial_prop_values(index, cls, cinfo);
      for (size_t i = 0, size = cls.methods.size(); i < size; ++i) {
        auto const& func = cls.methods[i];
        auto& finfo = cinfo.funcInfos[i];
        assertx(func->name == finfo->name);
        assertx(finfo->inferred.returnTy.is(BInitCell));
        finfo->inferred.returnTy = initial_return_type(index, *func);
      }
    };

    assertx(classes.vals.size() == cinfos.vals.size());
    for (size_t i = 0, size = classes.vals.size(); i < size; ++i) {
      auto& cls = classes.vals[i];
      auto& cinfo = cinfos.vals[i];
      onCls(*cls, *cinfo);

      assertx(cls->closures.size() == cinfo->closures.size());
      for (size_t j = 0, size2 = cls->closures.size(); j < size2; ++j) {
        auto& clo = cls->closures[j];
        auto& cloinfo = cinfo->closures[j];
        onCls(*clo, *cloinfo);
      }
    }

    assertx(funcs.vals.size() == finfos.vals.size());
    for (size_t i = 0, size = funcs.vals.size(); i < size; ++i) {
      auto const& func = funcs.vals[i];
      auto& finfo = finfos.vals[i];
      assertx(func->name == finfo->name);
      assertx(finfo->inferred.returnTy.is(BInitCell));
      unresolve_missing(index, *func);
      finfo->inferred.returnTy = initial_return_type(index, *func);
    }

    return std::make_tuple(
      std::move(classes),
      std::move(cinfos),
      std::move(funcs),
      std::move(finfos)
    );
  }

private:

  struct LocalIndex {
    TSStringToOneT<ClassInfo2*> classInfos;
    TSStringToOneT<php::Class*> classes;
    FSStringToOneT<FuncInfo2*> funcInfos;
  };

  static void unresolve_missing(const LocalIndex& index, TypeConstraint& tc) {
    if (!tc.isSubObject()) return;
    auto const name = tc.clsName();
    if (index.classInfos.contains(name)) return;
    FTRACE(
      4, "Unresolving type-constraint for '{}' because it does not exist\n",
      name
    );
    tc.unresolve();
  }

  static void unresolve_missing(const LocalIndex& index, php::Func& func) {
    auto const UNUSED bump = trace_bump(func, Trace::hhbbc_index);
    for (auto& p : func.params) {
      p.typeConstraints.forEachMutable([&](TypeConstraint& tc) {
        unresolve_missing(index, tc);
      });
    }

    func.retTypeConstraints.forEachMutable([&](TypeConstraint& tc) {
      unresolve_missing(index, tc);
    });
  }

  static void unresolve_missing(const LocalIndex& index, php::Class& cls) {
    auto const UNUSED bump = trace_bump(cls, Trace::hhbbc_index);
    if (cls.attrs & AttrEnum) unresolve_missing(index, cls.enumBaseTy);
    for (auto& meth : cls.methods) unresolve_missing(index, *meth);
    for (auto& prop : cls.properties) {
      prop.typeConstraints.forEachMutable([&](TypeConstraint& tc) {
        unresolve_missing(index, tc);
      });
    }
  }

  static Type initial_return_type(const LocalIndex& index,
                                  const php::Func& f) {
    auto const UNUSED bump = trace_bump(f, Trace::hhbbc_index);

    auto const ensure = [&] (res::Class c) {
      auto const onFunc = [&] (SString n) {
        auto const finfo = folly::get_default(index.funcInfos, n);
        always_assert(finfo);
        if (!finfo->auxClassGraphs) {
          finfo->auxClassGraphs = std::make_unique<AuxClassGraphs>();
        }
        finfo->auxClassGraphs->withChildren.emplace(c.graph());
      };

      auto const onCls = [&] (SString n) {
        auto const cinfo = folly::get_default(index.classInfos, n);
        always_assert(cinfo);
        if (cinfo->classGraph == c.graph()) return;
        cinfo->auxClassGraphs.withChildren.emplace(c.graph());
      };

      if (f.cls) {
        auto const cls = folly::get_default(index.classes, f.cls->name);
        always_assert(cls);
        if (cls->closureContextCls) {
          onCls(cls->closureContextCls);
        } else if (cls->closureDeclFunc) {
          onFunc(cls->closureDeclFunc);
        } else {
          onCls(f.cls->name);
        }
      } else {
        onFunc(f.name);
      }
    };

    auto ty = return_type_from_constraints(
      f,
      [&] (SString name) -> Optional<res::Class> {
        if (auto const ci = folly::get_default(index.classInfos, name)) {
          auto const c = res::Class::get(*ci);
          assertx(c.isComplete());
          ensure(c);
          return c;
        }
        return std::nullopt;
      },
      [&] () -> Optional<Type> {
        if (!f.cls) return std::nullopt;
        auto const& cls = [&] () -> const php::Class& {
          if (!f.cls->closureContextCls) return *f.cls;
          auto const c = folly::get_default(index.classes, f.cls->closureContextCls);
          always_assert_flog(
            c,
            "When processing return-type for {}, "
            "tried to access missing class {}",
            func_fullname(f),
            f.cls->closureContextCls
          );
          return *c;
        }();
        if (cls.attrs & AttrTrait) return std::nullopt;
        auto const c = res::Class::get(cls.name);
        assertx(c.isComplete());
        return subCls(c, true);
      }
    );
    FTRACE(3, "Initial return type for {}: {}\n", func_fullname(f), show(ty));
    return serialize_classes(std::move(ty));
  }

  static void set_bad_initial_prop_values(const LocalIndex& index,
                                          php::Class& cls,
                                          ClassInfo2& cinfo) {
    Trace::Bump _{
      Trace::hhbbc_index, kSystemLibBump, is_systemlib_part(cls.unit)
    };

    assertx(cinfo.hasBadInitialPropValues);

    auto const isClosure = is_closure(cls);

    cinfo.hasBadInitialPropValues = false;
    for (auto& prop : cls.properties) {
      assertx(!(prop.attrs & AttrInitialSatisfiesTC));

      // Check whether the property's initial value satisfies it's
      // type-hint.
      auto const initialSatisfies = [&] {
        if (isClosure) return true;
        if (is_used_trait(cls)) return false;

        // Any property with an unresolved type-constraint here might
        // fatal when we initialize the class.
        for (auto const& tc : prop.typeConstraints.range()) {
          if (tc.isUnresolved()) return false;
        }

        if (prop.attrs & (AttrSystemInitialValue | AttrLateInit)) return true;

        auto const initial = from_cell(prop.val);
        if (initial.subtypeOf(BUninit)) return false;

        auto const make_type = [&] (const TypeConstraint& tc) {
          auto lookup = type_from_constraint(
            tc,
            initial,
            [&] (SString name) -> Optional<res::Class> {
              if (auto const ci = folly::get_default(index.classInfos, name)) {
                auto const c = res::Class::get(*ci);
                assertx(c.isComplete());
                return c;
              }
              return std::nullopt;
            },
            [&] () -> Optional<Type> {
              auto const& ctx = [&] () -> const php::Class& {
                if (!cls.closureContextCls) return cls;
                auto const c =
                  folly::get_default(index.classes, cls.closureContextCls);
                always_assert_flog(
                  c,
                  "When processing bad initial prop values for {}, "
                  "tried to access missing class {}",
                  cls.name,
                  cls.closureContextCls
                );
                return *c;
              }();
              if (ctx.attrs & AttrTrait) return std::nullopt;
              auto const c = res::Class::get(ctx.name);
              assertx(c.isComplete());
              return subCls(c, true);
            }
          );
          return unctx(std::move(lookup.lower));
        };

        for (auto const& tc : prop.typeConstraints.range()) {
          if (!initial.subtypeOf(make_type(tc))) return false;
        }
        return true;
      }();

      if (initialSatisfies) {
        attribute_setter(prop.attrs, true, AttrInitialSatisfiesTC);
      } else {
        cinfo.hasBadInitialPropValues = true;
      }
    }
  }
};

/*
 * "Fixups" a php::Unit by removing specified funcs from it, and
 * adding specified classes. This is needed to add closures created
 * from trait flattening into their associated units. While we're
 * doing this, we also remove redundant meth caller funcs here
 * (because it's convenient).
 */
struct UnitFixupJob {
  static std::string name() { return "hhbbc-unit-fixup"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
  }
  static void fini() {}

  static std::unique_ptr<php::Unit> run(std::unique_ptr<php::Unit> unit,
                                        const InitTypesMetadata::Fixup& fixup) {
    SCOPE_ASSERT_DETAIL("unit") { return unit->filename->toCppString(); };

    if (!fixup.removeFunc.empty()) {
      // If we want to remove a func, it should be in this unit.
      auto DEBUG_ONLY erased = false;
      unit->funcs.erase(
        std::remove_if(
          begin(unit->funcs),
          end(unit->funcs),
          [&] (SString func) {
            // This is a kinda dumb O(N^2) algorithm, but these lists
            // are typicaly size 1.
            auto const erase = std::any_of(
              begin(fixup.removeFunc),
              end(fixup.removeFunc),
              [&] (SString remove) { return remove == func; }
            );
            if (erase) erased = true;
            return erase;
          }
        ),
        end(unit->funcs)
      );
      assertx(erased);
    }

    auto const before = unit->classes.size();
    unit->classes.insert(
      end(unit->classes),
      begin(fixup.addClass),
      end(fixup.addClass)
    );
    // Only sort the newly added classes. The order of the existing
    // classes is visible to programs.
    std::sort(
      begin(unit->classes) + before,
      end(unit->classes),
      string_data_lt_type{}
    );
    always_assert(
      std::adjacent_find(
        begin(unit->classes), end(unit->classes),
        string_data_tsame{}) == end(unit->classes)
    );
    return unit;
  }
};

/*
 * BuildSubclassListJob produces name-only func family entries. This
 * job merges entries for the same name into one.
 */
struct AggregateNameOnlyJob: public BuildSubclassListJob {
  static std::string name() { return "hhbbc-aggregate-name-only"; }

  struct OutputMeta {
    std::vector<std::vector<FuncFamily2::Id>> newFuncFamilyIds;
    std::vector<FuncFamilyEntry> nameOnly;
    template <typename SerDe> void serde(SerDe& sd) {
      ScopedStringDataIndexer _;
      sd(newFuncFamilyIds)
        (nameOnly)
        ;
    }
  };
  using Output = Multi<
    Variadic<FuncFamilyGroup>,
    OutputMeta
  >;

  static Output
  run(std::vector<std::pair<SString, std::vector<FuncFamilyEntry>>> allEntries,
      Variadic<FuncFamilyGroup> funcFamilies) {
    LocalIndex index;

    for (auto& group : funcFamilies.vals) {
      for (auto& ff : group.m_ffs) {
        auto const id = ff->m_id;
        // We could have multiple groups which contain the same
        // FuncFamily, so don't assert uniqueness here. We'll just
        // take the first one we see (they should all be equivalent).
        index.funcFamilies.emplace(id, std::move(ff));
      }
    }

    OutputMeta meta;

    for (auto const& [name, entries] : allEntries) {
      Data::MethInfo info;
      info.complete = false;
      info.regularComplete = false;

      for (auto const& entry : entries) {
        auto entryInfo = meth_info_from_func_family_entry(index, entry);
        for (auto const& meth : entryInfo.regularMeths) {
          if (info.regularMeths.contains(meth)) continue;
          info.regularMeths.emplace(meth);
          info.nonRegularPrivateMeths.erase(meth);
          info.nonRegularMeths.erase(meth);
        }
        for (auto const& meth : entryInfo.nonRegularPrivateMeths) {
          if (info.regularMeths.contains(meth) ||
              info.nonRegularPrivateMeths.contains(meth)) {
            continue;
          }
          info.nonRegularPrivateMeths.emplace(meth);
          info.nonRegularMeths.erase(meth);
        }
        for (auto const& meth : entryInfo.nonRegularMeths) {
          if (info.regularMeths.contains(meth) ||
              info.nonRegularPrivateMeths.contains(meth) ||
              info.nonRegularMeths.contains(meth)) {
            continue;
          }
          info.nonRegularMeths.emplace(meth);
        }

        if (entryInfo.allStatic) {
          if (!info.allStatic) {
            info.allStatic = std::move(*entryInfo.allStatic);
          } else {
            *info.allStatic |= *entryInfo.allStatic;
          }
        }
        if (entryInfo.regularStatic) {
          if (!info.regularStatic) {
            info.regularStatic = std::move(*entryInfo.regularStatic);
          } else {
            *info.regularStatic |= *entryInfo.regularStatic;
          }
        }
      }

      meta.nameOnly.emplace_back(
        make_method_family_entry(index, name, std::move(info))
      );
    }

    Variadic<FuncFamilyGroup> funcFamilyGroups;
    group_func_families(index, funcFamilyGroups.vals, meta.newFuncFamilyIds);

    return std::make_tuple(
      std::move(funcFamilyGroups),
      std::move(meta)
    );
  }
};

Job<InitTypesJob> s_initTypesJob;
Job<UnitFixupJob> s_unitFixupJob;
Job<AggregateNameOnlyJob> s_aggregateNameOnlyJob;

// Initialize return-types, fixup units, and aggregate name-only
// func-families all at once.
void init_types(IndexData& index, InitTypesMetadata meta) {
  trace_time tracer{"init types", index.sample};

  constexpr size_t kTypesBucketSize = 2000;
  constexpr size_t kFixupsBucketSize = 3000;
  constexpr size_t kAggregateBucketSize = 3000;

  TSStringToOneT<std::vector<SString>> classAliases;

  auto typeBuckets = consistently_bucketize(
    [&] {
      // Temporarily suppress case collision logging
      auto oldLogLevel = Cfg::Eval::LogTsameCollisions;
      Cfg::Eval::LogTsameCollisions = 0;
      SCOPE_EXIT { Cfg::Eval::LogTsameCollisions = oldLogLevel; };

      std::vector<SString> roots;
      roots.reserve(meta.classes.size() + meta.funcs.size());
      for (auto const& [name, _] : meta.classes) {
        // Ignore closures, they'll be processed alongside their
        // owning class/func.
        if (is_closure_name(name)) continue;
        roots.emplace_back(name);
      }
      for (auto const& [name, _] : meta.funcs) {
        // A class and a func could have the same name. Avoid
        // duplicates. If we do have a name collision it just means
        // the func and class will be assigned to the same bucket. We
        // do, however, need to record the alias to disambiguate in a
        // few cases.
        if (meta.classes.contains(name)) {
          classAliases[name].emplace_back(name);
          continue;
        }
        roots.emplace_back(name);
      }
      return roots;
    }(),
    kTypesBucketSize
  );

  auto fixupBuckets = consistently_bucketize(
    [&] {
      std::vector<SString> sorted;
      sorted.reserve(meta.fixups.size());
      for (auto& [unit, _] : meta.fixups) sorted.emplace_back(unit);
      std::sort(sorted.begin(), sorted.end(), string_data_lt{});
      return sorted;
    }(),
    kFixupsBucketSize
  );

  auto aggregateBuckets = consistently_bucketize(
    [&] {
      std::vector<SString> sorted;
      sorted.reserve(meta.nameOnlyFF.size());
      for (auto const& [name, entries] : meta.nameOnlyFF) {
        if (entries.size() <= 1) {
          // If there's only one entry for a name, there's nothing to
          // aggregate, and can be inserted directly as the final
          // result.
          always_assert(
            index.nameOnlyMethodFamilies.emplace(name, entries[0]).second
          );
          continue;
        }
        // Otherwise insert a dummy entry. This will let us update the
        // entry later from multiple threads without having to mutate
        // the map.
        always_assert(
          index.nameOnlyMethodFamilies.emplace(name, FuncFamilyEntry{}).second
        );
        sorted.emplace_back(name);
      }
      std::sort(begin(sorted), end(sorted), string_data_lt{});
      return sorted;
    }(),
    kAggregateBucketSize
  );

  // We want to avoid updating any Index data-structures until after
  // all jobs have read their inputs. We use the latch to block tasks
  // until all tasks have passed the point of reading their inputs.
  CoroLatch typesLatch{typeBuckets.size()};

  auto const runTypes = [&] (std::vector<SString> work) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    if (work.empty()) {
      typesLatch.count_down();
      co_return;
    }

    std::vector<UniquePtrRef<php::Class>> classes;
    std::vector<UniquePtrRef<ClassInfo2>> cinfos;
    std::vector<UniquePtrRef<php::Func>> funcs;
    std::vector<UniquePtrRef<FuncInfo2>> finfos;
    std::vector<UniquePtrRef<ClassInfo2>> cinfoDeps;

    TSStringSet roots;
    std::vector<SString> classNames;
    std::vector<SString> funcNames;

    // Closures are not part of the worklist. For classes this is fine
    // since they belong to the owning class and will be processed as
    // part of them. For funcs, however, they are stored
    // separately. So add any func closures to the worklist now (we do
    // this after bucketizing to ensure a func and it's closures
    // always end up on the same worker).
    auto const addClosures = [&] (SString f) {
      auto const clos = folly::get_ptr(index.funcToClosures, f);
      if (!clos) return;
      for (auto const n : *clos) work.emplace_back(n);
    };

    auto const beforeWorkSize = work.size();
    for (size_t i = 0, size = beforeWorkSize; i < size; ++i) {
      auto const w = work[i];
      if (meta.funcs.contains(w)) {
        addClosures(w);
      } else if (auto const aliases = folly::get_ptr(classAliases, w)) {
        for (auto const a : *aliases) addClosures(a);
      }
    }
    std::sort(
      begin(work) + beforeWorkSize,
      end(work),
      string_data_lt_type{}
    );

    roots.reserve(work.size());
    classNames.reserve(work.size());

    for (auto const w : work) {
      if (meta.classes.contains(w)) {
        always_assert(roots.emplace(w).second);
        classNames.emplace_back(w);
        if (auto const aliases = folly::get_ptr(classAliases, w)) {
          for (auto const a : *aliases) funcNames.emplace_back(a);
        }
      } else if (meta.funcs.contains(w)) {
        funcNames.emplace_back(w);
      }
    }

    // Add a dependency to the job. A class is a dependency if it
    // shows up in a class' type-hints, or if it's a potential
    // reg-only equivalent.
    auto const addDep = [&] (SString dep, bool addEquiv) {
      if (!meta.classes.contains(dep) || roots.contains(dep)) return;
      cinfoDeps.emplace_back(index.classInfoRefs.at(dep));
      if (!addEquiv) return;
      if (auto const cls = folly::get_ptr(meta.classes, dep)) {
        for (auto const d : cls->candidateRegOnlyEquivs) {
          if (!meta.classes.contains(d) || roots.contains(d)) continue;
          cinfoDeps.emplace_back(index.classInfoRefs.at(d));
        }
      }
    };

    auto const addFuncDeps = [&] (SString f) {
      auto const func = folly::get_ptr(meta.funcs, f);
      always_assert(func);
      funcs.emplace_back(index.funcRefs.at(f));
      finfos.emplace_back(index.funcInfoRefs.at(f));
      for (auto const d : func->deps) addDep(d, true);
    };

    for (auto const w : work) {
      if (auto const cls = folly::get_ptr(meta.classes, w)) {
        classes.emplace_back(index.classRefs.at(w));
        cinfos.emplace_back(index.classInfoRefs.at(w));
        for (auto const d : cls->deps) addDep(d, true);
        for (auto const d : cls->candidateRegOnlyEquivs) addDep(d, false);
        if (auto const aliases = folly::get_ptr(classAliases, w)) {
          for (auto const a : *aliases) addFuncDeps(a);
        }
      } else if (meta.funcs.contains(w)) {
        addFuncDeps(w);
      }
    }
    addDep(s_Awaitable.get(), true);
    addDep(s_AsyncGenerator.get(), true);
    addDep(s_Generator.get(), true);

    // Record that we've read our inputs
    typesLatch.count_down();

    std::sort(begin(cinfoDeps), end(cinfoDeps));
    cinfoDeps.erase(
      std::unique(begin(cinfoDeps), end(cinfoDeps)),
      end(cinfoDeps)
    );

    auto config = co_await index.configRef->getCopy();
    auto metadata = make_exec_metadata("init types", work[0]->toCppString());

    auto results = co_await
      index.client->exec(
        s_initTypesJob,
        std::move(config),
        singleton_vec(
          std::make_tuple(
            std::move(classes),
            std::move(cinfos),
            std::move(funcs),
            std::move(finfos),
            std::move(cinfoDeps)
          )
        ),
        std::move(metadata)
      );
    assertx(results.size() == 1);
    auto& [classRefs, cinfoRefs, funcRefs, finfoRefs] = results[0];
    assertx(classRefs.size() == classNames.size());
    assertx(cinfoRefs.size() == classNames.size());
    assertx(funcRefs.size() == funcNames.size());
    assertx(finfoRefs.size() == funcNames.size());

    // Wait for all tasks to finish reading from the Index ref tables
    // before starting to overwrite them.
    co_await typesLatch.wait();

    for (size_t i = 0, size = classNames.size(); i < size; ++i) {
      auto const name = classNames[i];
      index.classRefs.at(name) = std::move(classRefs[i]);
      index.classInfoRefs.at(name) = std::move(cinfoRefs[i]);
    }
    for (size_t i = 0, size = funcNames.size(); i < size; ++i) {
      auto const name = funcNames[i];
      index.funcRefs.at(name) = std::move(funcRefs[i]);
      index.funcInfoRefs.at(name) = std::move(finfoRefs[i]);
    }

    co_return;
  };

  auto const runFixups = [&] (std::vector<SString> units) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    if (units.empty()) co_return;

    std::vector<InitTypesMetadata::Fixup> fixups;

    // Gather up the fixups and ensure a deterministic ordering.
    fixups.reserve(units.size());
    for (auto const unit : units) {
      auto f = std::move(meta.fixups.at(unit));
      assertx(!f.addClass.empty() || !f.removeFunc.empty());
      std::sort(f.addClass.begin(), f.addClass.end(), string_data_lt_type{});
      std::sort(f.removeFunc.begin(), f.removeFunc.end(), string_data_lt{});
      fixups.emplace_back(std::move(f));
    }
    auto fixupRefs = co_await index.client->storeMulti(std::move(fixups));
    assertx(fixupRefs.size() == units.size());

    std::vector<
      std::tuple<UniquePtrRef<php::Unit>, Ref<InitTypesMetadata::Fixup>>
    > inputs;
    inputs.reserve(units.size());

    for (size_t i = 0, size = units.size(); i < size; ++i) {
      inputs.emplace_back(
        index.unitRefs.at(units[i]),
        std::move(fixupRefs[i])
      );
    }

    auto metadata = make_exec_metadata("fixup units", units[0]->toCppString());
    auto config = co_await index.configRef->getCopy();
    auto outputs = co_await index.client->exec(
      s_unitFixupJob,
      std::move(config),
      std::move(inputs),
      std::move(metadata)
    );
    assertx(outputs.size() == units.size());

    // Every unit is already in the Index table, so we can overwrite
    // them without locking.
    for (size_t i = 0, size = units.size(); i < size; ++i) {
      index.unitRefs.at(units[i]) = std::move(outputs[i]);
    }

    co_return;
  };

  struct AggregateUpdates {
    std::vector<
      std::pair<FuncFamily2::Id, Ref<FuncFamilyGroup>>
    > funcFamilies;
  };

  auto const runAggregate = [&] (std::vector<SString> names)
    -> coro::Task<AggregateUpdates> {
    co_await coro::co_reschedule_on_current_executor;

    if (names.empty()) co_return AggregateUpdates{};

    std::vector<std::pair<SString, std::vector<FuncFamilyEntry>>> entries;
    std::vector<Ref<FuncFamilyGroup>> funcFamilies;

    entries.reserve(names.size());
    // Extract out any func families the entries refer to, so they can
    // be provided to the job.
    for (auto const n : names) {
      auto& e = meta.nameOnlyFF.at(n);
      entries.emplace_back(n, std::move(e));
      for (auto const& entry : entries.back().second) {
        match(
          entry.m_meths,
          [&] (const FuncFamilyEntry::BothFF& e) {
            funcFamilies.emplace_back(index.funcFamilyRefs.at(e.m_ff));
          },
          [&] (const FuncFamilyEntry::FFAndSingle& e) {
            funcFamilies.emplace_back(index.funcFamilyRefs.at(e.m_ff));
          },
          [&] (const FuncFamilyEntry::FFAndNone& e) {
            funcFamilies.emplace_back(index.funcFamilyRefs.at(e.m_ff));
          },
          [&] (const FuncFamilyEntry::BothSingle&)    {},
          [&] (const FuncFamilyEntry::SingleAndNone&) {},
          [&] (const FuncFamilyEntry::None&)          {}
        );
      }
    }

    std::sort(begin(funcFamilies), end(funcFamilies));
    funcFamilies.erase(
      std::unique(begin(funcFamilies), end(funcFamilies)),
      end(funcFamilies)
    );

    auto [entriesRef, config] = co_await coro::collectAll(
      index.client->store(std::move(entries)),
      index.configRef->getCopy()
    );

    auto metadata = make_exec_metadata(
      "aggregate name-only",
      names[0]->toCppString()
    );

    auto results = co_await
      index.client->exec(
        s_aggregateNameOnlyJob,
        std::move(config),
        singleton_vec(
          std::make_tuple(std::move(entriesRef), std::move(funcFamilies))
        ),
        std::move(metadata)
      );
    assertx(results.size() == 1);
    auto& [ffRefs, outMetaRef] = results[0];

    auto outMeta = co_await index.client->load(std::move(outMetaRef));
    assertx(outMeta.newFuncFamilyIds.size() == ffRefs.size());
    assertx(outMeta.nameOnly.size() == names.size());

    // Update the dummy entries with the actual result.
    for (size_t i = 0, size = names.size(); i < size; ++i) {
      auto& old = index.nameOnlyMethodFamilies.at(names[i]);
      assertx(std::get_if<FuncFamilyEntry::None>(&old.m_meths));
      old = std::move(outMeta.nameOnly[i]);
    }

    AggregateUpdates updates;
    updates.funcFamilies.reserve(outMeta.newFuncFamilyIds.size());
    for (size_t i = 0, size = outMeta.newFuncFamilyIds.size(); i < size; ++i) {
      auto const ref = ffRefs[i];
      for (auto const& id : outMeta.newFuncFamilyIds[i]) {
        updates.funcFamilies.emplace_back(id, ref);
      }
    }

    co_return updates;
  };

  auto const runAggregateCombine = [&] (auto tasks) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    auto const updates = co_await coro::collectAllRange(std::move(tasks));

    for (auto const& u : updates) {
      for (auto const& [id, ref] : u.funcFamilies) {
        // The same FuncFamily can be grouped into multiple
        // different groups. Prefer the group that's smaller and
        // if they're the same size, use the one with the lowest
        // id to keep determinism.
        auto const& [existing, inserted] =
          index.funcFamilyRefs.emplace(id, ref);
        if (inserted) continue;
        if (existing->second.id().m_size < ref.id().m_size) continue;
        if (ref.id().m_size < existing->second.id().m_size) {
          existing->second = ref;
          continue;
        }
        if (existing->second.id() <= ref.id()) continue;
        existing->second = ref;
      }
    }

    co_return;
  };

  using namespace folly::gen;

  std::vector<coro::TaskWithExecutor<void>> tasks;
  tasks.reserve(typeBuckets.size() + fixupBuckets.size() + 1);

  // Temporarily suppress case collision logging
  auto oldTypeLogLevel = Cfg::Eval::LogTsameCollisions;
  Cfg::Eval::LogTsameCollisions = 0;
  SCOPE_EXIT {
    Cfg::Eval::LogTsameCollisions = oldTypeLogLevel;
  };

  for (auto& work : typeBuckets) {
    tasks.emplace_back(
      co_withExecutor(index.executor->sticky(), runTypes(std::move(work)))
    );
  }
  for (auto& work : fixupBuckets) {
    tasks.emplace_back(
      co_withExecutor(index.executor->sticky(), runFixups(std::move(work)))
    );
  }
  auto subTasks = from(aggregateBuckets)
    | move
    | map([&] (std::vector<SString>&& work) {
        return co_withExecutor(index.executor->sticky(), runAggregate(
          std::move(work)
        ));
      })
    | as<std::vector>();
  tasks.emplace_back(
    co_withExecutor(index.executor->sticky(), runAggregateCombine(
      std::move(subTasks)
    ))
  );

  coro::blockingWait(coro::collectAllRange(std::move(tasks)));
}

//////////////////////////////////////////////////////////////////////

struct BundleClassesJob {
  static std::string name() { return "hhbbc-bundle-classes"; }
  static void init(const Config& config) {
    process_init(config.o, config.gd, false);
    ClassGraph::init();
  }
  static void fini() { ClassGraph::destroy(); }

  static std::unique_ptr<ClassBundle> run(
    Variadic<std::unique_ptr<php::Class>> classes,
    Variadic<std::unique_ptr<ClassInfo2>> classInfos,
    Variadic<std::unique_ptr<php::ClassBytecode>> classBytecode,
    Variadic<std::unique_ptr<php::Func>> funcs,
    Variadic<std::unique_ptr<FuncInfo2>> funcInfos,
    Variadic<std::unique_ptr<php::FuncBytecode>> funcBytecode,
    Variadic<std::unique_ptr<php::Unit>> units,
    Variadic<std::unique_ptr<MethodsWithoutCInfo>> methInfos
  ) {
    auto bundle = std::make_unique<ClassBundle>(
      std::move(classes.vals),
      std::move(classInfos.vals),
      std::move(classBytecode.vals),
      std::move(funcs.vals),
      std::move(funcInfos.vals),
      std::move(funcBytecode.vals),
      std::move(units.vals),
      std::move(methInfos.vals)
    );
    return bundle;
  }
};

Job<BundleClassesJob> s_bundleClassesJob;

struct ClassBundleAssignments {
  struct Assignments {
    std::vector<SString> classes;
    std::vector<SString> funcs;
    std::vector<SString> units;
  };
  SStringToOneT<Assignments> assignments;
  std::vector<std::vector<SString>> buckets;
};

//////////////////////////////////////////////////////////////////////

/*
 * ClassBundler - Groups classes, functions, and units into bundles for
 * distributed whole-program analysis.
 *
 * PROBLEM:
 * Distributed analysis processes classes in parallel across multiple workers.
 * Classes with dependencies should ideally be analyzed together to minimize
 * remote data fetching and maximize cache locality. However, naively grouping
 * all interdependent classes can create bundles that are too large to
 * efficiently process.
 *
 * APPROACH:
 * ClassBundler uses a multi-phase algorithm to create balanced bundles:
 *
 * 1. Initial Setup (build_bundles):
 *    - Start with one bundle per unit (compilation unit)
 *    - Track dependencies between bundles based on unit predeps
 *    - Calculate bundle weights (sum of serialized sizes of classes/funcs)
 *
 * 2. Merge Single Predecessors:
 *    - Eagerly merge bundles that have exactly one predecessor
 *    - This handles simple linear dependency chains efficiently
 *
 * 3. Similarity-Based Merging (merge_nearest):
 *    - Use Approximate Nearest Neighbor (ANN) to find similar bundles
 *    - Similarity measured by Jaccard index of dependency sets
 *    - Multiple rounds with progressively relaxed similarity thresholds
 *    - Only merge if combined weight stays under kMaxBundleWeight
 *
 * 4. Directory-Based Grouping (merge_by_directory):
 *    - As a final pass, merge bundles from the same directory structure
 *    - Respects smaller limits (kMaxPathBundleWeight, kMaxPathBundleSize)
 *    - Helps group related code even when dependency analysis misses connections
 *
 * CONSTRAINTS:
 * - Maximum bundle weight: ~5MB (kMaxBundleWeight)
 * - Path-based merging limited to ~256KB and 1000 units
 * - Bundles merged only if they can fit within weight limits
 * - Algorithm continues until work reduction drops below 20% per round
 *
 * OUTPUT:
 * A mapping from bundle names to the classes, functions, and units they contain.
 * These assignments are used to partition work across distributed analysis workers.
 */
struct ClassBundler {
  explicit ClassBundler(const IndexData& i)
    : index{i}
  { build_bundles(); }

  SStringToOneT<ClassBundleAssignments::Assignments> operator()();
private:
  // Represents a bundle during the merging process
  struct Bundle {
    SString name;                     // Unit name (bundle identifier)
    size_t bundleIdx{std::numeric_limits<size_t>::max()};  // Stable index in bundles array
    size_t workIdx{std::numeric_limits<size_t>::max()};    // Index in current worklist
    size_t weight{0};                 // Sum of serialized sizes (bytes)
    size_t size{0};                   // Number of units in bundle
    Bundle* canonical{nullptr};       // For union-find of merged bundles
    std::vector<Bundle*> preds;       // Dependencies (units this depends on)
    std::vector<Bundle*> succs;       // Dependents (units that depend on this)
    std::vector<size_t> set;          // workIdx of neighbors (for ANN similarity)

    folly::AtomicIntrusiveLinkedListHook<Bundle> hook;
    folly::AtomicIntrusiveLinkedList<Bundle, &Bundle::hook> toMerge;
  };

  static constexpr size_t kMaxBundleWeight = 5*1024*1024;
  static constexpr size_t kBundleWeightSlack = 32*1024;

  static constexpr size_t kMaxPathBundleWeight = 256*1024;
  static constexpr size_t kMaxPathBundleSize = 1000;

  static constexpr double kMinBundleWorkReduction = 0.2;

  std::vector<std::unique_ptr<Bundle>> bundles;
  SStringToOneT<Bundle*> nameToBundle;

  std::vector<Bundle*> worklist;

  const IndexData& index;

  using ANN = ApproximateNearestNeighbor<uint32_t, uint32_t>;

  void build_bundles();
  void prepare_worklist();
  void merge_bundles(double);
  void merge_single_preds();
  void add_all(ANN&);
  void merge_nearest(ANN&, double);
  bool merge_by_directory(size_t);

  static bool can_fit(const Bundle& b1, const Bundle& b2) {
    return
      b1.weight <= kMaxBundleWeight &&
      b2.weight <= kMaxBundleWeight &&
      (b1.weight + b2.weight) <= (kMaxBundleWeight + kBundleWeightSlack);
  }

  static Bundle* canonicalize(Bundle* b) {
    if (!b) return nullptr;
    while (b->canonical != b) b = b->canonical;
    return b;
  }

  static void union_bundles(Bundle* b1, Bundle* b2) {
    auto const c1 = canonicalize(b1);
    if (!c1) return;
    auto const c2 = canonicalize(b2);
    if (!c2 || c1 == c2) return;
    if (c1->workIdx < c2->workIdx) {
      c2->canonical = c1;
    } else {
      c1->canonical = c2;
    }
  }

  static double jaccard(const Bundle&, const Bundle&);
};

/*
 * Main entry point: Runs the multi-phase bundling algorithm.
 *
 * Algorithm flow:
 * 1. Initialize worklist with all bundles
 * 2. Run multiple rounds of similarity-based merging with progressively
 *    relaxed thresholds (params array controls similarity bounds)
 * 3. Each round:
 *    - Merge bundles with single predecessors (easy wins)
 *    - Build ANN structure for similarity search
 *    - Find and merge similar bundles
 *    - Stop if work reduction drops below 20%
 * 4. Final cleanup: merge single preds and group by directory
 * 5. Collect results into final bundle assignments
 *
 * Returns: Map from bundle name -> {classes, funcs, units} in that bundle
 */
SStringToOneT<ClassBundleAssignments::Assignments> ClassBundler::operator()() {
  // Fixed seed for reproducibility across builds
  std::seed_seq seed{
    0x08063e940685c5c0, 0x300a9ddf20bef7e6,
    0x2dc449bd348bc372, 0x0639cfcb12ee5bf3,
    0x3b17aad01d3f4333, 0x02d0668f2608fedb,
    0x1eb61a4615f13e61, 0x0b63193e0a474be5,
    0x254665be1a8dc9d7, 0x2555912023d35fca,
    0x0760f9a005c66bf7, 0x308ebd302742054c,
    0x113336a30c1f717a, 0x01e4ee8833c98079,
    0x292a0eb90b952d49, 0x232362b63903bfcc,
    0x11111111faceb00c, 0x7777faceb00c7777
  };
  std::mt19937_64 prng{seed};

  worklist.clear();
  for (auto const& b : bundles) {
    assertx(b->canonical == b.get());
    b->workIdx = worklist.size();
    worklist.emplace_back(b.get());
  }

  struct Params {
    ANN::Bound lower;
    ANN::Bound higher;
  };
  const std::array params{
    Params{ ANN::Bound{0.25, 0.01}, ANN::Bound{0.75, 0.90} },
    Params{ ANN::Bound{0.25, 0.05}, ANN::Bound{0.50, 0.90} },
    Params{ ANN::Bound{0.25, 0.25}, ANN::Bound{0.40, 0.90} },
    Params{ ANN::Bound{0.25, 0.50}, ANN::Bound{0.35, 0.90} }
  };
  static_assert(std::tuple_size<decltype(params)>() > 0);

  for (size_t round = 0;; ++round) {
    if (worklist.empty()) break;
    auto const beforeWork = worklist.size();
    merge_single_preds();
    if (worklist.empty()) break;

    auto const& param = round < params.size()
      ? params[round]
      : params.back();
    ANN ann{prng, param.lower, param.higher, worklist.size(), worklist.size()};

    add_all(ann);
    merge_nearest(ann, param.lower.similarity);

    if (round+1 >= params.size()) {
      auto const reduction =
        double(beforeWork - worklist.size()) / beforeWork;
      if (reduction < kMinBundleWorkReduction) break;
    }
  }

  merge_single_preds();
  for (size_t d = 1; merge_by_directory(d); ++d) {}

  worklist.clear();

  SStringToOneT<ClassBundleAssignments::Assignments> out;
  out.reserve(bundles.size());

  for (auto const& [c, _] : index.classRefs) {
    auto const b = canonicalize(nameToBundle.at(index.classToUnit.at(c)));
    out[b->name].classes.emplace_back(c);
  }
  for (auto const& [f, _] : index.funcRefs) {
    auto const b = canonicalize(nameToBundle.at(index.funcToUnit.at(f)));
    out[b->name].funcs.emplace_back(f);
  }
  for (auto const& [u, _] : index.unitRefs) {
    auto const b = canonicalize(nameToBundle.at(u));
    out[b->name].units.emplace_back(u);
  }

  nameToBundle.clear();
  bundles.clear();

  for (auto& [_, a] : out) {
    std::sort(begin(a.classes), end(a.classes), string_data_lt_type{});
    std::sort(begin(a.funcs), end(a.funcs), string_data_lt_func{});
    std::sort(begin(a.units), end(a.units), string_data_lt{});
  }
  return out;
}

/*
 * Initialize bundles: one bundle per unit with dependency graph.
 *
 * Creates initial bundle state:
 * 1. Create one bundle per compilation unit
 * 2. Calculate bundle weights by summing serialized sizes of all
 *    classes, funcs, and units in each bundle
 * 3. Build predecessor/successor dependency graph based on unit predeps
 *    (units this bundle depends on / units that depend on this bundle)
 * 4. Sort bundles by name for stable ordering
 *
 * The dependency graph is crucial for similarity-based merging: bundles
 * with similar dependencies are more likely to benefit from being merged.
 */
void ClassBundler::build_bundles() {
  nameToBundle.reserve(index.unitRefs.size());
  bundles.reserve(index.unitRefs.size());

  for (auto const& [n, r] : index.unitRefs) {
    auto b = std::make_unique<Bundle>(n);
    b->weight += r.id().m_size;
    b->size = 1;
    b->canonical = b.get();
    nameToBundle.emplace(n, b.get());
    bundles.emplace_back(std::move(b));
  }
  always_assert(bundles.size() < std::numeric_limits<uint32_t>::max());

  std::sort(
    begin(bundles), end(bundles),
    [] (const std::unique_ptr<Bundle>& a,
        const std::unique_ptr<Bundle>& b) {
      return string_data_lt{}(a->name, b->name);
    }
  );
  for (size_t i = 0, size = bundles.size(); i < size; ++i) {
    bundles[i]->bundleIdx = i;
  }

  for (auto const& [c, r] : index.classRefs) {
    auto& b = nameToBundle.at(index.classToUnit.at(c));
    b->weight += r.id().m_size;
  }
  for (auto const& [c, r] : index.classInfoRefs) {
    auto& b = nameToBundle.at(index.classToUnit.at(c));
    b->weight += r.id().m_size;
  }
  for (auto const& [c, r] : index.classBytecodeRefs) {
    auto& b = nameToBundle.at(index.classToUnit.at(c));
    b->weight += r.id().m_size;
  }
  for (auto const& [c, r] : index.uninstantiableClsMethRefs) {
    auto& b = nameToBundle.at(index.classToUnit.at(c));
    b->weight += r.id().m_size;
  }

  for (auto const& [f, r] : index.funcRefs) {
    auto& b = nameToBundle.at(index.funcToUnit.at(f));
    b->weight += r.id().m_size;
  }
  for (auto const& [f, r] : index.funcInfoRefs) {
    auto& b = nameToBundle.at(index.funcToUnit.at(f));
    b->weight += r.id().m_size;
  }
  for (auto const& [f, r] : index.funcBytecodeRefs) {
    auto& b = nameToBundle.at(index.funcToUnit.at(f));
    b->weight += r.id().m_size;
  }

  struct PredList {
    folly::AtomicLinkedList<Bundle*> preds;
  };
  std::vector<PredList> predLists;
  predLists.resize(bundles.size());

  parallel::for_each(
    bundles,
    [&] (std::unique_ptr<Bundle>& bundle) {
      auto const add = [&] (SString n) {
        if (n == bundle->name) return;
        auto& b = nameToBundle.at(n);
        bundle->succs.emplace_back(b);
        assertx(b->bundleIdx < predLists.size());
        predLists[b->bundleIdx].preds.insertHead(bundle.get());
      };

      auto const onPredep = [&] (const SStringSet* predeps) {
        if (!predeps) return;
        for (auto const d : *predeps) {
          if (auto const u = folly::get_default(index.classToUnit, d))     add(u);
          if (auto const u = folly::get_default(index.funcToUnit, d))      add(u);
          if (auto const u = folly::get_default(index.typeAliasToUnit, d)) add(u);
          if (auto const u = folly::get_ptr(index.constantToUnit, d))      add(u->first);
        }
      };
      onPredep(folly::get_ptr(index.unitCInitPredeps, bundle->name));
      onPredep(folly::get_ptr(index.unitPredeps, bundle->name));
    }
  );

  parallel::for_each(
    bundles,
    [&] (std::unique_ptr<Bundle>& bundle) {
      std::sort(
        begin(bundle->succs), end(bundle->succs),
        [] (const Bundle* a, const Bundle* b) {
          return string_data_lt{}(a->name, b->name);
        }
      );
      bundle->succs.erase(
        std::unique(begin(bundle->succs), end(bundle->succs)),
        end(bundle->succs)
      );

      assertx(bundle->bundleIdx < predLists.size());
      predLists[bundle->bundleIdx].preds.sweepOnce(
        [&] (Bundle* b) { bundle->preds.emplace_back(b); }
      );
      std::sort(
        begin(bundle->preds), end(bundle->preds),
        [] (const Bundle* a, const Bundle* b) {
          return string_data_lt{}(a->name, b->name);
        }
      );

      bundle->succs.shrink_to_fit();
      bundle->preds.shrink_to_fit();
    }
  );
}

/*
 * Calculate Jaccard similarity between two bundles.
 *
 * Jaccard index = |intersection| / |union|
 *              = |A  B| / |A  B|
 *              = matches / (size(A) + size(B) - matches)
 *
 * The similarity is measured on the bundles' dependency sets (stored in
 * bundle.set), which contain the workIdx of neighboring bundles (both
 * predecessors and successors). High similarity (close to 1.0) means the
 * bundles have very similar dependencies and would likely benefit from
 * being merged to improve cache locality.
 *
 * This function assumes bundle.set is sorted, which is ensured by
 * prepare_worklist(). The implementation uses a two-pointer merge
 * technique for O(n+m) complexity.
 */
double ClassBundler::jaccard(const Bundle& a, const Bundle& b) {
  size_t matches = 0;
  auto it1 = begin(a.set);
  auto it2 = begin(b.set);
  auto const end1 = end(a.set);
  auto const end2 = end(b.set);
  while (it1 != end1 && it2 != end2) {
    if (*it1 < *it2) {
      ++it1;
    } else {
      if (!(*it2 < *it1)) {
        ++it1;
        ++matches;
      }
      ++it2;
    }
  }
  return matches/double(a.set.size()+b.set.size()-matches);
}

/*
 * Prepare worklist for the next merging phase.
 *
 * Filters and rebuilds the worklist to exclude:
 * - Bundles that have been merged (canonical != self)
 * - Bundles already at or over max weight limit
 *
 * For remaining bundles:
 * - Assigns new workIdx (position in current worklist)
 * - Builds the "neighbor set" (bundle.set) containing workIdx of all
 *   bundles this bundle depends on or is depended upon by
 * - Sorts and deduplicates the neighbor set
 *
 * The neighbor set is critical for ANN similarity search: it's the feature
 * vector used to find similar bundles. Bundles with overlapping neighbor
 * sets are considered similar and are candidates for merging.
 *
 * Called after each merge phase to compact the worklist and update indices.
 */
void ClassBundler::prepare_worklist() {
  worklist.erase(
    std::remove_if(
      begin(worklist), end(worklist),
      [&] (Bundle* b) {
        b->workIdx = std::numeric_limits<size_t>::max();
        if (b->canonical != b) return true;
        if (b->weight >= kMaxBundleWeight) return true;
        return false;
      }
    ),
    end(worklist)
  );

  parallel::gen(
    worklist.size(),
    [&] (size_t workIdx) {
      auto b = worklist[workIdx];
      b->workIdx = workIdx;
      b->set.clear();
      b->set.reserve(b->preds.size() + b->succs.size() + 1);
      b->set.emplace_back(b->workIdx);
      return nullptr;
    }
  );

  parallel::for_each(
    worklist,
    [&] (Bundle* b) {
      for (auto const p : b->preds) {
        auto const idx = canonicalize(p)->workIdx;
        if (idx >= worklist.size()) continue;
        b->set.emplace_back(idx);
      }
      for (auto const s : b->succs) {
        auto const idx = canonicalize(s)->workIdx;
        if (idx >= worklist.size()) continue;
        b->set.emplace_back(idx);
      }
      std::sort(begin(b->set), end(b->set));
      b->set.erase(
        std::unique(begin(b->set), end(b->set)),
        end(b->set)
      );
      b->set.shrink_to_fit();
    }
  );
}

/*
 * Execute bundle merges using union-find and update dependency graph.
 *
 * This is the core merging function that implements the union-find
 * structure via the canonical pointer. The process is parallelized:
 *
 * Phase 1 (parallel): Collect merge decisions
 * - For each bundle, if it's been marked to merge (canonical != self),
 *   add it to its canonical bundle's toMerge list
 *
 * Phase 2 (parallel): Perform merges
 * - For each canonical bundle:
 *   - If all merged bundles fit within kMaxBundleWeight, merge them all
 *   - Otherwise, greedily merge bundles by Jaccard similarity until
 *     weight limit is reached
 *   - Merged bundles get their preds/succs moved to the canonical bundle
 * - The tricky part: If the combined weight exceeds the limit, we sort
 *   the candidates by similarity and greedily pack as many as fit,
 *   rejecting those that would exceed the limit (they become canonical
 *   again via w->canonical = w)
 *
 * Phase 3 (parallel): Clean up dependency graph
 * - Canonicalize all predecessor/successor pointers (follow canonical chain)
 * - Remove self-edges (bundle depending on itself)
 * - Sort and deduplicate preds/succs lists
 * - Clear temporary data structures
 *
 * Finally, calls prepare_worklist() to rebuild the worklist for the next round.
 *
 * The lower_bound parameter enforces minimum similarity for greedy merging
 * when weight limits are exceeded.
 */
void ClassBundler::merge_bundles(double lower_bound) {
  parallel::for_each(
    worklist,
    [&] (Bundle* b) {
      assertx(b->workIdx < worklist.size());
      auto c = canonicalize(b);
      if (b != c) c->toMerge.insertHead(b);
    }
  );

  parallel::for_each(
    worklist,
    [&] (Bundle* b) {
      if (b->canonical != b) return;

      auto totalWeight = b->weight;
      TinyVector<Bundle*, 4> toMerge;
      b->toMerge.sweepOnce(
        [&] (Bundle* m) {
          assertx(canonicalize(m) == b);
          toMerge.emplace_back(m);
          totalWeight += m->weight;
        }
      );

      auto const merge = [&] (Bundle* m) {
        assertx(can_fit(*b, *m));

        b->preds.insert(
          end(b->preds),
          begin(m->preds),
          end(m->preds)
        );
        b->succs.insert(
          end(b->succs),
          begin(m->succs),
          end(m->succs)
        );
        assertx(b->size > 0);
        assertx(m->size > 0);
        b->size += m->size;
        assertx(b->weight > 0);
        assertx(m->weight > 0);
        b->weight += m->weight;

        m->size = 0;
        m->weight = 0;
        decltype(m->preds){}.swap(m->preds);
        decltype(m->succs){}.swap(m->succs);
        decltype(m->set){}.swap(m->set);
      };

      if (totalWeight <= kMaxBundleWeight) {
        for (auto const m : toMerge) merge(m);
        return;
      }

      std::sort(
        toMerge.begin(),
        toMerge.end(),
        [&] (const Bundle* m1, const Bundle* m2) {
          auto const j1 = jaccard(*b, *m1);
          auto const j2 = jaccard(*b, *m2);
          if (j1 != j2) return j1 > j2;
          if (m1->weight != m2->weight) return m1->weight < m2->weight;
          return m1->workIdx < m2->workIdx;
        }
      );

      for (auto const w : toMerge) {
        if (can_fit(*b, *w) && jaccard(*b, *w) >= lower_bound) {
          merge(w);
        } else {
          w->canonical = w;
        }
      }
    }
  );

  parallel::for_each(
    worklist,
    [&] (Bundle* b) {
      if (b->canonical != b) return;

      for (auto& s : b->succs) s = canonicalize(s);
      for (auto& p : b->preds) p = canonicalize(p);

      b->succs.erase(
        std::remove_if(
          begin(b->succs),
          end(b->succs),
          [&] (Bundle* s) { return s == b; }
        ),
        end(b->succs)
      );
      b->preds.erase(
        std::remove_if(
          begin(b->preds),
          end(b->preds),
          [&] (Bundle* p) { return p == b; }
        ),
        end(b->preds)
      );

      auto const compare = [] (const Bundle* a, const Bundle* b) {
        return a->bundleIdx < b->bundleIdx;
      };
      std::sort(begin(b->succs), end(b->succs), compare);
      std::sort(begin(b->preds), end(b->preds), compare);

      b->succs.erase(
        std::unique(begin(b->succs), end(b->succs)),
        end(b->succs)
      );
      b->preds.erase(
        std::unique(begin(b->preds), end(b->preds)),
        end(b->preds)
      );

      b->succs.shrink_to_fit();
      b->preds.shrink_to_fit();
      decltype(b->set){}.swap(b->set);
    }
  );

  prepare_worklist();
}

/*
 * Eagerly merge bundles that have exactly one predecessor.
 *
 * This is an optimization for the common case of linear dependency chains:
 * if bundle A is the only thing that depends on bundle B, we can safely
 * merge them without needing similarity analysis.
 *
 * Only merges if the combined weight stays under kMaxBundleWeight.
 *
 * This is called at the beginning of each round (before ANN search) to
 * quickly reduce the worklist size by handling the easy cases. It's also
 * called at the very end to merge any remaining single-predecessor bundles
 * after similarity-based merging is done.
 */
void ClassBundler::merge_single_preds() {
  for (auto const w : worklist) {
    if (w->preds.size() != 1) continue;
    if (!can_fit(*w, *w->preds[0])) continue;
    union_bundles(w, w->preds[0]);
  }
  merge_bundles(0.0);
}

/*
 * Add all canonical bundles to the ANN (Approximate Nearest Neighbor) index.
 *
 * The ANN structure is used for efficiently finding similar bundles based
 * on their dependency sets (bundle.set). This is a two-phase process:
 *
 * Phase 1 (parallel): preadd() - Reserves space in the ANN structure
 * for each bundle's workIdx
 *
 * Phase 2 (parallel by experiment): addByExperiment() - Adds each bundle's
 * feature vector (the sorted neighbor set) to the ANN structure
 *
 * The ANN maintains multiple "experiments" (hash tables with different random
 * seeds) to improve the probability of finding nearest neighbors. Adding
 * bundles separately by experiment allows parallelization while avoiding
 * race conditions within each experiment's data structure.
 *
 * Only canonical bundles are added (skips merged bundles where canonical != self).
 */
void ClassBundler::add_all(ANN& ann) {
  parallel::for_each(
    worklist,
    [&] (Bundle* bundle) {
      if (bundle->canonical != bundle) return;
      ann.preadd(bundle->workIdx);
    }
  );

  parallel::gen(
    ann.numExperiments(),
    [&] (size_t experiment) {
      for (auto const bundle : worklist) {
        if (bundle->canonical != bundle) continue;
        ann.addByExperiment(
          bundle->workIdx,
          experiment,
          begin(bundle->set),
          end(bundle->set)
        );
      }
      return nullptr;
    }
  );
}

/*
 * Find and merge nearest neighbors using ANN similarity search.
 *
 * For each canonical bundle:
 * 1. Query ANN for its nearest neighbor (bundle with most similar dependency set)
 * 2. Filter: only consider neighbors that fit within weight limit (can_fit check)
 * 3. Verify similarity: calculate exact Jaccard similarity and reject if below
 *    lower_similarity threshold
 * 4. Mark the bundle for merging with its nearest neighbor (via union_bundles)
 *
 * This is the core similarity-based merging step. The ANN structure makes
 * finding similar bundles efficient (approximate O(log n) rather than O(n^2)),
 * which is crucial at scale.
 *
 * Key insight: The ANN gives us *candidates* for similar bundles, but we verify
 * with exact Jaccard similarity before committing to the merge. This balances
 * performance (fast approximate search) with correctness (exact similarity check).
 *
 * After collecting all merge decisions, calls merge_bundles() to execute them
 * with the given lower_similarity as the minimum threshold.
 */
void ClassBundler::merge_nearest(ANN& ann, double lower_similarity) {
  std::vector<ANN::Counter> counters{parallel::num_threads};
  auto const mergeInto = parallel::gen(
    worklist.size(),
    [&] (size_t idx, size_t worker) -> Bundle* {
      auto const bundle = worklist[idx];
      if (bundle->canonical != bundle) return nullptr;

      auto const n = ann.nearest(
        bundle->workIdx,
        counters[worker],
        [&] (size_t id) {
          assertx(id < worklist.size());
          return can_fit(*bundle, *worklist[id]);
        }
      );
      if (!n) return nullptr;
      assertx(*n != bundle->workIdx);
      assertx(*n < worklist.size());
      auto const best = worklist[*n];

      assertx(best->workIdx == *n);
      auto const j = jaccard(*bundle, *best);
      return (j < lower_similarity) ? nullptr : best;
    }
  );

  assertx(mergeInto.size() == worklist.size());
  for (size_t i = 0, size = worklist.size(); i < size; ++i) {
    union_bundles(worklist[i], mergeInto[i]);
  }

  merge_bundles(lower_similarity);
}

/*
 * Merge bundles by directory structure as a final cleanup pass.
 *
 * After similarity-based merging is complete, this groups bundles that come
 * from the same directory path, even if they don't share dependencies. This
 * helps merge related code that the dependency analysis might have missed.
 *
 * Algorithm:
 * 1. Group bundles by directory prefix (strip 'depth' levels from the path)
 * 2. For each directory group:
 *    - Sort bundles by weight (smallest first) for better packing
 *    - Greedily merge bundles into groups that fit within stricter limits:
 *      * kMaxPathBundleWeight (~256KB) - much smaller than normal limit
 *      * kMaxPathBundleSize (1000 units) - prevents huge directory bundles
 * 3. When a bundle doesn't fit, start a new group within that directory
 *
 * The depth parameter controls how many directory levels to strip. Called
 * iteratively with increasing depth (1, 2, 3, ...) until maxDepth is reached
 * or no more bundles can be grouped.
 *
 * Returns true if this depth level found bundles to group (indicating we
 * should try a deeper level), false otherwise.
 *
 * Path-based merging uses conservative limits because directory proximity is
 * a weaker signal than dependency similarity.
 */
bool ClassBundler::merge_by_directory(size_t depth) {
  assertx(depth > 0);

  hphp_fast_map<std::string, std::vector<Bundle*>> dirs;
  size_t maxDepth = 0;
  for (auto const w : worklist) {
    if (w->weight >= kMaxPathBundleWeight) continue;
    if (w->size >= kMaxPathBundleSize) continue;
    auto n = w->name->toCppString();
    for (size_t i = 0; i < depth; ++i) {
      auto pos = n.find_last_of("/");
      if (pos == std::string::npos) pos = 0;
      n.erase(pos);
      if (n.empty()) break;
      maxDepth = std::max(maxDepth, i+1);
    }
    dirs[n].emplace_back(w);
  }

  std::vector<std::string> allDirs;
  allDirs.reserve(dirs.size());
  for (auto const& [dir, _] : dirs) allDirs.emplace_back(dir);

  parallel::for_each(
    allDirs,
    [&] (const std::string& dir) {
      auto& units = dirs.at(dir);

      std::sort(
        begin(units), end(units),
        [&] (const Bundle* a, const Bundle* b) {
          return
            std::tie(a->weight, a->workIdx) <
            std::tie(b->weight, b->workIdx);
        }
      );

      Bundle* first = nullptr;
      size_t totalWeight = 0;
      size_t totalCount = 0;
      for (auto const b : units) {
        if (totalWeight + b->weight > kMaxPathBundleWeight ||
            totalCount + b->size > kMaxPathBundleSize) {
          first = nullptr;
          totalWeight = 0;
          totalCount = 0;
        }
        totalWeight += b->weight;
        totalCount += b->size;
        if (!first) {
          first = b;
        } else {
          union_bundles(first, b);
        }
      }
    }
  );

  merge_bundles(0.0);
  return maxDepth == depth;
}

//////////////////////////////////////////////////////////////////////

ClassBundleAssignments find_class_bundles(IndexData& i) {
  trace_time tracer{"find class bundles"};
  tracer.ignore_client_stats();

  ClassBundler bundler{i};
  auto assignments = bundler();

  std::vector<SString> bundles;
  bundles.reserve(assignments.size());

  for (auto const& [n, m] : assignments) {
    bundles.emplace_back(n);
    for (auto const c : m.classes) {
      always_assert(i.classToBundle.emplace(c, n).second);
    }
    for (auto const f : m.funcs) {
      always_assert(i.funcToBundle.emplace(f, n).second);
    }
    for (auto const u : m.units) {
      always_assert(i.unitToBundle.emplace(u, n).second);
    }
  }

  constexpr size_t kBundleBucketSize = 500;

  return ClassBundleAssignments{
    std::move(assignments),
    consistently_bucketize(bundles, kBundleBucketSize)
  };
}

void bundle_classes(IndexData& index) {
  trace_time tracer{"bundle classes"};
  tracer.ignore_client_stats();

  auto assignments = find_class_bundles(index);

  using namespace folly::gen;

  struct Update {
    SString bundle;
    UniquePtrRef<ClassBundle> ref;
  };

  auto const run =
    [&] (std::vector<SString> bucket) -> coro::Task<std::vector<Update>> {
    co_await coro::co_reschedule_on_current_executor;

    if (bucket.empty()) co_return {};

    auto const make = [&] (SString b) {
      UniquePtrRefVec<php::Class> classes;
      UniquePtrRefVec<ClassInfo2> classInfos;
      UniquePtrRefVec<php::ClassBytecode> classBytecode;
      UniquePtrRefVec<php::Func> funcs;
      UniquePtrRefVec<FuncInfo2> funcInfos;
      UniquePtrRefVec<php::FuncBytecode> funcBytecode;
      UniquePtrRefVec<php::Unit> units;
      UniquePtrRefVec<MethodsWithoutCInfo> methInfos;

      auto const& a = assignments.assignments.at(b);
      for (auto const c : a.classes) {
        if (auto const r = folly::get_ptr(index.classRefs, c)) {
          classes.emplace_back(*r);
        }
        if (auto const r = folly::get_ptr(index.classInfoRefs, c)) {
          classInfos.emplace_back(*r);
        }
        if (auto const r = folly::get_ptr(index.classBytecodeRefs, c)) {
          classBytecode.emplace_back(*r);
        }
        if (auto const r = folly::get_ptr(index.uninstantiableClsMethRefs, c)) {
          methInfos.emplace_back(*r);
        }
      }
      for (auto const f : a.funcs) {
        if (auto const r = folly::get_ptr(index.funcRefs, f)) {
          funcs.emplace_back(*r);
        }
        if (auto const r = folly::get_ptr(index.funcInfoRefs, f)) {
          funcInfos.emplace_back(*r);
        }
        if (auto const r = folly::get_ptr(index.funcBytecodeRefs, f)) {
          funcBytecode.emplace_back(*r);
        }
      }
      for (auto const u : a.units) {
        if (auto const r = folly::get_ptr(index.unitRefs, u)) {
          units.emplace_back(*r);
        }
      }

      return std::make_tuple(
        std::move(classes),
        std::move(classInfos),
        std::move(classBytecode),
        std::move(funcs),
        std::move(funcInfos),
        std::move(funcBytecode),
        std::move(units),
        std::move(methInfos)
      );
    };
    auto inputs = from(bucket) | map(make) | as<std::vector>();

    auto metadata = make_exec_metadata(
      "bundle classes",
      bucket[0]->toCppString()
    );
    auto config = co_await index.configRef->getCopy();
    auto results = co_await index.client->exec(
      s_bundleClassesJob,
      std::move(config),
      std::move(inputs),
      std::move(metadata)
    );
    assertx(results.size() == bucket.size());

    std::vector<Update> out;
    out.reserve(results.size());
    for (size_t i = 0, size = results.size(); i < size; ++i) {
      out.emplace_back(bucket[i], results[i]);
    }
    co_return out;
  };

  trace_time tracer2{"bundle classes work", index.sample};

  auto const updates = coro::blockingWait(coro::collectAllRange(
    from(assignments.buckets)
      | move
      | map([&] (std::vector<SString> b) {
          return run(std::move(b)).scheduleOn(index.executor->sticky());
        })
      | as<std::vector>()
  ));

  for (auto const& u : updates) {
    for (auto const& [b, r] : u) {
      always_assert(index.bundleRefs.emplace(b, r).second);
    }
  }

  // Everything is bundled so we don't need refs to individual
  // elements.
  decltype(index.unitRefs){}.swap(index.unitRefs);
  decltype(index.classRefs){}.swap(index.classRefs);
  decltype(index.funcRefs){}.swap(index.funcRefs);
  decltype(index.classInfoRefs){}.swap(index.classInfoRefs);
  decltype(index.funcInfoRefs){}.swap(index.funcInfoRefs);
  decltype(index.classBytecodeRefs){}.swap(index.classBytecodeRefs);
  decltype(index.funcBytecodeRefs){}.swap(index.funcBytecodeRefs);
  decltype(index.uninstantiableClsMethRefs){}.swap(
    index.uninstantiableClsMethRefs
  );
}

//////////////////////////////////////////////////////////////////////

Index::Input::UnitMeta make_native_unit_meta(IndexData& index) {
  auto unit = make_native_unit();
  auto const name = unit->filename;

  std::vector<std::pair<SString, bool>> constants;
  constants.reserve(unit->constants.size());
  for (auto const& cns : unit->constants) {
    constants.emplace_back(cns->name, type(cns->val) == KindOfUninit);
  }

  auto unitRef = coro::blockingWait(index.client->store(std::move(unit)));
  Index::Input::UnitMeta meta{ std::move(unitRef), name };
  meta.constants = std::move(constants);
  return meta;
}

// Set up the async state, populate the (initial) table of
// extern-worker refs in the Index, and build some metadata needed for
// class flattening.
IndexFlattenMetadata make_remote(IndexData& index,
                                 Config config,
                                 Index::Input input,
                                 std::unique_ptr<TicketExecutor> executor,
                                 std::unique_ptr<Client> client,
                                 DisposeCallback dispose) {
  trace_time tracer("make remote");
  tracer.ignore_client_stats();

  assertx(input.classes.size() == input.classBC.size());
  assertx(input.funcs.size() == input.funcBC.size());

  index.executor = std::move(executor);
  index.client = std::move(client);
  index.disposeClient = std::move(dispose);

  // Kick off the storage of the global config. We'll start early so
  // it will (hopefully) be done before we need it.
  index.configRef = std::make_unique<CoroAsyncValue<Ref<Config>>>(
    [&index, config = std::move(config)] () mutable {
      return index.client->store(std::move(config));
    },
    index.executor->sticky()
  );

  // Create a fake unit to store native constants and add it as an
  // input.
  input.units.emplace_back(make_native_unit_meta(index));

  IndexFlattenMetadata flattenMeta;
  SStringToOneT<SString> methCallerUnits;

  flattenMeta.cls.reserve(input.classes.size());
  flattenMeta.allCls.reserve(input.classes.size());
  flattenMeta.allFuncs.reserve(input.funcs.size());

  // Add unit and class information to their appropriate tables. This
  // is also where we'll detect duplicate funcs and class names (which
  // should be caught earlier during parsing).
  for (auto& unit : input.units) {
    FTRACE(5, "unit {} -> {}\n", unit.name, unit.unit.id().toString());

    for (auto& typeMapping : unit.typeMappings) {
      auto const name = typeMapping.name;
      auto const isTypeAlias = typeMapping.isTypeAlias;
      always_assert_flog(
        flattenMeta.typeMappings.emplace(name, std::move(typeMapping)).second,
        "Duplicate type-mapping: {}",
        name
      );
      if (isTypeAlias) {
        always_assert(index.typeAliasToUnit.emplace(name, unit.name).second);
        index.unitsWithTypeAliases.emplace(unit.name);
      }
    }

    index.allUnits.emplace(unit.name);

    always_assert_flog(
      index.unitRefs.emplace(unit.name, std::move(unit.unit)).second,
      "Duplicate unit: {}",
      unit.name
    );

    always_assert(
      index.unitPredeps.emplace(unit.name, std::move(unit.predeps)).second
    );
    always_assert(
      index.unitCInitPredeps.emplace(
        unit.name,
        std::move(unit.cinitPredeps)
      ).second
    );

    for (auto const& [cnsName, hasInit] : unit.constants) {
      always_assert_flog(
        index.constantToUnit.emplace(
          cnsName,
          std::make_pair(unit.name, hasInit)
        ).second,
        "Duplicate constant: {}",
        cnsName
      );
    }
  }

  for (auto& cls : input.classes) {
    FTRACE(5, "class {} -> {}\n", cls.name, cls.cls.id().toString());
    always_assert_flog(
      index.classRefs.emplace(cls.name, std::move(cls.cls)).second,
      "Duplicate class: {}",
      cls.name
    );
    always_assert(index.classToUnit.emplace(cls.name, cls.unit).second);

    auto& meta = flattenMeta.cls[cls.name];
    if (cls.closureFunc) {
      assertx(cls.closures.empty());
      index.funcToClosures[cls.closureFunc].emplace(cls.name);
      index.closureToFunc.emplace(cls.name, cls.closureFunc);
      meta.isClosure = true;
    }
    index.classToClosures[cls.name].insert(
      begin(cls.closures),
      end(cls.closures)
    );
    for (auto const clo : cls.closures) {
      index.closureToClass.emplace(clo, cls.name);
    }

    meta.deps.insert(begin(cls.dependencies), end(cls.dependencies));
    meta.unresolvedTypes = std::move(cls.unresolvedTypes);
    meta.idx = flattenMeta.allCls.size();
    flattenMeta.allCls.emplace_back(cls.name);

    if (cls.has86init) index.classesWith86Inits.emplace(cls.name);
    if (!meta.isClosure) index.allClassesToAnalyze.emplace(cls.name);

    if (cls.typeMapping) {
      auto const name = cls.typeMapping->name;
      always_assert_flog(
        flattenMeta.typeMappings.emplace(
          name, std::move(*cls.typeMapping)
        ).second,
        "Duplicate type-mapping: {}",
        name
      );
    }
  }

  // Funcs have an additional wrinkle, however. A func might be a meth
  // caller. Meth callers are special in that they might be present
  // (with the same name) in multiple units. However only one "wins"
  // and is actually emitted in the repo. We detect that here and
  // select a winner. The "losing" meth callers will be actually
  // removed from their unit after class flattening.
  for (auto& func : input.funcs) {
    FTRACE(5, "func {} -> {}\n", func.name, func.func.id().toString());

    if (func.methCaller) {
      // If this meth caller a duplicate of one we've already seen?
      auto const [existing, emplaced] =
        methCallerUnits.emplace(func.name, func.unit);
      if (!emplaced) {
        // It is. The duplicate shouldn't be in the same unit,
        // however.
        always_assert_flog(
          existing->second != func.unit,
          "Duplicate meth-caller {} in same unit {}",
          func.name,
          func.unit
        );
        // The winner is the one with the unit with the "lesser"
        // name. This is completely arbitrary.
        if (string_data_lt{}(func.unit, existing->second)) {
          // This one wins. Schedule the older entry for deletion and
          // take over it's position in the map.
          FTRACE(
            4, "  meth caller {} from unit {} taking priority over unit {}",
            func.name, func.unit, existing->second
          );
          flattenMeta.unitDeletions[existing->second].emplace_back(func.name);
          existing->second = func.unit;
          index.funcRefs.at(func.name) = std::move(func.func);
          index.funcToUnit.at(func.name) = func.unit;
        } else {
          // This one loses. Schedule it for deletion.
          flattenMeta.unitDeletions[func.unit].emplace_back(func.name);
        }
        continue;
      }
      // It's not. Treat it like anything else.
    }

    // If not a meth caller, treat it like anything else.
    always_assert_flog(
      index.funcRefs.emplace(func.name, std::move(func.func)).second,
      "Duplicate func: {}",
      func.name
    );

    index.funcToUnit.emplace(func.name, func.unit);
    if (Constant::nameFromFuncName(func.name)) {
      index.constantInitFuncs.emplace(func.name);
    }
    index.allFuncs.emplace(func.name);

    auto& meta = flattenMeta.func[func.name];
    meta.unresolvedTypes = std::move(func.unresolvedTypes);

    flattenMeta.allFuncs.emplace_back(func.name);
  }

  for (auto& bc : input.classBC) {
    FTRACE(5, "class bytecode {} -> {}\n", bc.name, bc.bc.id().toString());

    always_assert_flog(
      index.classRefs.contains(bc.name),
      "Class bytecode for non-existent class {}",
      bc.name
    );
    always_assert_flog(
      index.classBytecodeRefs.emplace(bc.name, std::move(bc.bc)).second,
      "Duplicate class bytecode: {}",
      bc.name
    );
  }

  for (auto& bc : input.funcBC) {
    FTRACE(5, "func bytecode {} -> {}\n", bc.name, bc.bc.id().toString());

    always_assert_flog(
      index.funcRefs.contains(bc.name),
      "Func bytecode for non-existent func {}",
      bc.name
    );

    if (bc.methCaller) {
      // Only record this bytecode if it's associated meth-caller was
      // kept.
      auto const it = methCallerUnits.find(bc.name);
      always_assert_flog(
        it != end(methCallerUnits),
        "Bytecode for func {} is marked as meth-caller, "
        "but func is not a meth-caller",
        bc.name
      );
      auto const unit = it->second;
      if (bc.unit != unit) {
        FTRACE(
          4,
          "Bytecode for meth-caller func {} in unit {} "
          "skipped because the meth-caller was dropped\n",
          bc.name, bc.unit
        );
        continue;
      }
    } else {
      always_assert_flog(
        !methCallerUnits.contains(bc.name),
        "Bytecode for func {} is not marked as meth-caller, "
        "but func is a meth-caller",
        bc.name
      );
    }

    always_assert_flog(
      index.funcBytecodeRefs.emplace(bc.name, std::move(bc.bc)).second,
      "Duplicate func bytecode: {}",
      bc.name
    );
  }

  if constexpr (debug) {
    for (auto const f : special_builtins()) {
      always_assert_flog(
        index.funcRefs.contains(f) &&
        index.funcToUnit.contains(f),
        "{} is marked as a builtin, but does not exist",
        f
      );
    }

    for (auto const& [cns, unitAndInit] : index.constantToUnit) {
      if (!unitAndInit.second) continue;
      if (is_native_unit(unitAndInit.first)) continue;
      auto const initName = Constant::funcNameFromName(cns);
      always_assert_flog(
        index.funcRefs.contains(initName),
        "Constant {} is marked as having initialization func {}, "
        "but it does not exist",
        cns, initName
      );
    }
  }

  return flattenMeta;
}

//////////////////////////////////////////////////////////////////////

void remote_func_info_to_local(IndexData& index,
                               const php::Func& func,
                               FuncInfo2& rfinfo) {
  assertx(func.name == rfinfo.name);
  auto finfo = func_info(index, &func);
  assertx(finfo->returnTy.is(BInitCell));
  finfo->returnTy = std::move(rfinfo.inferred.returnTy);
  finfo->returnRefinements = rfinfo.inferred.returnRefinements;
  finfo->retParam = rfinfo.inferred.retParam;
  finfo->effectFree = rfinfo.inferred.effectFree;
  finfo->unusedParams = rfinfo.inferred.unusedParams;
}

// Convert the FuncInfo2s we loaded from extern-worker into their
// equivalent FuncInfos.
void make_func_infos_local(IndexData& index,
                           std::vector<std::unique_ptr<FuncInfo2>> remote) {
  trace_time tracer{"make func-infos local"};
  tracer.ignore_client_stats();

  parallel::for_each(
    remote,
    [&] (const std::unique_ptr<FuncInfo2>& rfinfo) {
      auto const it = index.funcs.find(rfinfo->name);
      always_assert_flog(
        it != end(index.funcs),
        "Func-info for {} has no associated php::Func in index",
        rfinfo->name
      );
      remote_func_info_to_local(index, *it->second, *rfinfo);
    }
  );
}

// Convert the ClassInfo2s we loaded from extern-worker into their
// equivalent ClassInfos (and store it in the Index).
void make_class_infos_local(
  IndexData& index,
  std::vector<std::unique_ptr<ClassInfo2>> remote,
  std::vector<std::unique_ptr<FuncFamily2>> funcFamilies
) {
  trace_time tracer{"make class-infos local"};
  tracer.ignore_client_stats();

  assertx(index.allClassInfos.empty());
  assertx(index.classInfo.empty());

  // First create a ClassInfo for each ClassInfo2. Since a ClassInfo
  // can refer to other ClassInfos, we can't do much more at this
  // stage.
  auto newCInfos = parallel::map(
    remote,
    [&] (const std::unique_ptr<ClassInfo2>& in) {
      std::vector<std::unique_ptr<ClassInfo>> out;

      auto const make = [&] (const ClassInfo2& cinfo) {
        auto c = std::make_unique<ClassInfo>();
        auto const it = index.classes.find(cinfo.name);
        always_assert_flog(
          it != end(index.classes),
          "Class-info for {} has no associated php::Class in index",
          cinfo.name
        );
        c->cls = it->second;
        out.emplace_back(std::move(c));
      };

      make(*in);
      for (auto const& clo : in->closures) make(*clo);
      return out;
    }
  );

  // Build table mapping name to ClassInfo.
  for (auto& cinfos : newCInfos) {
    for (auto& cinfo : cinfos) {
      always_assert(
        index.classInfo.emplace(cinfo->cls->name, cinfo.get()).second
      );
      index.allClassInfos.emplace_back(std::move(cinfo));
    }
  }
  newCInfos.clear();
  newCInfos.shrink_to_fit();
  index.allClassInfos.shrink_to_fit();

  // Set AttrNoOverride to true for all methods. If we determine it's
  // actually overridden below, we'll clear it.
  parallel::for_each(
    index.program->classes,
    [&] (std::unique_ptr<php::Class>& cls) {
      for (auto& m : cls->methods) {
        assertx(!(m->attrs & AttrNoOverride));
        if (is_special_method_name(m->name)) continue;
        attribute_setter(m->attrs, true, AttrNoOverride);
      }
      for (auto& clo : cls->closures) {
        assertx(clo->methods.size() == 1);
        auto& m = clo->methods[0];
        assertx(!(m->attrs & AttrNoOverride));
        assertx(!is_special_method_name(m->name));
        attribute_setter(m->attrs, true, AttrNoOverride);
      }
    }
  );

  auto const get = [&] (SString name) {
    auto const it = index.classInfo.find(name);
    always_assert_flog(
      it != end(index.classInfo),
      "Class-info for {} not found in index",
      name
    );
    return it->second;
  };

  struct FFState {
    explicit FFState(std::unique_ptr<FuncFamily2> ff) : m_ff{std::move(ff)} {}
    std::unique_ptr<FuncFamily2> m_ff;
    LockFreeLazyPtrNoDelete<FuncFamily> m_notExpanded;
    LockFreeLazyPtrNoDelete<FuncFamily> m_expanded;

    FuncFamily* notExpanded(IndexData& index) {
      return const_cast<FuncFamily*>(
        &m_notExpanded.get([&] { return make(index, false); })
      );
    }
    FuncFamily* expanded(IndexData& index) {
      return const_cast<FuncFamily*>(
        &m_expanded.get([&] { return make(index, true); })
      );
    }

    FuncFamily* make(IndexData& index, bool expanded) const {
      FuncFamily::PFuncVec funcs;
      funcs.reserve(
        m_ff->m_regular.size() +
        (expanded
         ? 0
         : (m_ff->m_nonRegularPrivate.size() + m_ff->m_nonRegular.size())
        )
      );

      for (auto const& m : m_ff->m_regular) {
        funcs.emplace_back(func_from_meth_ref(index, m), true);
      }
      for (auto const& m : m_ff->m_nonRegularPrivate) {
        funcs.emplace_back(func_from_meth_ref(index, m), true);
      }
      if (!expanded) {
        for (auto const& m : m_ff->m_nonRegular) {
          funcs.emplace_back(func_from_meth_ref(index, m), false);
        }
      }

      auto const extra = !expanded && !m_ff->m_nonRegular.empty() &&
        (m_ff->m_regular.size() + m_ff->m_nonRegularPrivate.size()) > 1;

      std::sort(
        begin(funcs), end(funcs),
        [] (FuncFamily::PossibleFunc a, const FuncFamily::PossibleFunc b) {
          if (a.inRegular() && !b.inRegular()) return true;
          if (!a.inRegular() && b.inRegular()) return false;
          return string_data_lt_type{}(a.ptr()->cls->name, b.ptr()->cls->name);
        }
      );
      funcs.shrink_to_fit();

      assertx(funcs.size() > 1);

      auto const convert = [&] (const FuncFamily2::StaticInfo& in) {
        FuncFamily::StaticInfo out;
        out.m_numInOut = in.m_numInOut;
        out.m_requiredCoeffects = in.m_requiredCoeffects;
        out.m_coeffectRules = in.m_coeffectRules;
        out.m_paramPreps = in.m_paramPreps;
        out.m_minNonVariadicParams = in.m_minNonVariadicParams;
        out.m_maxNonVariadicParams = in.m_maxNonVariadicParams;
        out.m_isReadonlyReturn = in.m_isReadonlyReturn;
        out.m_isReadonlyThis = in.m_isReadonlyThis;
        out.m_supportsAER = in.m_supportsAER;
        out.m_maybeReified = in.m_maybeReified;
        out.m_maybeCaresAboutDynCalls = in.m_maybeCaresAboutDynCalls;
        out.m_maybeBuiltin = in.m_maybeBuiltin;

        auto const it = index.funcFamilyStaticInfos.find(out);
        if (it != end(index.funcFamilyStaticInfos)) return it->first.get();
        return index.funcFamilyStaticInfos.insert(
          std::make_unique<FuncFamily::StaticInfo>(std::move(out)),
          false
        ).first->first.get();
      };

      auto newFuncFamily =
        std::make_unique<FuncFamily>(std::move(funcs), extra);

      always_assert(m_ff->m_allStatic);
      if (m_ff->m_regularStatic) {
        const FuncFamily::StaticInfo* reg = nullptr;
        if (expanded || extra) reg = convert(*m_ff->m_regularStatic);
        newFuncFamily->m_all.m_static =
          expanded ? reg : convert(*m_ff->m_allStatic);
        if (extra) {
          newFuncFamily->m_regular = std::make_unique<FuncFamily::Info>();
          newFuncFamily->m_regular->m_static = reg;
        }
      } else {
        newFuncFamily->m_all.m_static = convert(*m_ff->m_allStatic);
      }

      return index.funcFamilies.insert(
        std::move(newFuncFamily),
        false
      ).first->first.get();
    }
  };

  hphp_fast_map<FuncFamily2::Id, std::unique_ptr<FFState>> ffState;
  for (auto& ff : funcFamilies) {
    auto const id = ff->m_id;
    always_assert(
      ffState.emplace(
        id,
        std::make_unique<FFState>(std::move(ff))
      ).second
    );
  }
  funcFamilies.clear();

  std::mutex extraMethodLock;

  // Now that we can map name to ClassInfo, we can populate the rest
  // of the fields in each ClassInfo.
  parallel::for_each(
    remote,
    [&] (std::unique_ptr<ClassInfo2>& rcinfos) {
      auto const process = [&] (std::unique_ptr<ClassInfo2> rcinfo) {
        auto const cinfo = get(rcinfo->name);
        if (rcinfo->parent) cinfo->parent = get(rcinfo->parent);

        if (!(cinfo->cls->attrs & AttrNoExpandTrait)) {
          auto const traits = rcinfo->classGraph.usedTraits();
          cinfo->usedTraits.reserve(traits.size());
          for (auto const trait : traits) {
            cinfo->usedTraits.emplace_back(get(trait.name()));
          }
          cinfo->usedTraits.shrink_to_fit();
        }
        cinfo->traitProps = std::move(rcinfo->traitProps);

        cinfo->clsConstants.reserve(rcinfo->clsConstants.size());
        for (auto const& [name, cns] : rcinfo->clsConstants) {
          auto const it = index.classes.find(cns.idx.cls);
          always_assert_flog(
            it != end(index.classes),
            "php::Class for {} not found in index",
            name
          );
          cinfo->clsConstants.emplace(
            name,
            ClassInfo::ConstIndex { it->second, cns.idx.idx }
          );
        }

        for (size_t i = 0, size = cinfo->cls->constants.size(); i < size; ++i) {
          auto const& cns = cinfo->cls->constants[i];
          if (cns.kind != ConstModifierFlags::Kind::Value) continue;
          if (!cns.val.has_value())                    continue;
          if (cns.val->m_type != KindOfUninit)         continue;
          if (i >= cinfo->clsConstTypes.size()) {
            cinfo->clsConstTypes.resize(i+1, ClsConstInfo { TInitCell, 0 });
          }
          cinfo->clsConstTypes[i] = folly::get_default(
            rcinfo->inferred.clsConstantInfo,
            cns.name,
            ClsConstInfo { TInitCell, 0 }
          );
        }
        cinfo->clsConstTypes.shrink_to_fit();

        {
          std::vector<std::pair<SString, MethTabEntry>> methods;
          methods.reserve(cinfo->methods.size());
          for (auto const& [name, mte] : rcinfo->methods) {
            if (!(mte.attrs & AttrNoOverride)) {
              attribute_setter(
                func_from_meth_ref(index, mte.meth())->attrs,
                false,
                AttrNoOverride
              );
            }
            methods.emplace_back(name, mte);
          }
          std::sort(
            begin(methods), end(methods),
            [] (auto const& p1, auto const& p2) { return p1.first < p2.first; }
          );
          cinfo->methods.insert(
            folly::sorted_unique, begin(methods), end(methods)
          );
          cinfo->methods.shrink_to_fit();
        }

        cinfo->hasBadRedeclareProp = rcinfo->hasBadRedeclareProp;
        cinfo->hasBadInitialPropValues = rcinfo->hasBadInitialPropValues;
        cinfo->hasConstProp = rcinfo->hasConstProp;
        cinfo->hasReifiedParent = rcinfo->hasReifiedParent;
        cinfo->subHasConstProp = rcinfo->subHasConstProp;
        cinfo->isMocked = rcinfo->isMocked;
        cinfo->isSubMocked = rcinfo->isSubMocked;

        cinfo->classGraph = rcinfo->classGraph;
        cinfo->classGraph.setCInfo(*cinfo);

        auto const noOverride = [&] (SString name) {
          if (auto const mte = folly::get_ptr(cinfo->methods, name)) {
            return bool(mte->attrs & AttrNoOverride);
          }
          return true;
        };

        auto const noOverrideRegular = [&] (SString name) {
          if (auto const mte = folly::get_ptr(cinfo->methods, name)) {
            return mte->noOverrideRegular();
          }
          return true;
        };

        std::vector<std::pair<SString, FuncFamilyOrSingle>> entries;
        std::vector<std::pair<SString, FuncFamilyOrSingle>> aux;
        for (auto const& [name, entry] : rcinfo->methodFamilies) {
          assertx(!is_special_method_name(name));

          auto expanded = false;
          if (!cinfo->methods.contains(name)) {
            if (!(cinfo->cls->attrs & (AttrAbstract|AttrInterface))) continue;
            if (!cinfo->classGraph.mightHaveRegularSubclass()) continue;
            if (entry.m_regularIncomplete || entry.m_privateAncestor) continue;
            if (name == s_construct.get()) continue;
            expanded = true;
          } else if (noOverride(name)) {
            continue;
          }

          match(
            entry.m_meths,
            [&, name=name, &entry=entry] (const FuncFamilyEntry::BothFF& e) {
              auto const it = ffState.find(e.m_ff);
              assertx(it != end(ffState));
              auto const& state = it->second;

              if (expanded) {
                if (state->m_ff->m_regular.empty()) return;
                if (state->m_ff->m_regular.size() == 1) {
                  entries.emplace_back(
                    name,
                    FuncFamilyOrSingle{
                      func_from_meth_ref(index, state->m_ff->m_regular[0]),
                      false
                    }
                  );
                  return;
                }
                entries.emplace_back(
                  name,
                  FuncFamilyOrSingle{
                    state->expanded(index),
                    false
                  }
                );
                return;
              }

              entries.emplace_back(
                name,
                FuncFamilyOrSingle{
                  state->notExpanded(index),
                  entry.m_allIncomplete
                }
              );
            },
            [&, name=name, &entry=entry] (const FuncFamilyEntry::FFAndSingle& e) {
              if (expanded) {
                if (e.m_nonRegularPrivate) return;
                entries.emplace_back(
                  name,
                  FuncFamilyOrSingle{
                    func_from_meth_ref(index, e.m_regular),
                    false
                  }
                );
                return;
              }

              auto const it = ffState.find(e.m_ff);
              assertx(it != end(ffState));

              entries.emplace_back(
                name,
                FuncFamilyOrSingle{
                  it->second->notExpanded(index),
                  entry.m_allIncomplete
                }
              );
              if (noOverrideRegular(name)) return;
              aux.emplace_back(
                name,
                FuncFamilyOrSingle{
                  func_from_meth_ref(index, e.m_regular),
                  false
                }
              );
            },
            [&, name=name, &entry=entry] (const FuncFamilyEntry::FFAndNone& e) {
              if (expanded) return;
              auto const it = ffState.find(e.m_ff);
              assertx(it != end(ffState));

              entries.emplace_back(
                name,
                FuncFamilyOrSingle{
                  it->second->notExpanded(index),
                  entry.m_allIncomplete
                }
              );
              if (!noOverrideRegular(name)) {
                aux.emplace_back(name, FuncFamilyOrSingle{});
              }
            },
            [&, name=name, &entry=entry] (const FuncFamilyEntry::BothSingle& e) {
              if (expanded && e.m_nonRegularPrivate) return;
              entries.emplace_back(
                name,
                FuncFamilyOrSingle{
                  func_from_meth_ref(index, e.m_all),
                  !expanded && entry.m_allIncomplete
                }
              );
            },
            [&, name=name, &entry=entry] (const FuncFamilyEntry::SingleAndNone& e) {
              if (expanded) return;
              entries.emplace_back(
                name,
                FuncFamilyOrSingle{
                  func_from_meth_ref(index, e.m_all),
                  entry.m_allIncomplete
                }
              );
              if (!noOverrideRegular(name)) {
                aux.emplace_back(name, FuncFamilyOrSingle{});
              }
            },
            [&, &entry=entry] (const FuncFamilyEntry::None&) {
              assertx(entry.m_allIncomplete);
            }
          );
        }

        // Sort the lists of new entries, so we can insert them into the
        // method family maps (which are sorted_vector_maps) in bulk.
        std::sort(
          begin(entries), end(entries),
          [] (auto const& p1, auto const& p2) { return p1.first < p2.first; }
        );
        std::sort(
          begin(aux), end(aux),
          [] (auto const& p1, auto const& p2) { return p1.first < p2.first; }
        );
        if (!entries.empty()) {
          cinfo->methodFamilies.insert(
            folly::sorted_unique, begin(entries), end(entries)
          );
        }
        if (!aux.empty()) {
          cinfo->methodFamiliesAux.insert(
            folly::sorted_unique, begin(aux), end(aux)
          );
        }
        cinfo->methodFamilies.shrink_to_fit();
        cinfo->methodFamiliesAux.shrink_to_fit();

        if (!rcinfo->extraMethods.empty()) {
          // This is rare. Only happens with unflattened traits, so
          // taking a lock here is fine.
          std::lock_guard<std::mutex> _{extraMethodLock};
          auto& extra = index.classExtraMethodMap[cinfo->cls];
          for (auto const& meth : rcinfo->extraMethods) {
            extra.emplace(func_from_meth_ref(index, meth));
          }
        }

        // Build the FuncInfo for every method on this class. The
        // FuncInfos already have default types, so update them with the
        // type from the FuncInfo2. Any class types here will be
        // unresolved (will be resolved later).
        assertx(cinfo->cls->methods.size() == rcinfo->funcInfos.size());
        for (size_t i = 0, size = cinfo->cls->methods.size(); i < size; ++i) {
          auto& func = cinfo->cls->methods[i];
          auto& rfi = rcinfo->funcInfos[i];
          remote_func_info_to_local(index, *func, *rfi);
        }
      };

      for (auto& clo : rcinfos->closures) process(std::move(clo));
      process(std::move(rcinfos));
    }
  );

  remote.clear();
  remote.shrink_to_fit();

  for (auto const& [name, entry] : index.nameOnlyMethodFamilies) {
    match(
      entry.m_meths,
      [&, name=name] (const FuncFamilyEntry::BothFF& e) {
        auto const it = ffState.find(e.m_ff);
        assertx(it != end(ffState));

        FuncFamilyOrSingle f{ it->second->notExpanded(index), true };
        index.methodFamilies.emplace(
          name,
          IndexData::MethodFamilyEntry { f, f }
        );
      },
      [&, name=name] (const FuncFamilyEntry::FFAndSingle& e) {
        auto const it = ffState.find(e.m_ff);
        assertx(it != end(ffState));

        index.methodFamilies.emplace(
          name,
          IndexData::MethodFamilyEntry {
            FuncFamilyOrSingle {
              it->second->notExpanded(index),
              true
            },
            FuncFamilyOrSingle {
              func_from_meth_ref(index, e.m_regular),
              true
            }
          }
        );
      },
      [&, name=name] (const FuncFamilyEntry::FFAndNone& e) {
        auto const it = ffState.find(e.m_ff);
        assertx(it != end(ffState));

        index.methodFamilies.emplace(
          name,
          IndexData::MethodFamilyEntry {
            FuncFamilyOrSingle {
              it->second->notExpanded(index),
              true
            },
            FuncFamilyOrSingle {}
          }
        );
      },
      [&, name=name] (const FuncFamilyEntry::BothSingle& e) {
        FuncFamilyOrSingle f{ func_from_meth_ref(index, e.m_all), true };
        index.methodFamilies.emplace(
          name,
          IndexData::MethodFamilyEntry { f, f }
        );
      },
      [&, name=name] (const FuncFamilyEntry::SingleAndNone& e) {
        index.methodFamilies.emplace(
          name,
          IndexData::MethodFamilyEntry {
            FuncFamilyOrSingle {
              func_from_meth_ref(index, e.m_all),
              true
            },
            FuncFamilyOrSingle {}
          }
        );
      },
      [&] (const FuncFamilyEntry::None&) { always_assert(false); }
    );
  }

  ffState.clear();
  decltype(index.nameOnlyMethodFamilies){}.swap(index.nameOnlyMethodFamilies);

  // Now that all of the FuncFamilies have been created, generate the
  // back links from FuncInfo to their FuncFamilies.
  std::vector<FuncFamily*> work;
  work.reserve(index.funcFamilies.size());
  for (auto const& kv : index.funcFamilies) work.emplace_back(kv.first.get());

  // First calculate the needed capacity for each FuncInfo's family
  // list. We use this to presize the family list. This is superior
  // just pushing back and then shrinking the vectors, as that can
  // excessively fragment the heap.
  {
    std::vector<std::atomic<size_t>> capacities(index.nextFuncId);

    parallel::for_each(
      work,
      [&] (FuncFamily* ff) {
        for (auto const pf : ff->possibleFuncs()) {
          ++capacities[pf.ptr()->idx];
        }
      }
    );

    parallel::for_each(
      index.funcInfo,
      [&] (FuncInfo& fi) {
        if (!fi.func) return;
        fi.families.reserve(capacities[fi.func->idx]);
      }
    );
  }

  // Different threads can touch the same FuncInfo when adding to the
  // func family list, so use sharded locking scheme.
  std::array<std::mutex, 256> locks;
  parallel::for_each(
    work,
    [&] (FuncFamily* ff) {
      for (auto const pf : ff->possibleFuncs()) {
        auto finfo = func_info(index, pf.ptr());
        auto& lock = locks[pointer_hash<FuncInfo>{}(finfo) % locks.size()];
        std::lock_guard<std::mutex> _{lock};
        finfo->families.emplace_back(ff);
      }
    }
  );
}

// Switch to "local" mode, in which all calculations are expected to
// be done locally (not using extern-worker). This involves
// downloading everything out of extern-worker and converting it. To
// improve efficiency, we first aggregate many small(er) items into
// larger aggregate blobs in external workers, then download the
// larger blobs.
void make_local(IndexData& index) {
  trace_time tracer{"make local"};
  tracer.ignore_client_stats();

  using namespace folly::gen;

  // These aren't needed below so we can free them immediately.
  decltype(index.funcToClosures){}.swap(index.funcToClosures);
  decltype(index.classToClosures){}.swap(index.classToClosures);
  decltype(index.classesWith86Inits){}.swap(index.classesWith86Inits);
  decltype(index.classToUnit){}.swap(index.classToUnit);
  decltype(index.funcToUnit){}.swap(index.funcToUnit);
  decltype(index.constantToUnit){}.swap(index.constantToUnit);
  decltype(index.constantInitFuncs){}.swap(index.constantInitFuncs);
  decltype(index.unitsWithTypeAliases){}.swap(index.unitsWithTypeAliases);
  decltype(index.closureToFunc){}.swap(index.closureToFunc);
  decltype(index.closureToClass){}.swap(index.closureToClass);
  decltype(index.classToCnsBases){}.swap(index.classToCnsBases);
  decltype(index.allClassesToAnalyze){}.swap(index.allClassesToAnalyze);
  decltype(index.allFuncs){}.swap(index.allFuncs);
  decltype(index.extraMethods){}.swap(index.extraMethods);
  decltype(index.unitToOriginalUnits){}.swap(index.unitToOriginalUnits);
  decltype(index.classToBundle){}.swap(index.classToBundle);
  decltype(index.funcToBundle){}.swap(index.funcToBundle);
  decltype(index.unitToBundle){}.swap(index.unitToBundle);

  // Unlike other cases, we want to bound each bucket to roughly the
  // same total byte size (since ultimately we're going to download
  // everything).
  constexpr size_t kSizePerBucket = 256*1024*1024;

  // Most items have already been grouped together into bundles. So,
  // we only need to aggregate the bundles and func-families. Luckily
  // the names for either are disjoint, so we don't have to worry
  // about name collisions here.
  SStringToOneT<Ref<FuncFamilyGroup>> nameToFuncFamilyGroup;
  auto const& [items, totalSize] = [&] {
    std::vector<SString> items;
    size_t totalSize = 0;

    for (auto const& [name, ref] : index.bundleRefs) {
      totalSize += ref.id().m_size;
      items.emplace_back(name);
    }

    for (auto const& [_, ref] : index.funcFamilyRefs) {
      auto const name = makeStaticString(ref.id().toString());
      if (!nameToFuncFamilyGroup.emplace(name, ref).second) continue;
      totalSize += ref.id().m_size;
      items.emplace_back(name);
    }

    std::sort(begin(items), end(items), string_data_lt{});
    return std::make_pair(items, totalSize);
  }();

  // Back out the number of buckets we want from the total size of
  // everything and the target size of a bucket.
  auto const numBuckets = (totalSize + kSizePerBucket - 1) / kSizePerBucket;
  auto buckets = consistently_bucketize(
    items,
    (items.size() + numBuckets - 1) / numBuckets
  );

  // We're going to be downloading all bytecode, so to avoid wasted
  // memory, try to re-use identical bytecode.
  Optional<php::FuncBytecode::Reuser> reuser;
  reuser.emplace();
  php::FuncBytecode::s_reuser = reuser.get_pointer();
  SCOPE_EXIT { php::FuncBytecode::s_reuser = nullptr; };

  std::mutex lock;
  auto program = std::make_unique<php::Program>();

  // Index stores ClassInfos, not ClassInfo2s, so we need a place to
  // store them until we convert it.
  std::vector<std::unique_ptr<ClassInfo2>> remoteClassInfos;

  std::vector<std::unique_ptr<FuncInfo2>> remoteFuncInfos;

  std::vector<std::unique_ptr<MethodsWithoutCInfo>> remoteMethInfos;

  hphp_fast_set<FuncFamily2::Id> remoteFuncFamilyIds;
  std::vector<std::unique_ptr<FuncFamily2>> remoteFuncFamilies;

  struct Bundle {
    std::vector<std::unique_ptr<ClassBundle>> bundles;
    std::vector<FuncFamilyGroup> funcFamilies;
  };

  // Rate-limit downloads to avoid memory blowups
  folly::fibers::Semaphore semaphore{100};

  auto const run = [&] (std::vector<SString> chunks) -> coro::Task<void> {
    co_await coro::co_reschedule_on_current_executor;

    if (chunks.empty()) co_return;

    co_await semaphore.co_wait();

    std::vector<Ref<FuncFamilyGroup>> funcFamilies;
    std::vector<UniquePtrRef<ClassBundle>> bundles;

    for (auto const name : chunks) {
      if (auto const r = folly::get_ptr(nameToFuncFamilyGroup, name)) {
        funcFamilies.emplace_back(*r);
      }
      if (auto const r = folly::get_ptr(index.bundleRefs, name)) {
        bundles.emplace_back(*r);
      }
    }

    Bundle chunk;
    auto [b, ff] = co_await coro::collectAll(
      index.client->load(std::move(bundles)),
      index.client->load(std::move(funcFamilies))
    );

    chunk.bundles.insert(
      end(chunk.bundles),
      std::make_move_iterator(begin(b)),
      std::make_move_iterator(end(b))
    );
    chunk.funcFamilies.insert(
      end(chunk.funcFamilies),
      std::make_move_iterator(begin(ff)),
      std::make_move_iterator(end(ff))
    );

    for (auto& bundle : chunk.bundles) {
      always_assert(bundle->classes.size() == bundle->classBytecode.size());
      for (size_t i = 0, size = bundle->classes.size(); i < size; ++i) {
        auto& cls = bundle->classes[i];
        auto& bc = bundle->classBytecode[i];

        size_t bcIdx = 0;
        for (auto& meth : cls->methods) {
          assertx(bcIdx < bc->methodBCs.size());
          auto& methBC = bc->methodBCs[bcIdx++];
          always_assert(methBC.name == meth->name);
          always_assert(!meth->rawBlocks);
          meth->rawBlocks = std::move(methBC.bc);
        }
        for (auto& clo : cls->closures) {
          assertx(bcIdx < bc->methodBCs.size());
          auto& methBC = bc->methodBCs[bcIdx++];
          assertx(clo->methods.size() == 1);
          always_assert(methBC.name == clo->methods[0]->name);
          always_assert(!clo->methods[0]->rawBlocks);
          clo->methods[0]->rawBlocks = std::move(methBC.bc);
        }
        assertx(bcIdx == bc->methodBCs.size());
      }
      bundle->classBytecode.clear();

      always_assert(bundle->funcs.size() == bundle->funcBytecode.size());
      for (size_t i = 0, size = bundle->funcs.size(); i < size; ++i) {
        auto& bytecode = bundle->funcBytecode[i];
        bundle->funcs[i]->rawBlocks = std::move(bytecode->bc);
      }
      bundle->funcBytecode.clear();

      // And add it to our php::Program.
      std::scoped_lock<std::mutex> _{lock};
      for (auto& unit : bundle->units) {
        // Local execution doesn't need the native unit, so strip it
        // out.
        if (is_native_unit(*unit)) continue;
        program->units.emplace_back(std::move(unit));
      }
      for (auto& cls : bundle->classes) {
        program->classes.emplace_back(std::move(cls));
      }
      for (auto& func : bundle->funcs) {
        program->funcs.emplace_back(std::move(func));
      }
      remoteClassInfos.insert(
        end(remoteClassInfos),
        std::make_move_iterator(begin(bundle->classInfos)),
        std::make_move_iterator(end(bundle->classInfos))
      );
      remoteFuncInfos.insert(
        end(remoteFuncInfos),
        std::make_move_iterator(begin(bundle->funcInfos)),
        std::make_move_iterator(end(bundle->funcInfos))
      );
      remoteMethInfos.insert(
        end(remoteMethInfos),
        std::make_move_iterator(begin(bundle->methInfos)),
        std::make_move_iterator(end(bundle->methInfos))
      );
    }

    if (!chunk.funcFamilies.empty()) {
      std::scoped_lock<std::mutex> _{lock};
      for (auto& group : chunk.funcFamilies) {
        // The same func family can be on multiple groups, so only
        // insert the first one with a given id.
        for (auto& ff : group.m_ffs) {
          if (remoteFuncFamilyIds.emplace(ff->m_id).second) {
            remoteFuncFamilies.emplace_back(std::move(ff));
          }
        }
      }
    }

    semaphore.signal();
    co_return;
  };

  // We're going to load ClassGraphs concurrently.
  ClassGraph::initConcurrent();

  {
    // Temporarily suppress case collision logging
    auto oldTypeLogLevel = Cfg::Eval::LogTsameCollisions;
    Cfg::Eval::LogTsameCollisions = 0;
    SCOPE_EXIT {
      Cfg::Eval::LogTsameCollisions = oldTypeLogLevel;
    };

    trace_time _{"make local download", index.sample};
    coro::blockingWait(coro::collectAllRange(
      from(buckets)
        | move
        | map([&] (std::vector<SString> chunks) {
            return co_withExecutor(index.executor->sticky(), run(std::move(chunks)));
          })
        | as<std::vector>()
    ));
  }

  // Deserialization done.
  ClassGraph::stopConcurrent();

  // We've used any refs we need. Free them now to save memory.
  decltype(index.funcFamilyRefs){}.swap(index.funcFamilyRefs);
  decltype(index.bundleRefs){}.swap(index.bundleRefs);

  // Done with any extern-worker stuff at this point:
  index.configRef.reset();

  Logger::FInfo(
    "{}",
    index.client->getStats().toString(
      "hhbbc",
      folly::sformat(
        "{:,} units, {:,} classes, {:,} class-infos, {:,} funcs",
        program->units.size(),
        program->classes.size(),
        remoteClassInfos.size(),
        program->funcs.size()
      )
    )
  );

  if (index.sample) {
    index.client->getStats().logSample("hhbbc", *index.sample);
  }

  index.disposeClient(
    std::move(index.executor),
    std::move(index.client)
  );
  index.disposeClient = decltype(index.disposeClient){};

  php::FuncBytecode::s_reuser = nullptr;
  reuser.reset();

  buckets.clear();
  nameToFuncFamilyGroup.clear();
  remoteFuncFamilyIds.clear();

  program->units.shrink_to_fit();
  program->classes.shrink_to_fit();
  program->funcs.shrink_to_fit();
  index.program = std::move(program);

  // For now we don't require system constants in any extern-worker
  // stuff we do. So we can just add it to the Index now.
  add_system_constants_to_index(index);

  // Buid Index data structures from the php::Program.
  add_program_to_index(index);

  // Convert the FuncInfo2s into FuncInfos.
  make_func_infos_local(
    index,
    std::move(remoteFuncInfos)
  );

  // Convert the ClassInfo2s into ClassInfos.
  make_class_infos_local(
    index,
    std::move(remoteClassInfos),
    std::move(remoteFuncFamilies)
  );

  // Convert any "orphan" FuncInfo2s (those representing methods for a
  // class without a ClassInfo).
  parallel::for_each(
    remoteMethInfos,
    [&] (std::unique_ptr<MethodsWithoutCInfo>& meths) {
      auto const cls = folly::get_default(index.classes, meths->cls);
      always_assert_flog(
        cls,
        "php::Class for {} not found in index",
        meths->cls
      );
      assertx(cls->methods.size() == meths->finfos.size());
      for (size_t i = 0, size = cls->methods.size(); i < size; ++i) {
        remote_func_info_to_local(index, *cls->methods[i], *meths->finfos[i]);
      }

      assertx(cls->closures.size() == meths->closureInvokes.size());
      for (size_t i = 0, size = cls->closures.size(); i < size; ++i) {
        auto const& clo = cls->closures[i];
        assertx(clo->methods.size() == 1);
        remote_func_info_to_local(
          index,
          *clo->methods[0],
          *meths->closureInvokes[i]
        );
      }
    }
  );

  // Ensure that all classes are unserialized since all local
  // processing requires that.

  trace_time tracer2{"unserialize classes"};
  tracer2.ignore_client_stats();

  parallel::for_each(
    index.allClassInfos,
    [&] (std::unique_ptr<ClassInfo>& cinfo) {
      for (auto& [ty, _] : cinfo->clsConstTypes) {
        ty = unserialize_classes(
          IndexAdaptor { *index.m_index },
          std::move(ty)
        );
      }
    }
  );

  parallel::for_each(
    index.funcInfo,
    [&] (FuncInfo& fi) {
      if (!fi.func) return;
      fi.returnTy = unserialize_classes(
        IndexAdaptor { *index.m_index },
        std::move(fi.returnTy)
      );
    }
  );
}

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

std::vector<SString> Index::Input::makeDeps(const php::Class& cls) {
  std::vector<SString> deps;
  if (cls.parentName) deps.emplace_back(cls.parentName);
  deps.insert(deps.end(), cls.interfaceNames.begin(), cls.interfaceNames.end());
  deps.insert(deps.end(), cls.usedTraitNames.begin(), cls.usedTraitNames.end());
  deps.insert(
    deps.end(),
    cls.includedEnumNames.begin(),
    cls.includedEnumNames.end()
  );
  return deps;
}

//////////////////////////////////////////////////////////////////////

Index::Index(Input input,
             Config config,
             std::unique_ptr<TicketExecutor> executor,
             std::unique_ptr<Client> client,
             DisposeCallback dispose,
             StructuredLogEntry* sample)
  : m_data{std::make_unique<IndexData>(this)}
{
  trace_time tracer("create index", sample);
  m_data->sample = sample;

  auto flattenMeta = make_remote(
    *m_data,
    std::move(config),
    std::move(input),
    std::move(executor),
    std::move(client),
    std::move(dispose)
  );

  flatten_type_mappings(*m_data, flattenMeta);
  auto [subclassMeta, initTypesMeta, ifaceConflicts] =
    flatten_classes(*m_data, std::move(flattenMeta));
  build_subclass_lists(*m_data, std::move(subclassMeta), initTypesMeta);
  init_types(*m_data, std::move(initTypesMeta));
  compute_iface_vtables(*m_data, std::move(ifaceConflicts));
  check_invariants(*m_data);
  bundle_classes(*m_data);
}

// Defined here so IndexData is a complete type for the unique_ptr
// destructor.
Index::~Index() = default;

Index::Index(Index&&) = default;
Index& Index::operator=(Index&&) = default;

//////////////////////////////////////////////////////////////////////

const php::Program& Index::program() const {
  return *m_data->program;
}

StructuredLogEntry* Index::sample() const {
  return m_data->sample;
}

//////////////////////////////////////////////////////////////////////

TicketExecutor& Index::executor() const {
  return *m_data->executor;
}

Client& Index::client() const {
  return *m_data->client;
}

const CoroAsyncValue<Ref<Config>>& Index::configRef() const {
  return *m_data->configRef;
}

//////////////////////////////////////////////////////////////////////

const TSStringSet& Index::classes_with_86inits() const {
  return m_data->classesWith86Inits;
}

const FSStringSet& Index::constant_init_funcs() const {
  return m_data->constantInitFuncs;
}

const SStringSet& Index::units_with_type_aliases() const {
  return m_data->unitsWithTypeAliases;
}

//////////////////////////////////////////////////////////////////////

const TSStringSet& Index::all_classes_to_analyze() const {
  return m_data->allClassesToAnalyze;
}

const FSStringSet& Index::all_funcs() const {
  return m_data->allFuncs;
}

const SStringSet& Index::all_units() const {
  return m_data->allUnits;
}

//////////////////////////////////////////////////////////////////////

void Index::make_local() {
  HHBBC::make_local(*m_data);
  check_local_invariants(*m_data);
}

//////////////////////////////////////////////////////////////////////

const php::Class* Index::lookup_closure_context(const php::Class& cls) const {
  if (!cls.closureContextCls) return &cls;
  return m_data->classes.at(cls.closureContextCls);
}

const php::Unit* Index::lookup_func_unit(const php::Func& func) const {
  return m_data->units.at(func.unit);
}

const php::Unit* Index::lookup_func_original_unit(const php::Func& func) const {
  auto const unit = func.originalUnit ? func.originalUnit : func.unit;
  return m_data->units.at(unit);
}

const php::Unit* Index::lookup_class_unit(const php::Class& cls) const {
  return m_data->units.at(cls.unit);
}

const php::Class* Index::lookup_const_class(const php::Const& cns) const {
  return m_data->classes.at(cns.cls);
}

const php::Class* Index::lookup_class(SString name) const {
  return folly::get_default(m_data->classes, name);
}

//////////////////////////////////////////////////////////////////////

void Index::for_each_unit_func(const php::Unit& unit,
                               std::function<void(const php::Func&)> f) const {
  for (auto const func : unit.funcs) {
    f(*m_data->funcs.at(func));
  }
}

void Index::for_each_unit_func_mutable(php::Unit& unit,
                                       std::function<void(php::Func&)> f) {
  for (auto const func : unit.funcs) {
    f(*m_data->funcs.at(func));
  }
}

void Index::for_each_unit_class(
    const php::Unit& unit,
    std::function<void(const php::Class&)> f) const {
  for (auto const cls : unit.classes) {
    f(*m_data->classes.at(cls));
  }
}

void Index::for_each_unit_class_mutable(php::Unit& unit,
                                        std::function<void(php::Class&)> f) {
  for (auto const cls : unit.classes) {
    f(*m_data->classes.at(cls));
  }
}

//////////////////////////////////////////////////////////////////////

const hphp_fast_set<const php::Func*>*
Index::lookup_extra_methods(const php::Class* cls) const {
  if (cls->attrs & AttrNoExpandTrait) return nullptr;
  auto const it = m_data->classExtraMethodMap.find(cls);
  if (it != end(m_data->classExtraMethodMap)) {
    return &it->second;
  }
  return nullptr;
}

//////////////////////////////////////////////////////////////////////

Optional<res::Class> Index::resolve_class(SString clsName) const {
  clsName = normalizeNS(clsName);
  auto const it = m_data->classInfo.find(clsName);
  if (it == end(m_data->classInfo)) return std::nullopt;
  return res::Class::get(*it->second);
}

Optional<res::Class> Index::resolve_class(const php::Class& cls) const {
  return resolve_class(cls.name);
}

const php::TypeAlias* Index::lookup_type_alias(SString name) const {
  auto const it = m_data->typeAliases.find(name);
  if (it == m_data->typeAliases.end()) return nullptr;
  return it->second;
}

Index::ClassOrTypeAlias Index::lookup_class_or_type_alias(SString name) const {
  auto const rcls = resolve_class(name);
  if (rcls) {
    auto const cls = [&] () -> const php::Class* {
      if (auto const ci = rcls->cinfo()) return ci->cls;
      return m_data->classes.at(rcls->name());
    }();
    return ClassOrTypeAlias{cls, nullptr, true};
  }
  if (auto const ta = lookup_type_alias(name)) {
    return ClassOrTypeAlias{nullptr, ta, true};
  }
  return ClassOrTypeAlias{nullptr, nullptr, false};
}

// Given a DCls, return the most specific res::Func for that DCls. For
// intersections, this will call process/general on every component of
// the intersection and combine the results. For non-intersections, it
// will call process/general on the sole member of the DCls. process
// is called to obtain a res::Func from a ClassInfo. If a ClassInfo
// isn't available, general will be called instead.
template <typename P, typename G>
res::Func Index::rfunc_from_dcls(const DCls& dcls,
                                 SString name,
                                 const P& process,
                                 const G& general) const {
  if (dcls.isExact() || dcls.isSub()) {
    // If this isn't an intersection, there's only one cinfo to
    // process and we're done.
    auto const cinfo = dcls.cls().cinfo();
    if (!cinfo) return general(dcls.containsNonRegular());
    return process(cinfo, dcls.isExact(), dcls.containsNonRegular());
  }

  if (dcls.isIsectAndExact()) {
    // Even though this has an intersection list, it always must be
    // the exact class, so it sufficies to provide that.
    auto const e = dcls.isectAndExact().first;
    auto const cinfo = e.cinfo();
    if (!cinfo) return general(dcls.containsNonRegular());
    return process(cinfo, true, dcls.containsNonRegular());
  }

  /*
   * Otherwise get a res::Func for all members of the intersection and
   * combine them together. Since the DCls represents a class which is
   * a subtype of every ClassInfo in the list, every res::Func we get
   * is true.
   *
   * The relevant res::Func types in order from most general to more
   * specific are:
   *
   * MethodName -> FuncFamily -> MethodOrMissing -> Method -> Missing
   *
   * Since every res::Func in the intersection is true, we take the
   * res::Func which is most specific. Two different res::Funcs cannot
   * be contradict. For example, we shouldn't get a Method and a
   * Missing since one implies there's no func and the other implies
   * one specific func. Or two different res::Funcs shouldn't resolve
   * to two different methods.
   */

  assertx(dcls.isIsect());
  using Func = res::Func;

  auto missing = TriBool::Maybe;
  Func::Isect isect;
  const php::Func* singleMethod = nullptr;

  auto const DEBUG_ONLY allIncomplete = !debug || std::all_of(
    begin(dcls.isect()), end(dcls.isect()),
    [] (res::Class c) { return !c.hasCompleteChildren(); }
  );

  for (auto const i : dcls.isect()) {
    auto const cinfo = i.cinfo();
    if (!cinfo) continue;

    auto const func = process(cinfo, false, dcls.containsNonRegular());
    match(
      func.val,
      [&] (Func::MethodName) {},
      [&] (Func::Method m) {
        if (singleMethod) {
          assertx(missing != TriBool::Yes);
          assertx(isect.families.empty());
          if (singleMethod != m.finfo->func) {
            assertx(allIncomplete);
            singleMethod = nullptr;
            missing = TriBool::Yes;
          } else {
            missing = TriBool::No;
          }
        } else if (missing != TriBool::Yes) {
          singleMethod = m.finfo->func;
          isect.families.clear();
          missing = TriBool::No;
        }
      },
      [&] (Func::MethodFamily fam) {
        if (missing == TriBool::Yes) {
          assertx(!singleMethod);
          assertx(isect.families.empty());
          return;
        }
        if (singleMethod) {
          assertx(missing != TriBool::Yes);
          assertx(isect.families.empty());
          return;
        }
        assertx(missing == TriBool::Maybe);
        isect.families.emplace_back(fam.family);
        isect.regularOnly |= fam.regularOnly;
      },
      [&] (Func::MethodOrMissing m) {
        if (singleMethod) {
          assertx(missing != TriBool::Yes);
          assertx(isect.families.empty());
          if (singleMethod != m.finfo->func) {
            assertx(allIncomplete);
            singleMethod = nullptr;
            missing = TriBool::Yes;
          }
        } else if (missing != TriBool::Yes) {
          singleMethod = m.finfo->func;
          isect.families.clear();
        }
      },
      [&] (Func::MissingMethod) {
        assertx(IMPLIES(missing == TriBool::No, allIncomplete));
        singleMethod = nullptr;
        isect.families.clear();
        missing = TriBool::Yes;
      },
      [&] (Func::FuncName)         { always_assert(false); },
      [&] (Func::Fun)              { always_assert(false); },
      [&] (Func::Fun2)             { always_assert(false); },
      [&] (Func::Method2)          { always_assert(false); },
      [&] (Func::MethodFamily2)    { always_assert(false); },
      [&] (Func::MethodOrMissing2) { always_assert(false); },
      [&] (Func::MissingFunc)      { always_assert(false); },
      [&] (const Func::Isect&)     { always_assert(false); },
      [&] (const Func::Isect2&)    { always_assert(false); }
    );
  }

  // If we got a method, that always wins. Again, every res::Func is
  // true, and method is more specific than a FuncFamily, so it is
  // preferred.
  if (singleMethod) {
    assertx(missing != TriBool::Yes);
    // If missing is Maybe, then *every* resolution was to a
    // MethodName or MethodOrMissing, so include that fact here by
    // using MethodOrMissing.
    if (missing == TriBool::Maybe) {
      return Func {
        Func::MethodOrMissing { func_info(*m_data, singleMethod) }
      };
    }
    return Func { Func::Method { func_info(*m_data, singleMethod) } };
  }
  // We only got unresolved classes. If missing is TriBool::Yes, the
  // function doesn't exist. Otherwise be pessimistic.
  if (isect.families.empty()) {
    if (missing == TriBool::Yes) {
      return Func { Func::MissingMethod { dcls.smallestCls().name(), name } };
    }
    assertx(missing == TriBool::Maybe);
    return general(dcls.containsNonRegular());
  }
  // Isect case. Isects always might contain missing funcs.
  assertx(missing == TriBool::Maybe);

  // We could add a FuncFamily multiple times, so remove duplicates.
  std::sort(begin(isect.families), end(isect.families));
  isect.families.erase(
    std::unique(begin(isect.families), end(isect.families)),
    end(isect.families)
  );
  // If everything simplifies down to a single FuncFamily, just use
  // that.
  if (isect.families.size() == 1) {
    return Func { Func::MethodFamily { isect.families[0], isect.regularOnly } };
  }
  return Func { std::move(isect) };
}

res::Func Index::resolve_method(Context ctx,
                                const Type& thisType,
                                SString name) const {
  assertx(thisType.subtypeOf(BCls) || thisType.subtypeOf(BObj));

  using Func = res::Func;

  /*
   * Without using the class type, try to infer a set of methods
   * using just the method name. This will, naturally, not produce
   * as precise a set as when using the class type, but it's better
   * than nothing. For all of these results, we need to include the
   * possibility of the method not existing (we cannot rule that out
   * for this situation).
   */
  auto const general = [&] (bool includeNonRegular, SString maybeCls) {
    assertx(name != s_construct.get());

    // We don't produce name-only global func families for special
    // methods, so be conservative. We don't call special methods in a
    // context where we'd expect to not know the class, so it's not
    // worthwhile. The same goes for __invoke and __debuginfo, which
    // is corresponds to every closure, and gets too large without
    // much value.
    if (!has_name_only_func_family(name)) {
      return Func { Func::MethodName { maybeCls, name } };
    }

    // Lookup up the name-only global func families for this name. If
    // we don't have one, the method cannot exist because it contains
    // every method with that name in the program.
    auto const famIt = m_data->methodFamilies.find(name);
    if (famIt == end(m_data->methodFamilies)) {
      return Func { Func::MissingMethod { maybeCls, name } };
    }

    // The entry exists. Consult the correct data in it, depending on
    // whether we're including non-regular classes or not.
    auto const& entry = includeNonRegular
      ? famIt->second.m_all
      : famIt->second.m_regular;
    assertx(entry.isEmpty() || entry.isIncomplete());

    if (auto const ff = entry.funcFamily()) {
      return Func { Func::MethodFamily { ff, !includeNonRegular } };
    } else if (auto const f = entry.func()) {
      return Func { Func::MethodOrMissing { func_info(*m_data, f) } };
    } else {
      return Func { Func::MissingMethod { maybeCls, name } };
    }
  };

  auto const process = [&] (ClassInfo* cinfo,
                            bool isExact,
                            bool includeNonRegular) {
    assertx(name != s_construct.get());

    auto const methIt = cinfo->methods.find(name);
    if (methIt == end(cinfo->methods)) {
      // We don't store metadata for special methods, so be
      // pessimistic (the lack of a method entry does not mean the
      // call might fail at runtme).
      if (is_special_method_name(name)) {
        return Func { Func::MethodName { cinfo->cls->name, name } };
      }
      // We're only considering this class, not it's subclasses. Since
      // it doesn't exist here, the resolution will always fail.
      if (isExact) {
        return Func { Func::MissingMethod { cinfo->cls->name, name } };
      }
      // The method isn't present on this class, but it might be in
      // the subclasses. In most cases try a general lookup to get a
      // slightly better type than nothing.
      if (includeNonRegular ||
          !(cinfo->cls->attrs & (AttrInterface|AttrAbstract))) {
        return general(includeNonRegular, cinfo->cls->name);
      }

      // A special case is if we're only considering regular classes,
      // and this is an interface or abstract class. For those, we
      // "expand" the method families table to include any methods
      // defined in *all* regular subclasses. This is needed to
      // preserve monotonicity. Check this now.
      auto const famIt = cinfo->methodFamilies.find(name);
      // If no entry, treat it pessimistically like the rest of the
      // cases.
      if (famIt == end(cinfo->methodFamilies)) {
        return general(false, cinfo->cls->name);
      }

      // We found an entry. This cannot be empty (remember the method
      // is guaranteed to exist on *all* regular subclasses), and must
      // be complete (for the same reason). Use it.
      auto const& entry = famIt->second;
      assertx(!entry.isEmpty());
      assertx(entry.isComplete());
      if (auto const ff = entry.funcFamily()) {
        return Func { Func::MethodFamily { ff, true } };
      } else if (auto const func = entry.func()) {
        return Func { Func::Method { func_info(*m_data, func) } };
      } else {
        always_assert(false);
      }
    }
    // The method on this class.
    auto const& meth = methIt->second;
    auto const ftarget = func_from_meth_ref(*m_data, meth.meth());

    // We don't store method family information about special methods
    // and they have special inheritance semantics.
    if (is_special_method_name(name)) {
      // If we know the class exactly, we can use ftarget.
      if (isExact) return Func { Func::Method { func_info(*m_data, ftarget) } };
      // The method isn't overwritten, but they don't inherit, so it
      // could be missing.
      if (meth.attrs & AttrNoOverride) {
        return Func { Func::MethodOrMissing { func_info(*m_data, ftarget) } };
      }
      // Otherwise be pessimistic.
      return Func { Func::MethodName { cinfo->cls->name, name } };
    }

    // Private method handling: Private methods have special lookup
    // rules. If we're in the context of a particular class, and that
    // class defines a private method, an instance of the class will
    // always call that private method (even if overridden) in that
    // context.
    assertx(cinfo->cls);
    if (ctx.cls == cinfo->cls) {
      // The context matches the current class. If we've looked up a
      // private method (defined on this class), then that's what
      // we'll call.
      if ((meth.attrs & AttrPrivate) && meth.topLevel()) {
        return Func { Func::Method { func_info(*m_data, ftarget) } };
      }
    } else if ((meth.attrs & AttrPrivate) || meth.hasPrivateAncestor()) {
      // Otherwise the context doesn't match the current class. If the
      // looked up method is private, or has a private ancestor,
      // there's a chance we'll call that method (or
      // ancestor). Otherwise there's no private method in the
      // inheritance tree we'll call.
      auto const ancestor = [&] () -> const php::Func* {
        if (!ctx.cls) return nullptr;
        // Look up the ClassInfo corresponding to the context:
        auto const it = m_data->classInfo.find(ctx.cls->name);
        if (it == end(m_data->classInfo)) return nullptr;
        auto const ctxCInfo = it->second;
        // Is this context a parent of our class?
        if (!cinfo->classGraph.exactSubtypeOf(ctxCInfo->classGraph,
                                              true,
                                              true)) {
          return nullptr;
        }
        // It is. See if it defines a private method.
        auto const it2 = ctxCInfo->methods.find(name);
        if (it2 == end(ctxCInfo->methods)) return nullptr;
        auto const& mte = it2->second;
        // If it defines a private method, use it.
        if ((mte.attrs & AttrPrivate) && mte.topLevel()) {
          return func_from_meth_ref(*m_data, mte.meth());
        }
        // Otherwise do normal lookup.
        return nullptr;
      }();
      if (ancestor) {
        return Func { Func::Method { func_info(*m_data, ancestor) } };
      }
    }
    // If none of the above cases trigger, we still might call a
    // private method (in a child class), but the func-family logic
    // below will handle that.

    // If we're only including regular subclasses, and this class
    // itself isn't regular, the result may not necessarily include
    // ftarget.
    if (!includeNonRegular && !is_regular_class(*cinfo->cls)) {
      // We're not including this base class. If we're exactly this
      // class, there's no method at all. It will always be missing.
      if (isExact) {
        return Func { Func::MissingMethod { cinfo->cls->name, name } };
      }
      if (meth.noOverrideRegular()) {
        // The method isn't overridden in a subclass, but we can't
        // use the base class either. This leaves two cases. Either
        // the method isn't overridden because there are no regular
        // subclasses (in which case there's no resolution at all), or
        // because there's regular subclasses, but they use the same
        // method (in which case the result is just ftarget).
        if (!cinfo->classGraph.mightHaveRegularSubclass()) {
          return Func { Func::MissingMethod { cinfo->cls->name, name } };
        }
        return Func { Func::Method { func_info(*m_data, ftarget) } };
      }
      // We can't use the base class (because it's non-regular), but
      // the method is overridden by a regular subclass.

      // Since this is a non-regular class and we want the result for
      // the regular subset, we need to consult the aux table first.
      auto const auxIt = cinfo->methodFamiliesAux.find(name);
      if (auxIt != end(cinfo->methodFamiliesAux)) {
        // Found an entry in the aux table. Use whatever it provides.
        auto const& aux = auxIt->second;
        if (auto const ff = aux.funcFamily()) {
          return Func { Func::MethodFamily { ff, true } };
        } else if (auto const f = aux.func()) {
          return aux.isComplete()
            ? Func { Func::Method { func_info(*m_data, f) } }
            : Func { Func::MethodOrMissing { func_info(*m_data, f) } };
        } else {
          return Func { Func::MissingMethod { cinfo->cls->name, name } };
        }
      }
      // No entry in the aux table. The result is the same as the
      // normal table, so fall through and use that.
    } else if (isExact ||
               meth.attrs & AttrNoOverride ||
               (!includeNonRegular && meth.noOverrideRegular())) {
      // Either we want all classes, or the base class is regular. If
      // the method isn't overridden we know it must be just ftarget
      // (the override bits include it being missing in a subclass, so
      // we know it cannot be missing either).
      return Func { Func::Method { func_info(*m_data, ftarget) } };
    }

    // Look up the entry in the normal method family table and use
    // whatever is there.
    auto const famIt = cinfo->methodFamilies.find(name);
    assertx(famIt != end(cinfo->methodFamilies));
    auto const& fam = famIt->second;
    assertx(!fam.isEmpty());

    if (auto const ff = fam.funcFamily()) {
      return Func { Func::MethodFamily { ff, !includeNonRegular } };
    } else if (auto const f = fam.func()) {
      return (!includeNonRegular || fam.isComplete())
        ? Func { Func::Method { func_info(*m_data, f) } }
        : Func { Func::MethodOrMissing { func_info(*m_data, f) } };
    } else {
      always_assert(false);
    }
  };

  auto const isClass = thisType.subtypeOf(BCls);
  if (name == s_construct.get()) {
    if (isClass) {
      return Func { Func::MethodName { nullptr, s_construct.get() } };
    }
    return resolve_ctor(thisType);
  }

  if (isClass) {
    if (!is_specialized_cls(thisType)) return general(true, nullptr);
  } else if (!is_specialized_obj(thisType)) {
    return general(false, nullptr);
  }

  auto const& dcls = isClass ? dcls_of(thisType) : dobj_of(thisType);
  return rfunc_from_dcls(
    dcls,
    name,
    process,
    [&] (bool i) { return general(i, dcls.smallestCls().name()); }
  );
}

res::Func Index::resolve_ctor(const Type& obj) const {
  assertx(obj.subtypeOf(BObj));

  using Func = res::Func;

  // Can't say anything useful if we don't know the object type.
  if (!is_specialized_obj(obj)) {
    return Func { Func::MethodName { nullptr, s_construct.get() } };
  }

  auto const& dcls = dobj_of(obj);
  return rfunc_from_dcls(
    dcls,
    s_construct.get(),
    [&] (ClassInfo* cinfo, bool isExact, bool includeNonRegular) {
      // We're dealing with an object here, which never uses
      // non-regular classes.
      assertx(!includeNonRegular);

      // See if this class has a ctor.
      auto const methIt = cinfo->methods.find(s_construct.get());
      if (methIt == end(cinfo->methods)) {
        // There's no ctor on this class. This doesn't mean the ctor
        // won't exist at runtime, it might get the default ctor, so
        // we have to be conservative.
        return Func {
          Func::MethodName { cinfo->cls->name, s_construct.get() }
        };
      }

      // We have a ctor, but it might be overridden in a subclass.
      auto const& mte = methIt->second;
      assertx(!(mte.attrs & AttrStatic));
      auto const ftarget = func_from_meth_ref(*m_data, mte.meth());
      assertx(!(ftarget->attrs & AttrStatic));

      // If this class is known exactly, or we know nothing overrides
      // this ctor, we know this ctor is precisely it.
      if (isExact || mte.noOverrideRegular()) {
        // If this class isn't regular, and doesn't have any regular
        // subclasses (or if it's exact), this resolution will always
        // fail.
        if (!is_regular_class(*cinfo->cls) &&
            (isExact || !cinfo->classGraph.mightHaveRegularSubclass())) {
          return Func {
            Func::MissingMethod { cinfo->cls->name, s_construct.get() }
          };
        }
        return Func { Func::Method { func_info(*m_data, ftarget) } };
      }

      // If this isn't a regular class, we need to check the "aux"
      // entry first (which always has priority when only looking at
      // the regular subset).
      if (!is_regular_class(*cinfo->cls)) {
        auto const auxIt = cinfo->methodFamiliesAux.find(s_construct.get());
        if (auxIt != end(cinfo->methodFamiliesAux)) {
          auto const& aux = auxIt->second;
          if (auto const ff = aux.funcFamily()) {
            return Func { Func::MethodFamily { ff, true } };
          } else if (auto const f = aux.func()) {
            return aux.isComplete()
              ? Func { Func::Method { func_info(*m_data, f) } }
              : Func { Func::MethodOrMissing { func_info(*m_data, f) } };
          } else {
            // Ctor doesn't exist in any regular subclasses. This can
            // happen with interfaces. The ctor might get the default
            // ctor at runtime, so be conservative.
            return Func {
              Func::MethodName { cinfo->cls->name, s_construct.get() }
            };
          }
        }
      }
      // Otherwise this class is regular (in which case there's just
      // method families, or there's no entry in aux, which means the
      // regular subset entry is the same as the full entry.

      auto const famIt = cinfo->methodFamilies.find(s_construct.get());
      assertx(famIt != cinfo->methodFamilies.end());
      auto const& fam = famIt->second;
      assertx(!fam.isEmpty());

      if (auto const ff = fam.funcFamily()) {
        return Func { Func::MethodFamily { ff, true } };
      } else if (auto const f = fam.func()) {
        // Since we're looking at the regular subset, we can assume
        // the set is complete, regardless of the flag on fam.
        return Func { Func::Method { func_info(*m_data, f) } };
      } else {
        always_assert(false);
      }
    },
    [&] (bool includeNonRegular) {
      assertx(!includeNonRegular);
      return Func {
        Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
      };
    }
  );
}

res::Func Index::resolve_func(SString name) const {
  name = normalizeNS(name);
  auto const it = m_data->funcs.find(name);
  if (it == end(m_data->funcs)) {
    return res::Func { res::Func::MissingFunc { name } };
  }
  auto const func = it->second;
  assertx(func->attrs & AttrPersistent);
  return res::Func { res::Func::Fun { func_info(*m_data, func) } };
}

res::Func Index::resolve_func_or_method(const php::Func& f) const {
  if (!f.cls) return res::Func { res::Func::Fun { func_info(*m_data, &f) } };
  return res::Func { res::Func::Method { func_info(*m_data, &f) } };
}

bool Index::func_depends_on_arg(const php::Func* func, size_t arg) const {
  auto const& finfo = *func_info(*m_data, func);
  return arg >= finfo.unusedParams.size() || !finfo.unusedParams.test(arg);
}

// Helper function: Given a DCls, visit every subclass it represents,
// passing it to the given callable. If the callable returns false,
// stop iteration. Return false if any of the classes is unresolved,
// true otherwise. This is used to simplify the below functions which
// need to iterate over all possible subclasses and union the results.
template <typename F>
bool Index::visit_every_dcls_cls(const DCls& dcls, const F& f) const {
  if (dcls.isExact()) {
    auto const cinfo = dcls.cls().cinfo();
    if (!cinfo) return false;
    if (dcls.containsNonRegular() || is_regular_class(*cinfo->cls)) {
      f(cinfo);
    }
    return true;
  } else if (dcls.isSub()) {
    auto unresolved = false;
    res::Class::visitEverySub(
      std::array<res::Class, 1>{dcls.cls()},
      dcls.containsNonRegular(),
      [&] (res::Class c) {
        if (c.hasCompleteChildren()) {
          if (auto const cinfo = c.cinfo()) return f(cinfo);
        }
        unresolved = true;
        return false;
      }
    );
    return !unresolved;
  } else if (dcls.isIsect()) {
    auto const& isect = dcls.isect();
    assertx(isect.size() > 1);

    auto unresolved = false;
    res::Class::visitEverySub(
      isect,
      dcls.containsNonRegular(),
      [&] (res::Class c) {
        if (c.hasCompleteChildren()) {
          if (auto const cinfo = c.cinfo()) return f(cinfo);
        }
        unresolved = true;
        return false;
      }
    );
    return !unresolved;
  } else {
    // Even though this has an intersection list, it must be the exact
    // class, so it's sufficient to provide that.
    assertx(dcls.isIsectAndExact());
    auto const e = dcls.isectAndExact().first;
    auto const cinfo = e.cinfo();
    if (!cinfo) return false;
    if (dcls.containsNonRegular() || is_regular_class(*cinfo->cls)) {
      f(cinfo);
    }
    return true;
  }
}

ClsConstLookupResult Index::lookup_class_constant(Context ctx,
                                                  const Type& cls,
                                                  const Type& name) const {
  ITRACE(4, "lookup_class_constant: ({}) {}::{}\n",
         show(ctx), show(cls), show(name));
  Trace::Indent _;

  using R = ClsConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R{ TInitCell, TriBool::Maybe, true };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R{ TBottom, TriBool::No, false };
  };

  if (!is_specialized_cls(cls)) return conservative();

  // We could easily support the case where we don't know the constant
  // name, but know the class (like we do for properties), by unioning
  // together all possible constants. However it very rarely happens,
  // but when it does, the set of constants to union together can be
  // huge and it becomes very expensive.
  if (!is_specialized_string(name)) return conservative();
  auto const sname = sval_of(name);

  // If this lookup is safe to cache. Some classes can have a huge
  // number of subclasses and unioning together all possible constants
  // can become very expensive. We can aleviate some of this expense
  // by caching results. We cannot cache a result when we use 86cinit
  // analysis since that can change.
  auto cachable = true;

  auto const process = [&] (const ClassInfo* ci) {
    ITRACE(4, "{}:\n", ci->cls->name);
    Trace::Indent _;

    // Does the constant exist on this class?
    auto const it = ci->clsConstants.find(sname);
    if (it == ci->clsConstants.end()) return notFound();

    // Is it a value and is it non-abstract (we only deal with
    // concrete constants).
    auto const& cns = *it->second.get();
    if (cns.kind != ConstModifierFlags::Kind::Value) return notFound();
    if (!cns.val.has_value()) return notFound();

    auto const cnsIdx = it->second.idx;

    // Determine the constant's value and return it
    auto const r = [&] {
      if (cns.val->m_type == KindOfUninit) {
        // Constant is defined by a 86cinit. Use the result from
        // analysis and add a dependency. We cannot cache in this
        // case.
        cachable = false;
        auto const cnsCls = m_data->classes.at(cns.cls);
        if (ctx.func) {
          auto const cinit = cnsCls->methods.back().get();
          assertx(cinit->name == s_86cinit.get());
          add_dependency(*m_data, cinit, ctx, Dep::ClsConst);
        }

        ITRACE(4, "(dynamic)\n");
        auto const type = [&] {
          auto const cnsClsCi = folly::get_default(m_data->classInfo, cnsCls->name);
          if (!cnsClsCi || cnsIdx >= cnsClsCi->clsConstTypes.size()) {
            return TInitCell;
          }
          return cnsClsCi->clsConstTypes[cnsIdx].type;
        }();
        return R{ type, TriBool::Yes, true };
      }

      // Fully resolved constant with a known value
      auto mightThrow = bool(ci->cls->attrs & AttrInternal);
      return R{ from_cell(*cns.val), TriBool::Yes, mightThrow };
    }();
    ITRACE(4, "-> {}\n", show(r));
    return r;
  };

  auto const& dcls = dcls_of(cls);
  if (dcls.isSub()) {
    // Before anything, look up this entry in the cache. We don't
    // bother with the cache for the exact case because it's quick and
    // there's little point.
    auto const cinfo = dcls.cls().cinfo();
    if (!cinfo) return conservative();
    if (auto const it =
        m_data->clsConstLookupCache.find(std::make_pair(cinfo->cls, sname));
        it != m_data->clsConstLookupCache.end()) {
      ITRACE(4, "cache hit: {}\n", show(it->second));
      return it->second;
    }
  }

  Optional<R> result;
  auto const resolved = visit_every_dcls_cls(
    dcls,
    [&] (const ClassInfo* cinfo) {
      if (result) ITRACE(5, "-> {}\n", show(*result));
      auto r = process(cinfo);
      if (!result) {
        result.emplace(std::move(r));
      } else {
        *result |= r;
      }
      return true;
    }
  );
  if (!resolved) return conservative();
  assertx(result.has_value());

  // Save this for future lookups if we can
  if (dcls.isSub() && cachable) {
    auto const cinfo = dcls.cls().cinfo();
    assertx(cinfo);
    m_data->clsConstLookupCache.emplace(
      std::make_pair(cinfo->cls, sname),
      *result
    );
  }

  ITRACE(4, "-> {}\n", show(*result));
  return *result;
}

std::vector<std::pair<SString, ConstIndex>>
Index::lookup_flattened_class_type_constants(const php::Class& cls) const {
  std::vector<std::pair<SString, ConstIndex>> out;

  auto const rcls = resolve_class(cls);
  if (!rcls) return out;
  auto const cinfo = rcls->cinfo();
  if (!cinfo) return out;

  out.reserve(cinfo->clsConstants.size());
  for (auto const& [name, idx] : cinfo->clsConstants) {
    if (idx->kind != ConstModifierFlags::Kind::Type) continue;
    out.emplace_back(name, ConstIndex{ idx.cls->name, idx.idx });
  }
  std::sort(
    begin(out), end(out),
    [] (auto const& p1, auto const& p2) {
      return string_data_lt{}(p1.first, p2.first);
    }
  );
  return out;
}

std::vector<std::pair<SString, ClsConstInfo>>
Index::lookup_class_constants(const php::Class& cls) const {
  std::vector<std::pair<SString, ClsConstInfo>> out;
  out.reserve(cls.constants.size());

  auto const cinfo = folly::get_default(m_data->classInfo, cls.name);
  for (size_t i = 0, size = cls.constants.size(); i < size; ++i) {
    auto const& cns = cls.constants[i];
    if (cns.kind != ConstModifierFlags::Kind::Value) continue;
    if (!cns.val) continue;
    if (cns.val->m_type != KindOfUninit) {
      out.emplace_back(cns.name, ClsConstInfo{ from_cell(*cns.val), 0 });
    } else if (cinfo && i < cinfo->clsConstTypes.size()) {
      out.emplace_back(cns.name, cinfo->clsConstTypes[i]);
    } else {
      out.emplace_back(cns.name, ClsConstInfo{ TInitCell, 0 });
    }
  }
  return out;
}

ClsTypeConstLookupResult
Index::lookup_class_type_constant(
    const Type& cls,
    const Type& name,
    const ClsTypeConstLookupResolver& resolver) const {
  ITRACE(4, "lookup_class_type_constant: {}::{}\n", show(cls), show(name));
  Trace::Indent _;

  using R = ClsTypeConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R{
      TypeStructureResolution { TSDictN, true },
      TriBool::Maybe,
      TriBool::Maybe
    };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::No
    };
  };

  // Unlike lookup_class_constant, we distinguish abstract from
  // not-found, as the runtime sometimes treats them differently.
  auto const abstract = [] {
    ITRACE(4, "abstract\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::Yes
    };
  };

  if (!is_specialized_cls(cls)) return conservative();

  // As in lookup_class_constant, we could handle this, but it's not
  // worth it.
  if (!is_specialized_string(name)) return conservative();
  auto const sname = sval_of(name);

  auto const process = [&] (const ClassInfo* ci) {
    ITRACE(4, "{}:\n", ci->cls->name);
    Trace::Indent _;

    // Does the type constant exist on this class?
    auto const it = ci->clsConstants.find(sname);
    if (it == ci->clsConstants.end()) return notFound();

    // Is it an actual non-abstract type-constant?
    auto const& cns = *it->second;
    if (cns.kind != ConstModifierFlags::Kind::Type) return notFound();
    if (!cns.val.has_value()) return abstract();

    assertx(tvIsDict(*cns.val));
    ITRACE(4, "({}) {}\n", cns.cls, show(dict_val(val(*cns.val).parr)));

    // If we've been given a resolver, use it. Otherwise resolve it in
    // the normal way.
    auto resolved = resolver
      ? resolver(cns, *ci->cls)
      : resolve_type_structure(IndexAdaptor { *this }, cns, *ci->cls);

    // The result of resolve_type_structure isn't, in general,
    // static. However a type-constant will always be, so force that
    // here.
    assertx(resolved.type.is(BBottom) || resolved.type.couldBe(BUnc));
    resolved.type &= TUnc;
    auto const r = R{
      std::move(resolved),
      TriBool::Yes,
      TriBool::No
    };
    ITRACE(4, "-> {}\n", show(r));
    return r;
  };

  auto const& dcls = dcls_of(cls);

  Optional<R> result;
  auto const resolved = visit_every_dcls_cls(
    dcls,
    [&] (const ClassInfo* cinfo) {
      if (result) {
        ITRACE(5, "-> {}\n", show(*result));
      }
      auto r = process(cinfo);
      if (!result) {
        result.emplace(std::move(r));
      } else {
        *result |= r;
      }
      return true;
    }
  );
  if (!resolved) return conservative();
  assertx(result.has_value());

  ITRACE(4, "-> {}\n", show(*result));
  return *result;
}

ClsTypeConstLookupResult
Index::lookup_class_type_constant(const php::Class& ctx,
                                  SString name,
                                  HHBBC::ConstIndex idx) const {
  ITRACE(4, "lookup_class_type_constant: {}::{}\n",
         ctx.name, show(idx, IndexAdaptor{ *this }));
  Trace::Indent _;

  using R = ClsTypeConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R {
      TypeStructureResolution { TSDictN, true },
      TriBool::Maybe,
      TriBool::Maybe
    };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::No
    };
  };

  // Unlike lookup_class_constant, we distinguish abstract from
  // not-found, as the runtime sometimes treats them differently.
  auto const abstract = [] {
    ITRACE(4, "abstract\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::Yes
    };
  };

  auto const cinfo = folly::get_default(m_data->classInfo, idx.cls);
  if (!cinfo) return conservative();

  assertx(idx.idx < cinfo->cls->constants.size());
  auto const& cns = cinfo->cls->constants[idx.idx];
  if (cns.kind != ConstModifierFlags::Kind::Type) return notFound();
  if (!cns.val.has_value()) return abstract();

  assertx(tvIsDict(*cns.val));

  ITRACE(4, "({}) {}\n", cns.cls, show(dict_val(val(*cns.val).parr)));

  auto resolved = resolve_type_structure(IndexAdaptor { *this }, cns, ctx);

  // The result of resolve_type_structure isn't, in general,
  // static. However a type-constant will always be, so force that
  // here.
  assertx(resolved.type.is(BBottom) || resolved.type.couldBe(BUnc));
  resolved.type &= TUnc;
  auto const r = R{
    std::move(resolved),
    TriBool::Yes,
    TriBool::No
  };
  ITRACE(4, "-> {}\n", show(r));
  return r;
}

Type Index::lookup_constant(Context ctx, SString cnsName) const {
  auto iter = m_data->constants.find(cnsName);
  if (iter == end(m_data->constants)) return TBottom;

  auto constant = iter->second;
  if (type(constant->val) != KindOfUninit) {
    return from_cell(constant->val);
  }

  // Assume a runtime call to Constant::get(), which will invoke
  // 86cinit_<cnsName>(). Look up it's return type.

  auto const func_name = Constant::funcNameFromName(cnsName);
  assertx(func_name && "func_name will never be nullptr");

  auto rfunc = resolve_func(func_name);
  assertx(rfunc.exactFunc());

  return lookup_return_type(ctx, nullptr, rfunc, Dep::ConstVal).t;
}

Index::ReturnType
Index::lookup_foldable_return_type(Context ctx,
                                   const CallContext& calleeCtx) const {
  auto const func = calleeCtx.callee;
  constexpr auto max_interp_nexting_level = 2;
  static __thread uint32_t interp_nesting_level;

  using R = ReturnType;

  auto const ctxType = adjust_closure_context(
    IndexAdaptor { *this },
    calleeCtx
  );

  // Don't fold functions when staticness mismatches
  if (!func->isClosureBody) {
    if ((func->attrs & AttrStatic) && ctxType.couldBe(TObj)) {
      return R{ TInitCell, false };
    }
    if (!(func->attrs & AttrStatic) && ctxType.couldBe(TCls)) {
      return R{ TInitCell, false };
    }
  }

  auto const& finfo = *func_info(*m_data, func);
  if (finfo.effectFree && is_scalar(finfo.returnTy)) {
    return R{ finfo.returnTy, true };
  }

  auto showArgs DEBUG_ONLY = [] (const CompactVector<Type>& a) {
    std::string ret, sep;
    for (auto& arg : a) {
      folly::format(&ret, "{}{}", sep, show(arg));
      sep = ",";
    };
    return ret;
  };

  {
    ContextRetTyMap::const_accessor acc;
    if (m_data->foldableReturnTypeMap.find(acc, calleeCtx)) {
      FTRACE_MOD(
        Trace::hhbbc, 4,
        "Found foldableReturnType for {}{}{} with args {} (hash: {})\n",
        func->cls ? func->cls->name : staticEmptyString(),
        func->cls ? "::" : "",
        func->name,
        showArgs(calleeCtx.args),
        CallContextHashCompare{}.hash(calleeCtx));

      assertx(is_scalar(acc->second.t));
      assertx(acc->second.effectFree);
      return acc->second;
    }
  }

  if (frozen()) {
    FTRACE_MOD(
      Trace::hhbbc, 4,
      "MISSING: foldableReturnType for {}{}{} with args {} (hash: {})\n",
      func->cls ? func->cls->name : staticEmptyString(),
      func->cls ? "::" : "",
      func->name,
      showArgs(calleeCtx.args),
      CallContextHashCompare{}.hash(calleeCtx));
    return R{ TInitCell, false };
  }

  if (interp_nesting_level > max_interp_nexting_level) {
    add_dependency(*m_data, func, ctx, Dep::InlineDepthLimit);
    return R{ TInitCell, false };
  }

  auto const contextType = [&] {
    ++interp_nesting_level;
    SCOPE_EXIT { --interp_nesting_level; };

    auto const wf = php::WideFunc::cns(func);
    auto const fa = analyze_func_inline(
      IndexAdaptor { *this },
      AnalysisContext { func->unit, wf, func->cls, &ctx.forDep() },
      ctxType,
      calleeCtx.args,
      nullptr,
      CollectionOpts::EffectFreeOnly
    );
    return R{
      fa.effectFree ? fa.inferredReturn : TInitCell,
      fa.effectFree
    };
  }();

  if (!is_scalar(contextType.t)) return R{ TInitCell, false };

  ContextRetTyMap::accessor acc;
  if (m_data->foldableReturnTypeMap.insert(acc, calleeCtx)) {
    acc->second = contextType;
  } else {
    // someone beat us to it
    assertx(equal(acc->second.t, contextType.t));
  }
  return contextType;
}

Index::ReturnType Index::lookup_return_type(Context ctx,
                                            MethodsInfo* methods,
                                            res::Func rfunc,
                                            Dep dep) const {
  using R = ReturnType;

  auto const funcFamily = [&] (FuncFamily* fam, bool regularOnly) {
    add_dependency(*m_data, fam, ctx, dep);
    return fam->infoFor(regularOnly).m_returnTy.get(
      [&] {
        auto ret = TBottom;
        auto effectFree = true;
        for (auto const pf : fam->possibleFuncs()) {
          if (regularOnly && !pf.inRegular()) continue;
          auto const finfo = func_info(*m_data, pf.ptr());
          if (!finfo->func) return R{ TInitCell, false };
          ret |= unctx(finfo->returnTy);
          effectFree &= finfo->effectFree;
          if (!ret.strictSubtypeOf(BInitCell) && !effectFree) break;
        }
        return R{ std::move(ret), effectFree };
      }
    );
  };
  auto const meth = [&] (const php::Func* func) {
    if (methods) {
      if (auto ret = methods->lookupReturnType(*func)) {
        return R{ unctx(std::move(ret->t)), ret->effectFree };
      }
    }
    add_dependency(*m_data, func, ctx, dep);
    auto const finfo = func_info(*m_data, func);
    if (!finfo->func) return R{ TInitCell, false };
    return R{ unctx(finfo->returnTy), finfo->effectFree };
  };

  return match<R>(
    rfunc.val,
    [&] (res::Func::FuncName)   { return R{ TInitCell, false }; },
    [&] (res::Func::MethodName) { return R{ TInitCell, false }; },
    [&] (res::Func::Fun f) {
      add_dependency(*m_data, f.finfo->func, ctx, dep);
      return R{ unctx(f.finfo->returnTy), f.finfo->effectFree };
    },
    [&] (res::Func::Method m)          { return meth(m.finfo->func); },
    [&] (res::Func::MethodFamily fam)  {
      return funcFamily(fam.family, fam.regularOnly);
    },
    [&] (res::Func::MethodOrMissing m) { return meth(m.finfo->func); },
    [&] (res::Func::MissingFunc)       { return R{ TBottom, false }; },
    [&] (res::Func::MissingMethod)     { return R{ TBottom, false }; },
    [&] (const res::Func::Isect& i) {
      auto ty = TInitCell;
      auto anyEffectFree = false;
      for (auto const ff : i.families) {
        auto const [t, e] = funcFamily(ff, i.regularOnly);
        ty &= t;
        if (e) anyEffectFree = true;
      }
      return R{ std::move(ty), anyEffectFree };
    },
    [&] (res::Func::Fun2)             -> R { always_assert(false); },
    [&] (res::Func::Method2)          -> R { always_assert(false); },
    [&] (res::Func::MethodFamily2)    -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing2) -> R { always_assert(false); },
    [&] (res::Func::Isect2&)          -> R { always_assert(false); }
  );
}

Index::ReturnType Index::lookup_return_type(Context caller,
                                            MethodsInfo* methods,
                                            const CompactVector<Type>& args,
                                            const Type& context,
                                            res::Func rfunc,
                                            Dep dep) const {
  using R = ReturnType;

  auto const funcFamily = [&] (FuncFamily* fam, bool regularOnly) {
    add_dependency(*m_data, fam, caller, dep);
    auto ret = fam->infoFor(regularOnly).m_returnTy.get(
      [&] {
        auto ty = TBottom;
        auto effectFree = true;
        for (auto const pf : fam->possibleFuncs()) {
          if (regularOnly && !pf.inRegular()) continue;
          auto const finfo = func_info(*m_data, pf.ptr());
          if (!finfo->func) return R{ TInitCell, false };
          ty |= finfo->returnTy;
          effectFree &= finfo->effectFree;
          if (!ty.strictSubtypeOf(BInitCell) && !effectFree) break;
        }
        return R{ std::move(ty), effectFree };
      }
    );
    return R{
      return_with_context(std::move(ret.t), context),
      ret.effectFree
    };
  };
  auto const meth = [&] (const php::Func* func) {
    auto const finfo = func_info(*m_data, func);
    if (!finfo->func) return R{ TInitCell, false };

    auto returnType = [&] {
      if (methods) {
        if (auto ret = methods->lookupReturnType(*func)) {
          return *ret;
        }
      }
      add_dependency(*m_data, func, caller, dep);
      return R{ finfo->returnTy, finfo->effectFree };
    }();

    return context_sensitive_return_type(
      *m_data,
      caller,
      { finfo->func, args, context },
      std::move(returnType)
    );
  };

  return match<R>(
    rfunc.val,
    [&] (res::Func::FuncName) {
      return lookup_return_type(caller, methods, rfunc, dep);
    },
    [&] (res::Func::MethodName) {
      return lookup_return_type(caller, methods, rfunc, dep);
    },
    [&] (res::Func::Fun f) {
      add_dependency(*m_data, f.finfo->func, caller, dep);
      return context_sensitive_return_type(
        *m_data,
        caller,
        { f.finfo->func, args, context },
        R{ f.finfo->returnTy, f.finfo->effectFree }
      );
    },
    [&] (res::Func::Method m)          { return meth(m.finfo->func); },
    [&] (res::Func::MethodFamily fam)  {
      return funcFamily(fam.family, fam.regularOnly);
    },
    [&] (res::Func::MethodOrMissing m) { return meth(m.finfo->func); },
    [&] (res::Func::MissingFunc)       { return R { TBottom, false }; },
    [&] (res::Func::MissingMethod)     { return R { TBottom, false }; },
    [&] (const res::Func::Isect& i) {
      auto ty = TInitCell;
      auto anyEffectFree = false;
      for (auto const ff : i.families) {
        auto const [t, e] = funcFamily(ff, i.regularOnly);
        ty &= t;
        if (e) anyEffectFree = true;
      }
      return R{ std::move(ty), anyEffectFree };
    },
    [&] (res::Func::Fun2)             -> R { always_assert(false); },
    [&] (res::Func::Method2)          -> R { always_assert(false); },
    [&] (res::Func::MethodFamily2)    -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing2) -> R { always_assert(false); },
    [&] (res::Func::Isect2&)          -> R { always_assert(false); }
  );
}

std::pair<Index::ReturnType, size_t>
Index::lookup_return_type_raw(const php::Func* f) const {
  auto it = func_info(*m_data, f);
  if (it->func) {
    assertx(it->func == f);
    return {
      ReturnType{ it->returnTy, it->effectFree },
      it->returnRefinements
    };
  }
  return { ReturnType{ TInitCell, false }, 0 };
}

CompactVector<Type>
Index::lookup_closure_use_vars(const php::Func* func) const {
  assertx(func->isClosureBody);

  auto const numUseVars = closure_num_use_vars(func);
  if (!numUseVars) return {};
  auto const it = m_data->closureUseVars.find(func->cls);
  if (it == end(m_data->closureUseVars)) {
    return CompactVector<Type>(numUseVars, TCell);
  }
  return it->second;
}

PropState
Index::lookup_private_props(const php::Class* cls,
                            bool move) const {
  auto it = m_data->privatePropInfo.find(cls);
  if (it != end(m_data->privatePropInfo)) {
    if (move) return std::move(it->second);
    return it->second;
  }
  return make_unknown_propstate(
    *this,
    *cls,
    [&] (const php::Prop& prop) {
      return (prop.attrs & AttrPrivate) && !(prop.attrs & AttrStatic);
    }
  );
}

PropState
Index::lookup_private_statics(const php::Class* cls,
                              bool move) const {
  auto it = m_data->privateStaticPropInfo.find(cls);
  if (it != end(m_data->privateStaticPropInfo)) {
    if (move) return std::move(it->second);
    return it->second;
  }
  return make_unknown_propstate(
    *this,
    *cls,
    [&] (const php::Prop& prop) {
      return (prop.attrs & AttrPrivate) && (prop.attrs & AttrStatic);
    }
  );
}

PropState Index::lookup_public_statics(const php::Class* cls) const {
  auto const cinfo = [&] () -> const ClassInfo* {
    auto const it = m_data->classInfo.find(cls->name);
    if (it == end(m_data->classInfo)) return nullptr;
    return it->second;
  }();

  PropState state;
  for (auto const& prop : cls->properties) {
    if (!(prop.attrs & (AttrPublic|AttrProtected)) ||
        !(prop.attrs & AttrStatic)) {
      continue;
    }

    auto [ty, everModified] = [&] {
      if (!cinfo) return std::make_pair(TInitCell, true);

      if (!m_data->seenPublicSPropMutations) {
        return std::make_pair(
          union_of(
            adjust_type_for_prop(
              IndexAdaptor { *this },
              *cls,
              &prop.typeConstraints,
              TInitCell
            ),
            initial_type_for_public_sprop(*this, *cls, prop)
          ),
          true
        );
      }

      auto const it = cinfo->publicStaticProps.find(prop.name);
      if (it == end(cinfo->publicStaticProps)) {
        return std::make_pair(
          initial_type_for_public_sprop(*this, *cls, prop),
          false
        );
      }
      return std::make_pair(
        it->second.inferredType,
        it->second.everModified
      );
    }();
    state.emplace(
      prop.name,
      PropStateElem{
        std::move(ty),
        &prop.typeConstraints,
        prop.attrs,
        everModified
      }
    );
  }
  return state;
}

/*
 * Entry point for static property lookups from the Index. Return
 * metadata about a `cls'::`name' static property access in the given
 * context.
 */
PropLookupResult Index::lookup_static(Context ctx,
                                      const PropertiesInfo& privateProps,
                                      const Type& cls,
                                      const Type& name) const {
  ITRACE(4, "lookup_static: {} {}::${}\n", show(ctx), show(cls), show(name));
  Trace::Indent _;

  using R = PropLookupResult;

  // First try to obtain the property name as a static string
  auto const sname = [&] () -> SString {
    // Treat non-string names conservatively, but the caller should be
    // checking this.
    if (!is_specialized_string(name)) return nullptr;
    return sval_of(name);
  }();

  // Conservative result when we can't do any better. The type can be
  // anything, and anything might throw.
  auto const conservative = [&] {
    ITRACE(4, "conservative\n");
    return R{
      TInitCell,
      sname,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      true
    };
  };

  // If we don't know what `cls' is, there's not much we can do.
  if (!is_specialized_cls(cls)) return conservative();

  // Turn the context class into a ClassInfo* for convenience.
  const ClassInfo* ctxCls = nullptr;
  if (ctx.cls) {
    // I don't think this can ever fail (we should always be able to
    // resolve the class since we're currently processing it). If it
    // does, be conservative.
    auto const rCtx = resolve_class(ctx.cls->name);
    if (!rCtx) return conservative();
    ctxCls = rCtx->cinfo();
    if (!ctxCls) return conservative();
  }

  auto const& dcls = dcls_of(cls);
  auto const start = dcls.cls();

  Optional<R> result;
  auto const resolved = visit_every_dcls_cls(
    dcls,
    [&] (const ClassInfo* cinfo) {
      auto r = lookup_static_impl(
        *m_data,
        ctx,
        ctxCls,
        privateProps,
        cinfo,
        sname,
        dcls.isSub() && !sname && cinfo != start.cinfo()
      );
      ITRACE(4, "{} -> {}\n", cinfo->cls->name, show(r));
      if (!result) {
        result.emplace(std::move(r));
      } else {
        *result |= r;
      }
      return true;
    }
  );
  if (!resolved) return conservative();
  assertx(result.has_value());

  ITRACE(4, "union -> {}\n", show(*result));
  return *result;
}

Type Index::lookup_public_prop(const Type& obj, const Type& name) const {
  if (!is_specialized_obj(obj)) return TCell;

  if (!is_specialized_string(name)) return TCell;
  auto const sname = sval_of(name);

  auto ty = TBottom;
  auto const resolved = visit_every_dcls_cls(
    dobj_of(obj),
    [&] (const ClassInfo* cinfo) {
      ty |= lookup_public_prop_impl(
        *m_data,
        cinfo,
        sname
      );
      return ty.strictSubtypeOf(TCell);
    }
  );
  if (!resolved) return TCell;
  return ty;
}

Type Index::lookup_public_prop(const php::Class* cls, SString name) const {
  auto const it = m_data->classInfo.find(cls->name);
  if (it == end(m_data->classInfo)) {
    return TCell;
  }
  return lookup_public_prop_impl(*m_data, it->second, name);
}

Slot Index::lookup_iface_vtable_slot(const php::Class* cls) const {
  return folly::get_default(m_data->ifaceSlotMap, cls->name, kInvalidSlot);
}

//////////////////////////////////////////////////////////////////////

/*
 * Entry point for static property type mutation from the Index. Merge
 * `val' into the known type for any accessible `cls'::`name' static
 * property. The mutation will be recovered into either
 * `publicMutations' or `privateProps' depending on the properties
 * found. Mutations to AttrConst properties are ignored, unless
 * `ignoreConst' is true.
 */
PropMergeResult Index::merge_static_type(
    Context ctx,
    PublicSPropMutations& publicMutations,
    PropertiesInfo& privateProps,
    const Type& cls,
    const Type& name,
    const Type& val,
    bool checkUB,
    bool ignoreConst,
    bool mustBeReadOnly) const {
  ITRACE(
    4, "merge_static_type: {} {}::${} {}\n",
    show(ctx), show(cls), show(name), show(val)
  );
  Trace::Indent _;

  assertx(val.subtypeOf(BInitCell));

  using R = PropMergeResult;

  // In some cases we might try to merge Bottom if we're in
  // unreachable code. This won't affect anything, so just skip out
  // early.
  if (val.subtypeOf(BBottom)) return R{ TBottom, TriBool::No };

  // Try to turn the given property name into a static string
  auto const sname = [&] () -> SString {
    // Non-string names are treated conservatively here. The caller
    // should be checking for these and doing the right thing.
    if (!is_specialized_string(name)) return nullptr;
    return sval_of(name);
  }();

  // The case where we don't know `cls':
  auto const unknownCls = [&] {
    if (!sname) {
      // Very bad case. We don't know `cls' or the property name. This
      // mutation can be affecting anything, so merge it into all
      // properties (this drops type information for public
      // properties).
      ITRACE(4, "unknown class and prop. merging everything\n");
      publicMutations.mergeUnknown(ctx);
      privateProps.mergeInAllPrivateStatics(
        IndexAdaptor { *this }, unctx(val), ignoreConst, mustBeReadOnly
      );
    } else {
      // Otherwise we don't know `cls', but do know the property
      // name. We'll store this mutation separately and union it in to
      // any lookup with the same name.
      ITRACE(4, "unknown class. merging all props with name {}\n", sname);

      publicMutations.mergeUnknownClass(sname, unctx(val));

      // Assume that it could possibly affect any private property with
      // the same name.
      privateProps.mergeInPrivateStatic(
        IndexAdaptor { *this }, sname, unctx(val), ignoreConst, mustBeReadOnly
      );
    }

    // To be conservative, say we might throw and be conservative about
    // conversions.
    return PropMergeResult{
      loosen_likeness(val),
      TriBool::Maybe
    };
  };

  // check if we can determine the class.
  if (!is_specialized_cls(cls)) return unknownCls();

  const ClassInfo* ctxCls = nullptr;
  if (ctx.cls) {
    auto const rCtx = resolve_class(ctx.cls->name);
    // We should only be not able to resolve our own context if the
    // class is not instantiable. In that case, the merge can't
    // happen.
    if (!rCtx) return R{ TBottom, TriBool::No };
    ctxCls = rCtx->cinfo();
    if (!ctxCls) return unknownCls();
  }

  auto const mergePublic = [&] (const ClassInfo* ci,
                                const php::Prop& prop,
                                const Type& val) {
    publicMutations.mergeKnown(ci, prop, val);
  };

  auto const& dcls = dcls_of(cls);
  Optional<res::Class> start;
  if (dcls.isExact() || dcls.isSub()) {
    start = dcls.cls();
  } else if (dcls.isIsectAndExact()) {
    start = dcls.isectAndExact().first;
  }

  Optional<R> result;
  auto const resolved = visit_every_dcls_cls(
    dcls,
    [&] (const ClassInfo* cinfo) {
      auto r = merge_static_type_impl(
        *m_data,
        ctx,
        mergePublic,
        privateProps,
        ctxCls,
        cinfo,
        sname,
        val,
        checkUB,
        ignoreConst,
        mustBeReadOnly,
        dcls.isSub() && !sname && cinfo != start->cinfo()
      );
      ITRACE(4, "{} -> {}\n", cinfo->cls->name, show(r));
      if (!result) {
        result.emplace(std::move(r));
      } else {
        *result |= r;
      }
      return true;
    }
  );
  if (!resolved) return unknownCls();
  assertx(result.has_value());
  ITRACE(4, "union -> {}\n", show(*result));
  return *result;
}

//////////////////////////////////////////////////////////////////////

DependencyContext Index::dependency_context(const Context& ctx) const {
  return dep_context(*m_data, ctx);
}

bool Index::using_class_dependencies() const {
  return m_data->useClassDependencies;
}

void Index::use_class_dependencies(bool f) {
  if (f != m_data->useClassDependencies) {
    m_data->dependencyMap.clear();
    m_data->useClassDependencies = f;
  }
}

void Index::refine_class_constants(const Context& ctx,
                                   const ResolvedConstants& resolved,
                                   DependencyContextSet& deps) {
  if (resolved.empty()) return;

  auto changed = false;
  auto const cls = ctx.func->cls;
  assertx(cls);
  auto& constants = cls->constants;

  for (auto& c : resolved) {
    assertx(c.first < constants.size());
    auto& cnst = constants[c.first];
    assertx(cnst.kind == ConstModifierFlags::Kind::Value);

    always_assert(cnst.val && type(*cnst.val) == KindOfUninit);
    if (auto const val = tv(c.second.type)) {
      assertx(val->m_type != KindOfUninit);
      cnst.val = *val;
      // Deleting from the types map is too expensive, so just leave
      // any entry. We won't look at it if val is set.
      changed = true;
    } else if (auto const cinfo =
               folly::get_default(m_data->classInfo, cls->name)) {
      auto& old = [&] () -> ClsConstInfo& {
        if (c.first >= cinfo->clsConstTypes.size()) {
          auto const newSize = std::max(c.first+1, resolved.back().first+1);
          cinfo->clsConstTypes.resize(newSize, ClsConstInfo { TInitCell, 0 });
        }
        return cinfo->clsConstTypes[c.first];
      }();

      if (c.second.type.strictlyMoreRefined(old.type)) {
        always_assert(c.second.refinements > old.refinements);
        old = std::move(c.second);
        changed = true;
      } else {
        always_assert_flog(
          c.second.type.moreRefined(old.type),
          "Class constant type invariant violated for {}::{}\n"
          "    {} is not at least as refined as {}\n",
          ctx.func->cls->name,
          cnst.name,
          show(c.second.type),
          show(old.type)
        );
      }
    }
  }

  if (changed) {
    find_deps(*m_data, ctx.func, Dep::ClsConst, deps);
  }
}

void Index::refine_constants(const FuncAnalysisResult& fa,
                             DependencyContextSet& deps) {
  auto const& func = fa.ctx.func;
  if (func->cls) return;

  auto const cns_name = Constant::nameFromFuncName(func->name);
  if (!cns_name) return;

  auto const cns = m_data->constants.at(cns_name);
  auto const val = tv(fa.inferredReturn);
  if (!val) {
    always_assert_flog(
      type(cns->val) == KindOfUninit,
      "Constant value invariant violated in {}.\n"
      "    Value went from {} to {}",
      cns_name,
      show(from_cell(cns->val)),
      show(fa.inferredReturn)
    );
    return;
  }

  if (type(cns->val) != KindOfUninit) {
    always_assert_flog(
      equal(from_cell(cns->val), fa.inferredReturn),
      "Constant value invariant violated in {}.\n"
      "    Value went from {} to {}",
      cns_name,
      show(from_cell(cns->val)),
      show(fa.inferredReturn)
    );
  } else {
    cns->val = *val;
  }

  find_deps(*m_data, func, Dep::ConstVal, deps);
}

void Index::refine_return_info(const FuncAnalysisResult& fa,
                               DependencyContextSet& deps) {
  auto const& func = fa.ctx.func;
  auto const finfo = func_info(*m_data, func);

  auto const error_loc = [&] {
    return folly::sformat(
      "{} {}{}",
      func->unit,
      func->cls
        ? folly::to<std::string>(func->cls->name->data(), "::")
        : std::string{},
      func->name
    );
  };

  auto dep = Dep{};
  if (finfo->retParam == NoLocalId && fa.retParam != NoLocalId) {
    // This is just a heuristic; it doesn't mean that the value passed
    // in was returned, but that the value of the parameter at the
    // point of the RetC was returned. We use it to make (heuristic)
    // decisions about whether to do inline interps, so we only allow
    // it to change once (otherwise later passes might not do the
    // inline interp, and get worse results, which could trigger other
    // assertions in Index::refine_*).
    dep = Dep::ReturnTy;
    finfo->retParam = fa.retParam;
  }

  auto unusedParams = ~fa.usedParams;
  if (finfo->unusedParams != unusedParams) {
    dep = Dep::ReturnTy;
    always_assert_flog(
        (finfo->unusedParams | unusedParams) == unusedParams,
        "Index unusedParams decreased in {}.\n",
        error_loc()
    );
    finfo->unusedParams = unusedParams;
  }

  auto resetFuncFamilies = false;
  if (fa.inferredReturn.strictlyMoreRefined(finfo->returnTy)) {
    if (finfo->returnRefinements < options.returnTypeRefineLimit) {
      finfo->returnTy = fa.inferredReturn;
      // We've modifed the return type, so reset any cached FuncFamily
      // return types.
      resetFuncFamilies = true;
      dep = is_scalar(fa.inferredReturn)
        ? Dep::ReturnTy | Dep::InlineDepthLimit : Dep::ReturnTy;
      finfo->returnRefinements += fa.localReturnRefinements + 1;
      if (finfo->returnRefinements > options.returnTypeRefineLimit) {
        FTRACE(1, "maxed out return type refinements at {}\n", error_loc());
      }
    } else {
      FTRACE(1, "maxed out return type refinements at {}\n", error_loc());
    }
  } else {
    always_assert_flog(
      fa.inferredReturn.moreRefined(finfo->returnTy),
      "Index return type invariant violated in {}.\n"
      "   {} is not at least as refined as {}\n",
      error_loc(),
      show(fa.inferredReturn),
      show(finfo->returnTy)
    );
  }

  always_assert_flog(
    !finfo->effectFree || fa.effectFree,
    "Index effectFree changed from true to false in {} {}.\n",
    func->unit,
    func_fullname(*func)
  );

  if (finfo->effectFree != fa.effectFree) {
    finfo->effectFree = fa.effectFree;
    dep = Dep::InlineDepthLimit | Dep::ReturnTy;
  }

  if (dep != Dep{}) {
    find_deps(*m_data, func, dep, deps);
    if (resetFuncFamilies) {
      assertx(has_dep(dep, Dep::ReturnTy));
      for (auto const ff : finfo->families) {
        // Reset the cached return type information for all the
        // FuncFamilies this function is a part of. Always reset the
        // "all" information, and if there's regular subset
        // information, reset that too.
        if (!ff->m_all.m_returnTy.reset() &&
            (!ff->m_regular || !ff->m_regular->m_returnTy.reset())) {
          continue;
        }
        // Only load the deps for this func family if we're the ones
        // who successfully reset. Only one thread needs to do it.
        find_deps(*m_data, ff, Dep::ReturnTy, deps);
      }
    }
  }

  if (fa.reanalyzeOnUpdate) {
    // Insert the dependency on itself to redo analysis.
    deps.emplace(dep_context(*m_data, fa.ctx));
  }
}

bool Index::refine_closure_use_vars(const php::Class* cls,
                                    const CompactVector<Type>& vars) {
  assertx(is_closure(*cls));

  for (auto i = uint32_t{0}; i < vars.size(); ++i) {
    always_assert_flog(
      equal(vars[i], unctx(vars[i])),
      "Closure cannot have a used var with a context dependent type"
    );
  }

  auto& current = [&] () -> CompactVector<Type>& {
    std::lock_guard<std::mutex> _{closure_use_vars_mutex};
    return m_data->closureUseVars[cls];
  }();

  always_assert(current.empty() || current.size() == vars.size());
  if (current.empty()) {
    current = vars;
    return true;
  }

  auto changed = false;
  for (auto i = uint32_t{0}; i < vars.size(); ++i) {
    if (vars[i].strictSubtypeOf(current[i])) {
      changed = true;
      current[i] = vars[i];
    } else {
      always_assert_flog(
        vars[i].moreRefined(current[i]),
        "Index closure_use_var invariant violated in {}.\n"
        "   {} is not at least as refined as {}\n",
        cls->name,
        show(vars[i]),
        show(current[i])
      );
    }
  }

  return changed;
}

template<class Container>
void refine_private_propstate(Container& cont,
                              const php::Class* cls,
                              const PropState& state) {
  assertx(!is_used_trait(*cls));
  auto* elm = [&] () -> typename Container::value_type* {
    std::lock_guard<std::mutex> _{private_propstate_mutex};
    auto it = cont.find(cls);
    if (it == end(cont)) {
      if (!state.empty()) cont[cls] = state;
      return nullptr;
    }
    return &*it;
  }();

  if (!elm) return;

  for (auto& kv : state) {
    auto& target = elm->second[kv.first];
    assertx(target.tc == kv.second.tc);
    always_assert_flog(
      kv.second.ty.moreRefined(target.ty),
      "PropState refinement failed on {}::${} -- {} was not a subtype of {}\n",
      cls->name->data(),
      kv.first->data(),
      show(kv.second.ty),
      show(target.ty)
    );
    target.ty = kv.second.ty;

    if (kv.second.everModified) {
      always_assert_flog(
        target.everModified,
        "PropState refinement failed on {}::${} -- "
        "everModified flag went from false to true\n",
        cls->name->data(),
        kv.first->data()
      );
    } else {
      target.everModified = false;
    }
  }
}

void Index::refine_private_props(const php::Class* cls,
                                 const PropState& state) {
  refine_private_propstate(m_data->privatePropInfo, cls, state);
}

void Index::refine_private_statics(const php::Class* cls,
                                   const PropState& state) {
  // We can't store context dependent types in private statics since they
  // could be accessed using different contexts.
  auto cleanedState = PropState{};
  for (auto const& prop : state) {
    auto& elem = cleanedState[prop.first];
    elem.ty = unctx(prop.second.ty);
    elem.tc = prop.second.tc;
    elem.attrs = prop.second.attrs;
    elem.everModified = prop.second.everModified;
  }

  refine_private_propstate(m_data->privateStaticPropInfo, cls, cleanedState);
}

void Index::record_public_static_mutations(const php::Func& func,
                                           PublicSPropMutations mutations) {
  if (!mutations.m_data) {
    m_data->publicSPropMutations.erase(&func);
    return;
  }
  m_data->publicSPropMutations.insert_or_assign(&func, std::move(mutations));
}

void Index::update_prop_initial_values(const Context& ctx,
                                       const ResolvedPropInits& resolved,
                                       DependencyContextSet& deps) {
  auto& props = const_cast<php::Class*>(ctx.cls)->properties;

  auto changed = false;
  for (auto const& [idx, info] : resolved) {
    assertx(idx < props.size());
    auto& prop = props[idx];

    auto const allResolved = [&] {
      for (auto const& tc : prop.typeConstraints.range()) {
        if (tc.isUnresolved()) return false;
      }
      return true;
    };

    if (info.satisfies) {
      if (!(prop.attrs & AttrInitialSatisfiesTC) && allResolved()) {
        attribute_setter(prop.attrs, true, AttrInitialSatisfiesTC);
        changed = true;
      }
    } else {
      always_assert_flog(
        !(prop.attrs & AttrInitialSatisfiesTC),
        "AttrInitialSatisfiesTC invariant violated for {}::{}\n"
        "  Went from true to false",
        ctx.cls->name, prop.name
      );
    }

    always_assert_flog(
      IMPLIES(!(prop.attrs & AttrDeepInit), !info.deepInit),
      "AttrDeepInit invariant violated for {}::{}\n"
      "  Went from false to true",
      ctx.cls->name, prop.name
    );
    attribute_setter(prop.attrs, info.deepInit, AttrDeepInit);

    if (type(info.val) != KindOfUninit) {
      always_assert_flog(
        type(prop.val) == KindOfUninit ||
        equal(from_cell(prop.val), from_cell(info.val)),
        "Property initial value invariant violated for {}::{}\n"
        "  Value went from {} to {}",
        ctx.cls->name, prop.name,
        show(from_cell(prop.val)), show(from_cell(info.val))
      );
      prop.val = info.val;
    } else {
      always_assert_flog(
        type(prop.val) == KindOfUninit,
        "Property initial value invariant violated for {}::{}\n"
        " Value went from {} to not set",
        ctx.cls->name, prop.name,
        show(from_cell(prop.val))
      );
    }
  }
  if (!changed) return;

  auto const it = m_data->classInfo.find(ctx.cls->name);
  if (it == end(m_data->classInfo)) return;
  auto const cinfo = it->second;

  // Both a pinit and a sinit can have resolved property values. When
  // analyzing constants we'll process each function separately and
  // potentially in different threads. Both will want to inspect the
  // property Attrs and the hasBadInitialPropValues. So, if we reach
  // here, take a lock to ensure both don't stomp on each other.
  static std::array<std::mutex, 256> locks;
  auto& lock = locks[pointer_hash<const php::Class>{}(ctx.cls) % locks.size()];
  std::lock_guard<std::mutex> _{lock};

  auto const noBad = std::all_of(
    begin(props), end(props),
    [] (const php::Prop& prop) {
      return bool(prop.attrs & AttrInitialSatisfiesTC);
    }
  );

  if (cinfo->hasBadInitialPropValues) {
    if (noBad) {
      cinfo->hasBadInitialPropValues = false;
      find_deps(*m_data, ctx.cls, Dep::PropBadInitialValues, deps);
    }
  } else {
    // If it's false, another thread got here before us and set it to
    // false.
    always_assert(noBad);
  }
}

void Index::refine_public_statics(DependencyContextSet& deps) {
  trace_time update("update public statics");

  // Union together the mutations for each function, including the functions
  // which weren't analyzed this round.
  auto nothing_known = false;
  PublicSPropMutations::UnknownMap unknown;
  PublicSPropMutations::KnownMap known;
  for (auto const& mutations : m_data->publicSPropMutations) {
    if (!mutations.second.m_data) continue;
    if (mutations.second.m_data->m_nothing_known) {
      nothing_known = true;
      break;
    }

    for (auto const& kv : mutations.second.m_data->m_unknown) {
      auto const ret = unknown.insert(kv);
      if (!ret.second) ret.first->second |= kv.second;
    }
    for (auto const& kv : mutations.second.m_data->m_known) {
      auto const ret = known.insert(kv);
      if (!ret.second) ret.first->second |= kv.second;
    }
  }

  if (nothing_known) {
    // We cannot go from knowing the types to not knowing the types (this is
    // equivalent to widening the types).
    always_assert(!m_data->seenPublicSPropMutations);
    return;
  }
  m_data->seenPublicSPropMutations = true;

  std::vector<DependencyContextSet> deps_vec{parallel::num_threads};

  // Refine known class state
  parallel::for_each(
    m_data->allClassInfos,
    [&] (std::unique_ptr<ClassInfo>& cinfo, size_t worker) {
      assertx(worker < deps_vec.size());

      for (auto const& prop : cinfo->cls->properties) {
        if (!(prop.attrs & (AttrPublic|AttrProtected)) ||
            !(prop.attrs & AttrStatic)) {
          continue;
        }

        auto knownClsType = [&] {
          auto const it = known.find(
            PublicSPropMutations::KnownKey { cinfo.get(), prop.name }
          );
          // If we didn't see a mutation, the type is TBottom.
          return it == end(known) ? TBottom : it->second;
        }();

        auto unknownClsType = [&] {
          auto const it = unknown.find(prop.name);
          // If we didn't see a mutation, the type is TBottom.
          return it == end(unknown) ? TBottom : it->second;
        }();

        // We can't keep context dependent types in public properties.
        auto newType = adjust_type_for_prop(
          IndexAdaptor { *this },
          *cinfo->cls,
          &prop.typeConstraints,
          unctx(union_of(std::move(knownClsType), std::move(unknownClsType)))
        );

        auto& entry = cinfo->publicStaticProps[prop.name];

        if (!newType.is(BBottom)) {
          always_assert_flog(
            entry.everModified,
            "Static property index invariant violated on {}::{}:\n"
            " everModified flag went from false to true",
            cinfo->cls->name,
            prop.name
          );
        } else {
          entry.everModified = false;
        }

        // The type from the mutations doesn't contain the in-class
        // initializer types. Add that here.
        auto effectiveType = union_of(
          std::move(newType),
          initial_type_for_public_sprop(*this, *cinfo->cls, prop)
        );

        /*
         * We may only shrink the types we recorded for each property. (If a
         * property type ever grows, the interpreter could infer something
         * incorrect at some step.)
         */
        always_assert_flog(
          effectiveType.subtypeOf(entry.inferredType),
          "Static property index invariant violated on {}::{}:\n"
          "  {} is not a subtype of {}",
          cinfo->cls->name,
          prop.name,
          show(effectiveType),
          show(entry.inferredType)
        );

        // Put a limit on the refinements to ensure termination. Since
        // we only ever refine types, we can stop at any point and still
        // maintain correctness.
        if (effectiveType.strictSubtypeOf(entry.inferredType)) {
          if (entry.refinements + 1 < options.publicSPropRefineLimit) {
            find_deps(*m_data, &prop, Dep::PublicSProp, deps_vec[worker]);
            entry.inferredType = std::move(effectiveType);
            ++entry.refinements;
          } else {
            FTRACE(
              1, "maxed out public static property refinements for {}:{}\n",
              cinfo->cls->name,
              prop.name
            );
          }
        }
      }
    }
  );

  for (auto& dv : deps_vec) {
    for (auto& d : dv) deps.insert(d);
  }
}

bool Index::frozen() const {
  return m_data->frozen;
}

void Index::freeze() {
  m_data->frozen = true;
  m_data->ever_frozen = true;
}

bool AnalysisIndex::tracking_public_sprops() const {
  // Not implemented yet
  return false;
}

Type AnalysisIndex::unserialize_type(Type t) const {
  if (!might_have_dcls(t)) {
    return unserialize_classes(AnalysisIndexAdaptor { *this }, std::move(t));
  }

  IndexData::UnserializeKey key{
    fc_from_context(context_for_deps(*m_data), *m_data), t
  };
  if (auto const c = folly::get_ptr(m_data->unserializeCache, key)) {
    return *c;
  }
  t = unserialize_classes(AnalysisIndexAdaptor { *this }, std::move(t));
  m_data->unserializeCache.emplace(key, t);
  return t;
}

Type AnalysisIndex::serialize_type(Type t) const {
  return serialize_classes(std::move(t));
}

//////////////////////////////////////////////////////////////////////

Index::ReturnType
AnalysisIndex::return_type_for_func(const php::Func& func) const {
  using R = Index::ReturnType;

  auto const& finfo = func_info(*m_data, func);
  auto const inferred = unctx(unserialize_type(finfo.inferred.returnTy));
  auto effectFree = finfo.inferred.effectFree;
  auto ret = inferred;

  if (auto const rinfo = retained_for_context(*m_data)) {
    auto better = false;
    if (auto const info = rinfo->get(finfo)) {
      ret &= unctx(unserialize_type(info->returnTy));
      better = ret.strictSubtypeOf(inferred);
      if (!effectFree && info->effectFree) {
        effectFree = true;
        better = true;
      }
    }

    if (should_retain(finfo, better, *m_data)) {
      ITRACE_MOD(
        Trace::hhbbc, 4,
        "Retaining inferred return type information ({}{}) about {}\n",
        show(ret),
        effectFree ? " effect-free" : "",
        func_fullname(func)
      );
      auto& info = rinfo->retain(finfo);
      info.returnTy = serialize_type(ret);
      info.effectFree = effectFree;
    }
  }

  return R{ std::move(ret), effectFree };
}

ClsConstInfo
AnalysisIndex::info_for_class_constant(const php::Class& cls,
                                       const php::Const& cns) const {
  auto const inferred = folly::get_default(
    cls.cinfo->inferred.clsConstantInfo,
    cns.name,
    ClsConstInfo{ TInitCell, 0 }
  );
  auto const inferredTy = unserialize_type(inferred.type);
  auto ret = inferredTy;

  if (auto const retained = retained_for_context(*m_data)) {
    auto better = false;
    if (auto const info = retained->get(*cls.cinfo)) {
      if (auto const r = folly::get_ptr(info->clsConstantInfo, cns.name)) {
        ret &= unserialize_type(r->type);
        better = ret.strictSubtypeOf(inferredTy);
      }
    }

    if (should_retain(*cls.cinfo, better, *m_data)) {
      ITRACE_MOD(
        Trace::hhbbc, 4,
        "Retaining inferred class constant type ({}) about {}::{}\n",
        show(ret),
        cls.name,
        cns.name
      );
      auto& info = retained->retain(*cls.cinfo);
      info.clsConstantInfo.insert_or_assign(
        cns.name,
        ClsConstInfo { serialize_type(ret), inferred.refinements }
      );
    }
  }

  return ClsConstInfo { std::move(ret), inferred.refinements };
}

//////////////////////////////////////////////////////////////////////

/*
 * Note that these functions run in separate threads, and
 * intentionally don't bump Trace::hhbbc_time. If you want to see
 * these times, set TRACE=hhbbc_time:1
 */
#define CLEAR(x)                                \
  {                                             \
    trace_time _{"clearing " #x};               \
    _.ignore_client_stats();                    \
    (x).clear();                                \
  }

void Index::cleanup_for_final() {
  trace_time _{"cleanup for final", m_data->sample};
  CLEAR(m_data->dependencyMap);
}

void Index::cleanup_post_emit() {
  trace_time _{"cleanup post emit", m_data->sample};

  std::vector<std::function<void()>> clearers;
  #define CLEAR_PARALLEL(x) clearers.push_back([&] CLEAR(x));
  CLEAR_PARALLEL(m_data->classes);
  CLEAR_PARALLEL(m_data->funcs);
  CLEAR_PARALLEL(m_data->typeAliases);
  CLEAR_PARALLEL(m_data->enums);
  CLEAR_PARALLEL(m_data->constants);
  CLEAR_PARALLEL(m_data->modules);
  CLEAR_PARALLEL(m_data->units);

  CLEAR_PARALLEL(m_data->classExtraMethodMap);

  CLEAR_PARALLEL(m_data->classInfo);

  CLEAR_PARALLEL(m_data->privatePropInfo);
  CLEAR_PARALLEL(m_data->privateStaticPropInfo);
  CLEAR_PARALLEL(m_data->publicSPropMutations);
  CLEAR_PARALLEL(m_data->ifaceSlotMap);
  CLEAR_PARALLEL(m_data->closureUseVars);

  CLEAR_PARALLEL(m_data->methodFamilies);

  CLEAR_PARALLEL(m_data->funcFamilies);
  CLEAR_PARALLEL(m_data->funcFamilyStaticInfos);

  CLEAR_PARALLEL(m_data->clsConstLookupCache);

  CLEAR_PARALLEL(m_data->foldableReturnTypeMap);
  CLEAR_PARALLEL(m_data->contextualReturnTypes);

  parallel::for_each(clearers, [] (const std::function<void()>& f) { f(); });

  {
    trace_time t{"reset funcInfo"};
    t.ignore_client_stats();
    parallel::for_each(
      m_data->funcInfo,
      [] (auto& u) {
        u.returnTy = TBottom;
        u.families.clear();
      }
    );
    m_data->funcInfo.clear();
  }

  // Class-infos and program need to be freed after all Type instances
  // are destroyed, as Type::checkInvariants may try to access them.

  {
    trace_time t{"reset allClassInfos"};
    t.ignore_client_stats();
    parallel::for_each(m_data->allClassInfos, [] (auto& u) { u.reset(); });
    m_data->allClassInfos.clear();
  }

  {
    trace_time t{"reset program"};
    t.ignore_client_stats();
    parallel::for_each(m_data->program->units, [] (auto& u) { u.reset(); });
    parallel::for_each(m_data->program->classes, [] (auto& u) { u.reset(); });
    parallel::for_each(m_data->program->funcs, [] (auto& f) { f.reset(); });
    m_data->program.reset();
  }
}

void Index::thaw() {
  m_data->frozen = false;
}

//////////////////////////////////////////////////////////////////////

FuncClsUnit AnalysisWorklist::next() {
  if (list.empty()) return FuncClsUnit{};
  auto n = list.front();
  in.erase(n);
  list.pop_front();
  return n;
}

FuncClsUnit AnalysisWorklist::peek() const {
  if (list.empty()) return FuncClsUnit{};
  return list.front();
}

void AnalysisWorklist::schedule(FuncClsUnit fc) {
  assertx(IMPLIES(fc.cls(), !is_closure(*fc.cls())));
  if (!in.emplace(fc).second) return;
  ITRACE(2, "scheduling {} onto worklist\n", show(fc));
  list.emplace_back(fc);
}

void AnalysisWorklist::sort() {
  std::sort(
    begin(list), end(list),
    [] (FuncClsUnit f1, FuncClsUnit f2) { return f1.stableLT(f2); }
  );
}

//////////////////////////////////////////////////////////////////////

AnalysisDeps::Type AnalysisDeps::add(Class c, Type t) {
  return merge(classes[c.name], t | Type::Meta);
}

bool AnalysisDeps::add(ConstIndex cns, bool inTypeCns) {
  // Dependency on class constant implies a dependency on the class as
  // well.
  add(Class { cns.cls }, Type::Meta);
  return inTypeCns
    ? typeCnsClsConstants.emplace(cns).second
    : clsConstants.emplace(cns).second;
}

bool AnalysisDeps::add(Constant cns) {
  // Dependency on top-level constant implies a dependency on the
  // 86cinit initialized as well (which may not even exist).
  add(Func { HPHP::Constant::funcNameFromName(cns.name) }, Type::RetType);
  return constants.emplace(cns.name).second;
}

bool AnalysisDeps::add(AnyClassConstant any, bool inTypeCns) {
  // Dependency on class constant implies a dependency on the class as
  // well.
  add(Class { any.name }, Type::Meta);
  return inTypeCns
    ? typeCnsAnyClsConstants.emplace(any.name).second
    : anyClsConstants.emplace(any.name).second;
}

bool AnalysisDeps::add(Property p) {
  // Dependency on a property implies a dependency on the class as
  // well.
  add(Class { p.cls }, Type::Meta);
  return properties.emplace(std::move(p)).second;
}

bool AnalysisDeps::add(AnyProperty p) {
  // Dependency on a class' properties implies a dependency on the
  // class as well.
  add(Class { p.cls }, Type::Meta);
  return anyProperties.emplace(p.cls).second;
}

AnalysisDeps::Type AnalysisDeps::add(const php::Func& f, Type t) {
  return f.cls
    ? add(MethRef { f }, t)
    : add(Func { f.name }, t);
}

AnalysisDeps::Type AnalysisDeps::add(MethRef m, Type t) {
  add(Class { m.cls }, Type::Meta);
  return merge(methods[m], t | Type::Meta);
}

AnalysisDeps::Type AnalysisDeps::add(Func f, Type t) {
  return merge(funcs[f.name], t | Type::Meta);
}

AnalysisDeps::Type AnalysisDeps::merge(Type& o, Type n) {
  auto const added = n - o;
  o |= n;
  return added;
}

bool AnalysisDeps::empty() const {
  return
    funcs.empty() &&
    methods.empty() &&
    classes.empty() &&
    clsConstants.empty() &&
    constants.empty() &&
    anyClsConstants.empty() &&
    typeCnsClsConstants.empty() &&
    typeCnsAnyClsConstants.empty() &&
    properties.empty() &&
    anyProperties.empty();
}

AnalysisDeps& AnalysisDeps::operator|=(const AnalysisDeps& o) {
  clsConstants.insert(begin(o.clsConstants), end(o.clsConstants));
  constants.insert(begin(o.constants), end(o.constants));
  anyClsConstants.insert(begin(o.anyClsConstants), end(o.anyClsConstants));
  typeCnsClsConstants.insert(
    begin(o.typeCnsClsConstants),
    end(o.typeCnsClsConstants)
  );
  typeCnsAnyClsConstants.insert(
    begin(o.typeCnsAnyClsConstants),
    end(o.typeCnsAnyClsConstants)
  );
  properties.insert(
    begin(o.properties),
    end(o.properties)
  );
  anyProperties.insert(
    begin(o.anyProperties),
    end(o.anyProperties)
  );
  for (auto const [name, t] : o.funcs) funcs[name] |= t;
  for (auto const [name, t] : o.classes) classes[name] |= t;
  for (auto const [meth, t] : o.methods) methods[meth] |= t;
  return *this;
}

void AnalysisDeps::clean() {
  // Remove anything from class dependencies which is redundant due to
  // a more detailed dependency.
  auto const maybeRemove = [&] (SString cls) {
    auto const t = folly::get_ptr(classes, cls);
    if (t && ((*t & kValidForChanges) == Type::None)) {
      classes.erase(cls);
    }
  };
  for (auto const& cns : clsConstants)   maybeRemove(cns.cls);
  for (auto const cls : anyClsConstants) maybeRemove(cls);
  for (auto const& prop : properties)    maybeRemove(prop.cls);
  for (auto const cls : anyProperties)   maybeRemove(cls);
}

std::string show(AnalysisDeps::Type t) {
  using T = AnalysisDeps::Type;
  std::string out;
  auto const add = [&] (const char* s) {
    folly::format(&out, "{}{}", out.empty() ? "" : ",", s);
  };
  if (t & T::Meta)          add("meta");
  if (t & T::RetType)       add("return type");
  if (t & T::ScalarRetType) add("scalar return type");
  if (t & T::RetParam)      add("returned param");
  if (t & T::UnusedParams)  add("unused params");
  if (t & T::Bytecode)      add("bytecode");
  if (t & T::UseVars)       add("use-vars");
  if (t & T::ClassInitMightRaise) add("class-init-might-raise");
  if (t & T::PropInitVals)  add("prop-init-vals");
  return out;
}

std::string show(const AnalysisDeps& d) {
  using namespace folly::gen;
  auto const toCpp = [] (SString n) { return n->toCppString(); };

  std::string out;
  if (!d.classes.empty()) {
    folly::format(
      &out, "  classes: {}\n",
      from(d.classes)
        | map([&] (auto const& p) {
            return folly::sformat("{} -> [{}]", toCpp(p.first), show(p.second));
          })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.funcs.empty()) {
    folly::format(
      &out, "  funcs: {}\n",
      from(d.funcs)
        | map([&] (auto const& p) {
            return folly::sformat("{} -> [{}]", toCpp(p.first), show(p.second));
          })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.methods.empty()) {
    folly::format(
      &out, "  methods: {}\n",
      from(d.methods)
        | map([] (auto const& p) {
            return folly::sformat("{} -> [{}]", show(p.first), show(p.second));
          })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.constants.empty()) {
    folly::format(
      &out, "  constants: {}\n",
      from(d.constants) | map(toCpp) | unsplit<std::string>(", ")
    );
  }
  if (!d.clsConstants.empty()) {
    folly::format(
      &out, "  class-constants: {}\n",
      from(d.clsConstants)
        | map([] (ConstIndex idx) { return show(idx); })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.anyClsConstants.empty()) {
    folly::format(
      &out, "  any class-constants: {}\n",
      from(d.anyClsConstants) | map(toCpp) | unsplit<std::string>(", ")
    );
  }
  if (!d.typeCnsClsConstants.empty()) {
    folly::format(
      &out, "  type-cns class-constants: {}\n",
      from(d.typeCnsClsConstants)
        | map([] (ConstIndex idx) { return show(idx); })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.typeCnsAnyClsConstants.empty()) {
    folly::format(
      &out, "  type-cns any class-constants: {}\n",
      from(d.typeCnsAnyClsConstants) | map(toCpp) | unsplit<std::string>(", ")
    );
  }
  if (!d.properties.empty()) {
    folly::format(
      &out, "  properties: {}\n",
      from(d.properties)
        | map([] (auto const& p) {
            return folly::sformat("{}::{}", p.cls, p.prop);
          })
        | unsplit<std::string>(", ")
    );
  }
  if (!d.anyProperties.empty()) {
    folly::format(
      &out, "  any properties: {}\n",
      from(d.anyProperties) | map(toCpp) | unsplit<std::string>(", ")
    );
  }
  if (out.empty()) out = "  (none)\n";
  return out;
}

//////////////////////////////////////////////////////////////////////

void AnalysisChangeSet::changed(ConstIndex idx) {
  clsConstants.emplace(idx);
}

void AnalysisChangeSet::changed(const php::Constant& c) {
  constants.emplace(c.name);
}

void AnalysisChangeSet::changed(const php::Class& c, Type t) {
  assertx(AnalysisDeps::isValidForChanges(t));
  classes[c.name] |= t;
}

void AnalysisChangeSet::changed(const php::Func& f, Type t) {
  assertx(AnalysisDeps::isValidForChanges(t));
  if (f.cls) {
    methods[MethRef { f }] |= t;
  } else {
    funcs[f.name] |= t;
  }
}

void AnalysisChangeSet::changed(const php::Class& c, const php::Prop& p) {
  properties.emplace(c.name, p.name);
}

void AnalysisChangeSet::fixed(ConstIndex idx) {
  fixedClsConstants.emplace(idx);
}

void AnalysisChangeSet::fixed(const php::Class& cls) {
  allClsConstantsFixed.emplace(cls.name);
}

void AnalysisChangeSet::fixed(const php::Unit& unit) {
  unitsFixed.emplace(unit.filename);
}

void AnalysisChangeSet::typeCnsName(const php::Class& cls,
                                    Class name) {
  if (cls.name->tsame(name.name)) return;
  clsTypeCnsNames[cls.name].emplace(name.name);
}

void AnalysisChangeSet::typeCnsName(const php::Unit& unit,
                                    Class name) {
  unitTypeCnsNames[unit.filename].emplace(name.name);
}

void AnalysisChangeSet::filter(const TSStringSet& keepClasses,
                               const FSStringSet& keepFuncs,
                               const SStringSet& keepUnits,
                               const SStringSet& keepConstants) {
  folly::erase_if(
    funcs, [&] (auto const& p) { return !keepFuncs.contains(p.first); }
  );
  folly::erase_if(
    classes, [&] (auto const& p) { return !keepClasses.contains(p.first); }
  );
  folly::erase_if(
    methods, [&] (auto const& p) { return !keepClasses.contains(p.first.cls); }
  );
  folly::erase_if(
    constants, [&] (SString s) { return !keepConstants.contains(s); }
  );
  folly::erase_if(
    clsConstants, [&] (ConstIndex idx) { return !keepClasses.contains(idx.cls); }
  );
  folly::erase_if(
    fixedClsConstants,
    [&] (ConstIndex idx) {
      return !keepClasses.contains(idx.cls) || allClsConstantsFixed.contains(idx.cls);
    }
  );
  folly::erase_if(
    allClsConstantsFixed, [&] (SString s) { return !keepClasses.contains(s); }
  );
  folly::erase_if(
    unitsFixed, [&] (SString s) { return !keepUnits.contains(s); }
  );
  folly::erase_if(
    properties, [&] (const Property& p) { return !keepClasses.contains(p.cls); }
  );
  folly::erase_if(
    clsTypeCnsNames, [&] (auto const& p) { return !keepClasses.contains(p.first); }
  );
  folly::erase_if(
    unitTypeCnsNames, [&] (auto const& p) { return !keepUnits.contains(p.first); }
  );
}

//////////////////////////////////////////////////////////////////////

std::vector<SString> AnalysisInput::reportBundleNames() const {
  assertx(meta.bundleNames.size() >= reportBundles.size());
  return std::vector<SString>{
    begin(meta.bundleNames),
    begin(meta.bundleNames) + reportBundles.size()
  };
}

AnalysisInput::Tuple AnalysisInput::toInput(extern_worker::Ref<Meta> m) const {
  return std::make_tuple(
    reportBundles,
    noReportBundles,
    std::move(m)
  );
}

//////////////////////////////////////////////////////////////////////

/*
 * Intermediate bucket representation used during scheduling.
 *
 * Categorizes bundles into three mutually exclusive types based on their
 * role in the analysis job:
 *
 * - reportBundles: Bundles containing entities whose analysis results should
 *   be reported back. These are the "primary" work items for this bucket.
 *
 * - noReportBundles: Bundles containing entities that should be analyzed but
 *   whose results should NOT be reported. These are on someone else's trace -
 *   we analyze them speculatively to short-circuit dependency chains, but
 *   they'll be properly reported when scheduled as reportBundles later.
 *
 * - pureDepBundles: Bundles containing entities that should NOT be analyzed
 *   at all. These are stable dependencies (ineligible this round) that are
 *   needed only to provide information for analyzing other entities.
 *
 * - badClasses/badFuncs/badConstants: Missing/undefined entities that are
 *   referenced as dependencies. These must be passed to the job even though
 *   they don't exist so the analysis can handle them appropriately.
 */
struct AnalysisScheduler::Bucket {
  std::vector<SString> reportBundles;
  std::vector<SString> noReportBundles;
  std::vector<SString> pureDepBundles;
  std::vector<SString> badClasses;
  std::vector<SString> badFuncs;
  std::vector<SString> badConstants;
};

//////////////////////////////////////////////////////////////////////

AnalysisScheduler::AnalysisScheduler(Index& index)
  : index{index}
  , totalWorkItems{0}
  , lock{std::make_unique<std::mutex>()} {}

AnalysisScheduler::~AnalysisScheduler() = default;

AnalysisScheduler::AnalysisScheduler(AnalysisScheduler&&) = default;

Trace::Bump AnalysisScheduler::bumpFor(const DepState& state) const {
  if constexpr (Trace::enabled) {
    switch (state.kind) {
      case DepState::Kind::Func:
        return bump_for_func(*index.m_data, state.name);
      case DepState::Kind::Class:
        return bump_for_class(*index.m_data, state.name);
      case DepState::Kind::Unit:
        return bump_for_unit(*index.m_data, state.name);
    }
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

Trace::Bump AnalysisScheduler::bumpFor(const TraceState& state) const {
  if constexpr (Trace::enabled) {
    assertx(!state.depStates.empty());

    int bump = 0;
    auto first = true;
    auto const add = [&] (int b) {
      if (first) {
        bump = b;
        first = false;
      } else {
        bump = std::min(bump, b);
      }
    };

    for (auto const d : state.depStates) {
      if (!d->eligible) continue;

      switch (d->kind) {
        case DepState::Kind::Func:
          add(
            trace_bump_for(
              nullptr,
              d->name,
              folly::get_default(index.m_data->funcToUnit, d->name)
            )
          );
          break;
        case DepState::Kind::Class:
          add(
            trace_bump_for(
              d->name,
              nullptr,
              folly::get_default(index.m_data->classToUnit, d->name)
            )
          );
          break;
        case DepState::Kind::Unit:
          add(trace_bump_for(nullptr, nullptr, d->name));
          break;
      }
    }

    return Trace::Bump{Trace::hhbbc_index, bump};
  } else {
    return Trace::Bump{Trace::hhbbc_index, 0};
  }
}

void AnalysisScheduler::addPredeps(SString name,
                                   const SStringSet& predeps,
                                   AnalysisDeps& deps) const {
  using A = AnalysisDeps;
  auto const& i = *index.m_data;
  for (auto const d : predeps) {
    if (i.classToUnit.contains(d)) {
      FTRACE(5, "AnalysisScheduler: adding class pre-dep {} for {}\n", d, name);
      deps.add(A::Class{ d }, A::Type::Meta);
      deps.add(A::AnyClassConstant{ d }, false);
    }
    if (i.funcToUnit.contains(d)) {
      FTRACE(5, "AnalysisScheduler: adding func pre-dep {} for {}\n", d, name);
      deps.add(A::Func{ d }, A::Type::Meta | A::Type::RetType);
    }
    if (i.typeAliasToUnit.contains(d)) {
      FTRACE(5, "AnalysisScheduler: adding type-alias pre-dep {} for {}\n", d, name);
      deps.add(A::Class{ d }, A::Type::Meta);
    }
    if (i.constantToUnit.contains(d)) {
      FTRACE(5, "AnalysisScheduler: adding unit pre-dep {} for {}\n", d, name);
      deps.add(A::Constant { d });
    }
  }
}

void AnalysisScheduler::registerClass(SString name, AnalysisMode mode) {
  assertx(mode != AnalysisMode::Final);

  // Closures are only scheduled as part of the class or func they're
  // declared in.
  if (is_closure_name(name)) return;

  auto const UNUSED bump = bump_for_class(*index.m_data, name);
  FTRACE(5, "AnalysisScheduler: registering class {}\n", name);

  auto const [cState, emplaced1] = classState.try_emplace(name, name);
  if (!emplaced1) return;

  ++totalWorkItems;

  auto const& i = *index.m_data;
  auto const bundle = i.classToBundle.at(name);

  auto const [tState, emplaced2] = traceState.try_emplace(bundle, bundle);
  if (emplaced2) traceNames.emplace_back(bundle);
  tState->second.depStates.emplace_back(&cState->second.depState);

  classNames.emplace_back(name);
  namesSorted = false;

  if (auto const p = folly::get_ptr(i.unitCInitPredeps,
                                    i.classToUnit.at(name))) {
    addPredeps(name, *p, cState->second.depState.deps);
  }
  for (auto const base : folly::get_default(i.classToCnsBases, name)) {
    auto const unit = i.classToUnit.at(base);
    if (auto const p = folly::get_ptr(i.unitCInitPredeps, unit)) {
      addPredeps(name, *p, cState->second.depState.deps);
    }
  }

  if (mode == AnalysisMode::Full) {
    if (auto const p = folly::get_ptr(i.unitPredeps, i.classToUnit.at(name))) {
      addPredeps(name, *p, cState->second.depState.deps);
    }
  }

  auto const& closures = folly::get_default(i.classToClosures, name);
  for (auto const clo : closures) {
    FTRACE(
      5, "AnalysisScheduler: registering closure {} associated with class {}\n",
      clo, name
    );
    always_assert(classState.try_emplace(clo, clo).second);
  }
}

void AnalysisScheduler::registerFunc(SString name, AnalysisMode mode) {
  assertx(mode != AnalysisMode::Final);

  auto const UNUSED bump = bump_for_func(*index.m_data, name);
  FTRACE(5, "AnalysisScheduler: registering func {}\n", name);

  auto const [fState, emplaced1] = funcState.try_emplace(name, name);
  if (!emplaced1) return;

  ++totalWorkItems;

  funcNames.emplace_back(name);
  namesSorted = false;

  auto const& i = *index.m_data;

  auto const& closures = folly::get_default(i.funcToClosures, name);
  for (auto const clo : closures) {
    FTRACE(
      5, "AnalysisScheduler: registering closure {} associated with func {}\n",
      clo, name
    );
    always_assert(classState.try_emplace(clo, clo).second);
  }

  // If this func is a 86cinit, then register the associated constant
  // as well.
  if (auto const cns = Constant::nameFromFuncName(name)) {
    FTRACE(5, "AnalysisScheduler: registering constant {}\n", cns);
    always_assert(cnsChanged.try_emplace(cns).second);
    // Modifying a global constant func implies the unit will be
    // changed too, so the unit must be registered as well.
    registerUnit(i.funcToUnit.at(name), mode);
  }

  if (auto const p = folly::get_ptr(i.unitCInitPredeps,
                                    i.funcToUnit.at(name))) {
    addPredeps(name, *p, fState->second.depState.deps);
  }
  if (mode == AnalysisMode::Full) {
    if (auto const p = folly::get_ptr(i.unitPredeps, i.funcToUnit.at(name))) {
      addPredeps(name, *p, fState->second.depState.deps);
    }
  }

  auto const bundle = i.funcToBundle.at(name);
  auto const [tState, emplaced2] = traceState.try_emplace(bundle, bundle);
  if (emplaced2) traceNames.emplace_back(bundle);
  tState->second.depStates.emplace_back(&fState->second.depState);
}

void AnalysisScheduler::registerUnit(SString name, AnalysisMode mode) {
  assertx(mode != AnalysisMode::Final);
  auto const UNUSED bump = bump_for_unit(*index.m_data, name);
  FTRACE(5, "AnalysisScheduler: registering unit {}\n", name);

  auto const [uState, emplaced1] = unitState.try_emplace(name, name);
  if (!emplaced1) return;

  if (index.units_with_type_aliases().contains(name)) {
    ++totalWorkItems;
  }

  auto const& i = *index.m_data;
  if (auto const p = folly::get_ptr(i.unitCInitPredeps, name)) {
    addPredeps(name, *p, uState->second.depState.deps);
  }
  if (mode == AnalysisMode::Full) {
    if (auto const p = folly::get_ptr(i.unitPredeps, name)) {
      addPredeps(name, *p, uState->second.depState.deps);
    }
  }

  auto const bundle = i.unitToBundle.at(name);
  auto const [tState, emplaced2] = traceState.try_emplace(bundle, bundle);
  if (emplaced2) traceNames.emplace_back(bundle);
  tState->second.depStates.emplace_back(&uState->second.depState);

  unitNames.emplace_back(name);
  namesSorted = false;
}

void AnalysisScheduler::sortNames() {
  if (namesSorted) return;
  std::sort(begin(classNames), end(classNames), string_data_lt_type{});
  std::sort(begin(funcNames), end(funcNames), string_data_lt_func{});
  std::sort(begin(unitNames), end(unitNames), string_data_lt{});
  std::sort(begin(traceNames), end(traceNames), string_data_lt{});
  namesSorted = true;
}

// Called when an analysis job reports back its changes. This makes
// any dependencies affected by the change eligible to run in the next
// analysis round.
void AnalysisScheduler::recordChanges(const AnalysisOutput& output) {
  const SStringSet bundles{begin(output.bundleNames), end(output.bundleNames)};

  auto const& changed = output.meta.changed;

  // Sanity check that this bucket should actually be modifying the
  // entity.
  auto const valid = [&] (SString name, DepState::Kind kind) {
    switch (kind) {
      case DepState::Func: {
        if (output.meta.removedFuncs.contains(name)) return true;
        auto const b = folly::get_default(index.m_data->funcToBundle, name);
        return b && bundles.contains(b);
      }
      case DepState::Class: {
        if (!is_closure_name(name)) {
          auto const b = folly::get_default(index.m_data->classToBundle, name);
          return b && bundles.contains(b);
        }
        auto const ctx = folly::get_default(index.m_data->closureToClass, name);
        if (ctx) {
          auto const b = folly::get_default(index.m_data->classToBundle, ctx);
          return b && bundles.contains(b);
        }
        auto const f = folly::get_default(index.m_data->closureToFunc, name);
        always_assert(f);
        auto const b = folly::get_default(index.m_data->funcToBundle, f);
        return b && bundles.contains(b);
      }
      case DepState::Unit: {
        auto const b = folly::get_default(index.m_data->unitToBundle, name);
        return b && bundles.contains(b);
      }
    }
  };

  for (auto const [name, type] : changed.classes) {
    auto const UNUSED bump = bump_for_class(*index.m_data, name);
    FTRACE(4, "AnalysisScheduler: class {} changed ({})\n",
           name, show(type));
    auto state = folly::get_ptr(classState, name);
    always_assert_flog(
      state,
      "Trying to mark un-tracked func {} changed",
      name
    );
    always_assert_flog(
      valid(name, DepState::Class),
      "Trying to mark class {} as changed from wrong shard",
      name
    );
    assertx(AnalysisDeps::isValidForChanges(type));
    assertx(state->changed == Type::None);
    state->changed = type;
  }

  for (auto const [name, type] : changed.funcs) {
    auto const UNUSED bump = bump_for_func(*index.m_data, name);
    FTRACE(4, "AnalysisScheduler: func {} changed ({})\n", name, show(type));
    auto state = folly::get_ptr(funcState, name);
    always_assert_flog(
      state,
      "Trying to mark un-tracked func {} changed",
      name
    );
    always_assert_flog(
      valid(name, DepState::Func),
      "Trying to mark func {} as changed from wrong shard",
      name
    );
    assertx(AnalysisDeps::isValidForChanges(type));
    assertx(state->changed == Type::None);
    state->changed = type;
  }

  for (auto const [meth, type] : changed.methods) {
    auto const UNUSED bump = bump_for_class(*index.m_data, meth.cls);
    FTRACE(4, "AnalysisScheduler: method {} changed ({})\n",
           show(meth), show(type));
    auto state = folly::get_ptr(classState, meth.cls);
    always_assert_flog(
      state,
      "Trying to mark method for un-tracked class {} changed",
      meth.cls
    );
    always_assert_flog(
      valid(meth.cls, DepState::Class),
      "Trying to mark method for class {} as changed from wrong shard",
      meth.cls
    );
    assertx(AnalysisDeps::isValidForChanges(type));
    auto& t = state->methodChanges.ensure(meth.idx);
    assertx(t == Type::None);
    t = type;
  }

  for (auto const cns : changed.clsConstants) {
    auto const UNUSED bump = bump_for_class(*index.m_data, cns.cls);
    auto state = folly::get_ptr(classState, cns.cls);
    always_assert_flog(
      state,
      "Trying to mark constant for un-tracked class {} changed",
      cns.cls
    );
    always_assert_flog(
      valid(cns.cls, DepState::Class),
      "Trying to mark constant for class {} as changed from wrong shard",
      cns.cls
    );

    if (state->allCnsFixed) continue;
    if (cns.idx < state->cnsFixed.size() && state->cnsFixed[cns.idx]) continue;
    FTRACE(4, "AnalysisScheduler: class constant {} changed\n", show(cns));
    if (cns.idx >= state->cnsChanges.size()) {
      state->cnsChanges.resize(cns.idx+1);
    }
    assertx(!state->cnsChanges[cns.idx]);
    state->cnsChanges[cns.idx] = true;
  }

  for (auto const cns : changed.fixedClsConstants) {
    auto const UNUSED bump = bump_for_class(*index.m_data, cns.cls);
    auto state = folly::get_ptr(classState, cns.cls);
    always_assert_flog(
      state,
      "Trying to mark constant for un-tracked class {} as fixed",
      cns.cls
    );
    always_assert_flog(
      valid(cns.cls, DepState::Class),
      "Trying to mark constant for class {} as fixed from wrong shard",
      cns.cls
    );

    if (state->allCnsFixed) continue;
    if (cns.idx >= state->cnsFixed.size()) {
      state->cnsFixed.resize(cns.idx+1);
    }
    if (!state->cnsFixed[cns.idx]) {
      FTRACE(4, "AnalysisScheduler: class constant {} now fixed\n", show(cns));
      state->cnsFixed[cns.idx] = true;
    }
  }

  for (auto const cls : changed.allClsConstantsFixed) {
    auto const UNUSED bump = bump_for_class(*index.m_data, cls);
    auto state = folly::get_ptr(classState, cls);
    always_assert_flog(
      state,
      "Trying to mark all constants for un-tracked class {} as fixed",
      cls
    );
    always_assert_flog(
      valid(cls, DepState::Class),
      "Trying to mark all constants for class {} as fixed from wrong shard",
      cls
    );
    if (!state->allCnsFixed) {
      FTRACE(
        4, "AnalysisScheduler: all class constants for {} now fixed\n",
        cls
      );
      state->allCnsFixed = true;
      state->cnsFixed.clear();
    }
  }

  for (auto const name : changed.constants) {
    auto const UNUSED bump = bump_for_constant(*index.m_data, name);
    FTRACE(4, "AnalysisScheduler: constant {} changed\n", name);
    auto state = folly::get_ptr(cnsChanged, name);
    always_assert_flog(
      state,
      "Trying to mark un-tracked constant {} changed",
      name
    );
    auto const initName = Constant::funcNameFromName(name);
    always_assert_flog(
      valid(initName, DepState::Func),
      "Trying to mark constant {} as changed from wrong shard",
      name
    );
    assertx(!state->load(std::memory_order_acquire));
    state->store(true, std::memory_order_release);
  }

  for (auto const& p : changed.properties) {
    auto const UNUSED bump = bump_for_class(*index.m_data, p.cls);
    FTRACE(4, "AnalysisScheduler: property {}::{} changed\n", p.cls, p.prop);
    auto state = folly::get_ptr(classState, p.cls);
    always_assert_flog(
      state,
      "Trying to mark property {} for un-tracked class {} changed",
      p.prop, p.cls
    );
    always_assert_flog(
      valid(p.cls, DepState::Class),
      "Trying to mark property {} for class {} as changed from wrong shard",
      p.prop, p.cls
    );
    state->propertyChanges.emplace(p.prop);
  }

  for (auto const unit : changed.unitsFixed) {
    auto const UNUSED bump = bump_for_unit(*index.m_data, unit);
    auto state = folly::get_ptr(unitState, unit);
    always_assert_flog(
      state,
      "Trying to mark all type-aliases for un-tracked unit {} as fixed",
      unit
    );
    always_assert_flog(
      valid(unit, DepState::Unit),
      "Trying to mark all type-aliases for unit {} as fixed from wrong shard",
      unit
    );
    if (!state->fixed) {
      FTRACE(
        4, "AnalysisScheduler: all type-aliases for unit {} now fixed\n",
        unit
      );
      state->fixed = true;
    }
  }

  for (auto& [cls, names] : changed.clsTypeCnsNames) {
    auto state = folly::get_ptr(classState, cls);
    always_assert_flog(
      state,
      "Trying to record type constant names "
      "for un-tracked class {}",
      cls
    );
    always_assert_flog(
      valid(cls, DepState::Class),
      "Trying to record type constant names "
      "for class {} from wrong shard",
      cls
    );
    state->typeCnsNames = std::move(names);
  }

  for (auto& [unit, names] : changed.unitTypeCnsNames) {
    auto state = folly::get_ptr(unitState, unit);
    always_assert_flog(
      state,
      "Trying to record type constant names "
      "for un-tracked unit {}",
      unit
    );
    always_assert_flog(
      valid(unit, DepState::Unit),
      "Trying to record type constant names "
      "for unit {} from wrong shard",
      unit
    );
    state->typeCnsNames = std::move(names);
  }
}

// Update the dependencies stored in the scheduler to take into
// account the new set of dependencies reported by an analysis job.
void AnalysisScheduler::updateDepState(AnalysisOutput& output) {
  for (auto& [name, deps] : output.meta.classDeps) {
    auto const state = folly::get_ptr(classState, name);
    always_assert_flog(
      state,
      "Trying to set deps for un-tracked class {}",
      name
    );
    if (is_closure_name(name)) {
      assertx(deps.empty());
      assertx(state->depState.deps.empty());
    }
    state->depState.deps = std::move(deps);
  }

  for (auto& [name, deps] : output.meta.funcDeps) {
    auto const state = folly::get_ptr(funcState, name);
    always_assert_flog(
      state,
      "Trying to set deps for un-tracked func {}",
      name
    );
    state->depState.deps = std::move(deps);
  }

  for (auto& [name, deps] : output.meta.unitDeps) {
    auto const state = folly::get_ptr(unitState, name);
    always_assert_flog(
      state,
      "Trying to set deps for un-tracked unit {}",
      name
    );
    state->depState.deps = std::move(deps);
  }

  // Remove deps for any removed functions, to avoid them spuriously
  // being rescheduled again.
  for (auto const name : output.meta.removedFuncs) {
    auto const state = folly::get_ptr(funcState, name);
    always_assert_flog(
      state,
      "Trying to reset deps for un-tracked func {}",
      name
    );
    state->depState.deps = AnalysisDeps{};
  }

  for (auto& [cls, bases] : output.meta.cnsBases) {
    auto const state = folly::get_ptr(classState, cls);
    always_assert_flog(
      state,
      "Trying to update cns bases for untracked class {}",
      cls
    );
    auto old = folly::get_ptr(index.m_data->classToCnsBases, cls);
    if (!old) {
      assertx(bases.empty());
      continue;
    }
    if constexpr (debug) {
      // Class constant base classes should only shrink.
      for (auto const b : bases) always_assert(old->contains(b));
    }
    *old = std::move(bases);
  }
}

// Record the output of an analysis job. This means updating the
// various Refs to their new versions, recording new dependencies, and
// recording what has changed (to schedule the next round).
void AnalysisScheduler::record(AnalysisOutput output) {
  auto const numBundles = output.bundleNames.size();
  assertx(numBundles == output.bundles.size());

  // Update Ref mappings:
  for (size_t i = 0; i < numBundles; ++i) {
    auto const name = output.bundleNames[i];
    index.m_data->bundleRefs.at(name) =
      output.bundles[i].cast<std::unique_ptr<ClassBundle>>();
  }

  recordChanges(output);
  updateDepState(output);

  // If the analysis job optimized away any 86cinit functions, record
  // that here so they can be later removed from our tables.
  if (!output.meta.removedFuncs.empty()) {
    // This is relatively rare, so a lock is fine.
    std::lock_guard<std::mutex> _{*lock};
    funcsToRemove.insert(
      begin(output.meta.removedFuncs),
      end(output.meta.removedFuncs)
    );
  }
}

// Remove metadata for any 86cinit function that an analysis job
// optimized away. This must be done *after* calculating the next
// round of work.
void AnalysisScheduler::removeFuncs() {
  if (funcsToRemove.empty()) return;

  TSStringSet traceNamesToRemove;
  for (auto const name : funcsToRemove) {
    auto const UNUSED bump = bump_for_func(*index.m_data, name);
    FTRACE(4, "AnalysisScheduler: removing function {}\n", name);

    auto fstate = folly::get_ptr(funcState, name);
    always_assert(fstate);

    auto tstate = [&] {
      auto const b = folly::get_default(index.m_data->funcToBundle, name);
      always_assert(b);
      return folly::get_ptr(traceState, b);
    }();
    always_assert(tstate);

    tstate->depStates.erase(
      std::remove_if(
        begin(tstate->depStates), end(tstate->depStates),
        [&] (const DepState* d) { return d == &fstate->depState; }
      ),
      end(tstate->depStates)
    );
    if (tstate->depStates.empty()) {
      auto const tname = tstate->name;
      traceState.erase(tname);
      traceNamesToRemove.emplace(tname);
    }

    always_assert(index.m_data->funcToUnit.erase(name));
    always_assert(index.m_data->funcToBundle.erase(name));
    always_assert(funcState.erase(name));
    always_assert(!index.m_data->funcRefs.contains(name));
    index.m_data->funcToClosures.erase(name);
    if (auto const cns = Constant::nameFromFuncName(name)) {
      always_assert(index.m_data->constantInitFuncs.erase(name));
      always_assert(index.m_data->allFuncs.erase(name));
      always_assert(cnsChanged.erase(cns));
      index.m_data->constantToUnit.at(cns).second = false;
    }
  }

  funcNames.erase(
    std::remove_if(
      begin(funcNames), end(funcNames),
      [&] (SString name) { return funcsToRemove.contains(name); }
    ),
    end(funcNames)
  );

  if (!traceNamesToRemove.empty()) {
    traceNames.erase(
      std::remove_if(
        begin(traceNames), end(traceNames),
        [&] (SString name) { return traceNamesToRemove.contains(name); }
      ),
      end(traceNames)
    );
  }

  funcsToRemove.clear();
}

// Retrieve the BucketPresence appropriate for the class with the given
// name.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForClass(SString cls) const {
  if (is_closure_name(cls)) {
    if (auto const n = folly::get_default(index.m_data->closureToClass, cls)) {
      return bucketsForClass(n);
    }
    if (auto const n = folly::get_default(index.m_data->closureToFunc, cls)) {
      return bucketsForFunc(n);
    }
  }

  if (auto const u = folly::get_ptr(untracked.badClasses, cls)) {
    return BucketPresence{u, std::nullopt};
  }

  auto const b = folly::get_default(index.m_data->classToBundle, cls);
  if (!b) return BucketPresence{};
  auto const t = folly::get_ptr(traceState, b);
  if (!t) {
    return BucketPresence{
      folly::get_ptr(untracked.untrackedBundles, b),
      std::nullopt
    };
  }

  if (auto const p = folly::get_ptr(classState, cls)) {
    return BucketPresence{
      &t->present,
      p->depState.authBucket
    };
  }
  return BucketPresence{
    &t->present,
    std::nullopt
  };
}

// Retrieve the BucketPresence appropriate for the func with the given
// name.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForFunc(SString func) const {
  if (auto const cns = Constant::nameFromFuncName(func)) {
    return bucketsForConstant(cns);
  }

  if (auto const u = folly::get_ptr(untracked.badFuncs, func)) {
    return BucketPresence{u, std::nullopt};
  }

  auto const b = folly::get_default(index.m_data->funcToBundle, func);
  if (!b) return BucketPresence{};
  auto const t = folly::get_ptr(traceState, b);
  if (!t) {
    return BucketPresence{
      folly::get_ptr(untracked.untrackedBundles, b),
      std::nullopt
    };
  }

  if (auto const p = folly::get_ptr(funcState, func)) {
    return BucketPresence{
      &t->present,
      p->depState.authBucket
    };
  }
  return BucketPresence{
    &t->present,
    std::nullopt
  };
}

// Retrieve the BucketPresence appropriate for the unit with the given
// path.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForUnit(SString unit) const {
  auto const b = folly::get_default(index.m_data->unitToBundle, unit);
  if (!b) return BucketPresence{};
  auto const t = folly::get_ptr(traceState, b);
  if (!t) {
    return BucketPresence{
      folly::get_ptr(untracked.untrackedBundles, b),
      std::nullopt
    };
  }

  if (auto const p = folly::get_ptr(unitState, unit)) {
    return BucketPresence{
      &t->present,
      p->depState.authBucket
    };
  }
  return BucketPresence{
    &t->present,
    std::nullopt
  };
}

// Retrive the BucketPresence appropriate for the constant with the given
// name.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForConstant(SString cns) const {
  if (auto const u = folly::get_ptr(untracked.badConstants, cns)) {
    return BucketPresence{u, std::nullopt};
  }
  auto const unit = folly::get_ptr(index.m_data->constantToUnit, cns);
  if (!unit) return BucketPresence{};
  return bucketsForUnit(unit->first);
}

// Retrieve the BucketPresence appropriate for the type-alias with the
// given name.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForTypeAlias(SString typeAlias) const {
  auto const unit =
    folly::get_default(index.m_data->typeAliasToUnit, typeAlias);
  if (!unit) return BucketPresence{};
  return bucketsForUnit(unit);
}

// Retrieve the BucketPresence appropriate for the class or type-alias
// with the given name.
AnalysisScheduler::BucketPresence
AnalysisScheduler::bucketsForClassOrTypeAlias(SString name) const {
  if (auto const b = bucketsForClass(name); b.present)     return b;
  if (auto const b = bucketsForTypeAlias(name); b.present) return b;
  return BucketPresence{};
}

Either<const AnalysisScheduler::ClassState*,
       const AnalysisScheduler::UnitState*>
AnalysisScheduler::stateForClassOrTypeAlias(SString n) const {
  if (auto const s = folly::get_ptr(classState, n)) return s;
  if (auto const unit = folly::get_default(index.m_data->typeAliasToUnit, n)) {
    return folly::get_ptr(unitState, unit);
  }
  return nullptr;
}

AnalysisScheduler::Presence
AnalysisScheduler::presenceOf(const DepState& state,
                              const BucketPresence& o) const {
  if (!state.authBucket)          return Presence::Dep;
  if (state.authBucket == o.auth) return Presence::Full;
  return (o.present && o.present->contains(*state.authBucket))
    ? Presence::Dep
    : Presence::None;
}

AnalysisScheduler::Presence
AnalysisScheduler::presenceOfClass(const DepState& state,
                                   SString cls) const {
  return presenceOf(state, bucketsForClass(cls));
}

AnalysisScheduler::Presence
AnalysisScheduler::presenceOfClassOrTypeAlias(const DepState& state,
                                              SString cls) const {
  return presenceOf(state, bucketsForClassOrTypeAlias(cls));
}

AnalysisScheduler::Presence
AnalysisScheduler::presenceOfFunc(const DepState& state,
                                  SString func) const {
  return presenceOf(state, bucketsForFunc(func));
}

AnalysisScheduler::Presence
AnalysisScheduler::presenceOfConstant(const DepState& state,
                                      SString cns) const {
  return presenceOf(state, bucketsForConstant(cns));
}

// Calculate any classes or functions which should be scheduled to be
// analyzed in the next round.
void AnalysisScheduler::findToSchedule() {
  // Check if the given entity (class or function) needs to run again
  // due to one of its dependencies changing (or if it previously
  // registered a new dependency).
  auto const check = [&] (SString name, DepState& d) {
    // The algorithm for these are all similar: Compare the old
    // dependencies with the new dependencies. If the dependency is
    // new, or if it's not the same as the old, check the
    // BundleSet. If they're in the same job, ignore it (this entity
    // already incorporated the change inside the analysis
    // job). Otherwise schedule this class or func to run.

    for (auto const [cls, newT] : d.deps.classes) {
      if (newT == Type::None) continue;

      auto const schedule = [&, cls=cls, newT=newT] {
        switch (presenceOfClassOrTypeAlias(d, cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, cls);
            if (!state) return false;
            return (bool)(state->changed & newT);
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on class {},"
          " scheduling\n",
          name, cls
        );
        return true;
      }
    }

    for (auto const cls : d.deps.anyClsConstants) {
      auto const schedule = [&] {
        switch (presenceOfClassOrTypeAlias(d, cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, cls);
            return state && state->cnsChanges.any();
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on "
          "any class constant for {}, scheduling\n",
          name, cls
        );
        return true;
      }
    }

    for (auto const cls : d.deps.typeCnsAnyClsConstants) {
      if (presenceOfClassOrTypeAlias(d, cls) == Presence::None) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency (in type-cns) on "
          "any class constant for {}, scheduling\n",
          name, cls
        );
        return true;
      }
    }

    for (auto const cns : d.deps.typeCnsClsConstants) {
      if (presenceOfClassOrTypeAlias(d, cns.cls) == Presence::None) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency (in type-cns) on "
          "class constant {}, scheduling\n",
          name, show(cns)
        );
        return true;
      }
    }

    for (auto const [meth, newT] : d.deps.methods) {
      if (newT == Type::None) continue;

      auto const schedule = [&, meth=meth, newT=newT] {
        switch (presenceOfClass(d, meth.cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, meth.cls);
            if (!state) return false;
            auto const changed =
              state->methodChanges.get_default(meth.idx, Type::None);
            return (bool)(changed & newT);
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on method {},"
          " scheduling\n",
          name, show(meth)
        );
        return true;
      }
    }

    for (auto const [func, newT] : d.deps.funcs) {
      if (newT == Type::None) continue;

      auto const schedule = [&, func=func, newT=newT] {
        switch (presenceOfFunc(d, func)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(funcState, func);
            if (!state) return false;
            return (bool)(state->changed & newT);
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on func {}, "
          "scheduling\n",
          name, func
        );
        return true;
      }
    }

    for (auto const cns : d.deps.clsConstants) {
      auto const schedule = [&] {
        switch (presenceOfClass(d, cns.cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, cns.cls);
            if (!state) return false;
            return
              cns.idx < state->cnsChanges.size() && state->cnsChanges[cns.idx];
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on "
          "class constant {}, scheduling\n",
          name, show(cns)
        );
        return true;
      }
    }

    for (auto const cns : d.deps.constants) {
      auto const schedule = [&] {
        switch (presenceOfConstant(d, cns)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const changed = folly::get_ptr(cnsChanged, cns);
            return changed && changed->load(std::memory_order_acquire);
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on "
          "constant {}, scheduling\n",
          name, cns
        );
        return true;
      }
    }

    for (auto const& prop : d.deps.properties) {
      auto const schedule = [&] {
        switch (presenceOfClass(d, prop.cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, prop.cls);
            if (!state) return false;
            return state->propertyChanges.contains(prop.prop);
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on "
          "property {}::{}, scheduling\n",
          name, prop.cls, prop.prop
        );
        return true;
      }
    }

    for (auto const cls: d.deps.anyProperties) {
      auto const schedule = [&] {
        switch (presenceOfClass(d, cls)) {
          case Presence::None:
            return true;
          case Presence::Dep: {
            auto const state = folly::get_ptr(classState, cls);
            if (!state) return false;
            return !state->propertyChanges.empty();
          }
          case Presence::Full:
            return false;
        }
      }();

      if (schedule) {
        FTRACE(
          4, "AnalysisScheduler: {} new/changed dependency on "
          "any property from {}, scheduling\n",
          name, cls
        );
        return true;
      }
    }

    return false;
  };

  parallel::for_each(
    classNames,
    [&] (SString name) {
      assertx(!is_closure_name(name));
      auto& state = classState.at(name).depState;
      auto const UNUSED bump = bump_for_class(*index.m_data, name);
      FTRACE(5, "Class {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = check(name, state);
      if (state.toSchedule) ++totalWorkItems;
    }
  );
  parallel::for_each(
    funcNames,
    [&] (SString name) {
      auto& state = funcState.at(name).depState;
      auto const UNUSED bump = bump_for_func(*index.m_data, name);
      FTRACE(5, "Func {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = check(name, state);
      if (state.toSchedule) ++totalWorkItems;
    }
  );
  parallel::for_each(
    unitNames,
    [&] (SString name) {
      auto& state = unitState.at(name).depState;
      auto const UNUSED bump = bump_for_unit(*index.m_data, name);
      FTRACE(5, "Unit {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = check(name, state);
      if (state.toSchedule) {
        if (index.units_with_type_aliases().contains(name)) {
          ++totalWorkItems;
        }
      }
    }
  );
}

// Reset any recorded changes from analysis jobs, in preparation for
// another round.
void AnalysisScheduler::resetChanges() {
  auto const resetClass = [&] (SString name) {
    auto& state = classState.at(name);
    std::fill(
      begin(state.methodChanges),
      end(state.methodChanges),
      Type::None
    );
    state.cnsChanges.reset();
    state.propertyChanges.clear();
    state.changed = Type::None;
  };

  parallel::for_each(
    classNames,
    [&] (SString name) {
      resetClass(name);
      auto const& closures =
        folly::get_default(index.m_data->classToClosures, name);
      for (auto const c : closures) resetClass(c);
    }
  );
  parallel::for_each(
    funcNames,
    [&] (SString name) {
      funcState.at(name).changed = Type::None;
      auto const& closures =
        folly::get_default(index.m_data->funcToClosures, name);
      for (auto const c : closures) resetClass(c);
      if (auto const cns = Constant::nameFromFuncName(name)) {
        cnsChanged.at(cns).store(false, std::memory_order_release);
      }
    }
  );
}

// Called when all analysis jobs are finished. "Finalize" the changes
// and determine what should run in the next analysis round.
void AnalysisScheduler::recordingDone() {
  sortNames();
  findToSchedule();
  removeFuncs();
  resetChanges();
}

void AnalysisScheduler::enableAll() {
  sortNames();

  parallel::for_each(
    classNames,
    [&] (SString name) {
      assertx(!is_closure_name(name));
      auto& state = classState.at(name).depState;
      auto const UNUSED bump = bump_for_class(*index.m_data, name);
      FTRACE(5, "Class {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = true;
      ++totalWorkItems;
    }
  );
  parallel::for_each(
    funcNames,
    [&] (SString name) {
      auto& state = funcState.at(name).depState;
      auto const UNUSED bump = bump_for_func(*index.m_data, name);
      FTRACE(5, "Func {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = true;
      ++totalWorkItems;
    }
  );
  parallel::for_each(
    unitNames,
    [&] (SString name) {
      auto& state = unitState.at(name).depState;
      auto const UNUSED bump = bump_for_unit(*index.m_data, name);
      FTRACE(5, "Unit {} dep-state:\n{}", name, show(state.deps));
      state.toSchedule = true;
      if (index.units_with_type_aliases().contains(name)) {
        ++totalWorkItems;
      }
    }
  );

}

template <typename V>
void AnalysisScheduler::visitClassAsDep(SString n,
                                        bool fromClosure,
                                        bool full,
                                        const V& visit) const {
  auto const& i = *index.m_data;

  if (auto const u = folly::get_default(i.typeAliasToUnit, n)) {
    visitUnitAsDep(u, visit);
    return;
  }

  if (is_closure_name(n)) {
    if (auto const ctx = folly::get_default(i.closureToClass, n)) {
      if (!fromClosure) visitClassAsDep(ctx, false, full, visit);
      return;
    } else if (auto const f = folly::get_default(i.closureToFunc, n)) {
      if (!fromClosure) visitFuncAsDep(f, full, visit);
    } else if (full) {
      visit.onBadCls(n);
      return;
    }
  }

  if (auto const u = folly::get_default(i.classToUnit, n)) {
    visit.onCls(n);
    visitUnitAsDep(u, visit);
  } else if (full) {
    visit.onBadCls(n);
  }
}

template <typename V>
void AnalysisScheduler::visitFuncAsDep(SString n,
                                       bool full,
                                       const V& visit) const {
  auto const& i = *index.m_data;
  auto const u = folly::get_default(i.funcToUnit, n);
  if (!u) {
    if (full) visit.onBadFunc(n);
    return;
  }
  visit.onFunc(n);
  visitUnitAsDep(u, visit);
  auto const& closures = folly::get_default(i.funcToClosures, n);
  for (auto const clo : closures) {
    visitClassAsDep(clo, true, full, visit);
  }
}

template <typename V>
void AnalysisScheduler::visitUnitAsDep(SString n, const V& visit) const {
  assertx(index.m_data->unitToBundle.contains(n));
  visit.onUnit(n);
}

template <typename V>
void AnalysisScheduler::visitDeps(const DepState& state,
                                  bool full,
                                  const V& visit) const {
  if (full) {
    switch (state.kind) {
      case DepState::Kind::Func:
        visitFuncAsDep(state.name, true, visit);
        break;
      case DepState::Kind::Class:
        visitClassAsDep(state.name, false, true, visit);
        break;
      case DepState::Kind::Unit:
        visitUnitAsDep(state.name, visit);
        break;
    }
  }

  auto const& i = *index.m_data;
  auto const& d = state.deps;

  if (state.kind == DepState::Class) {
    if (auto const bases = folly::get_ptr(i.classToCnsBases, state.name)) {
      for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
    }
    for (auto const& m : folly::get_default(i.extraMethods, state.name)) {
      visitClassAsDep(m.cls, false, full, visit);
    }
  }

  stateForClassOrTypeAlias(state.name).match(
    [&] (const ClassState* s) {
      if (!s) return;
      for (auto const t : s->typeCnsNames) visitClassAsDep(t, false, full, visit);
    },
    [&] (const UnitState* s) {
      if (!s) return;
      for (auto const t : s->typeCnsNames) visitClassAsDep(t, false, full, visit);
    }
  );

  if (d.empty()) return;
  assertx(IMPLIES(state.kind == DepState::Class, !is_closure_name(state.name)));

  for (auto const [c, type] : d.classes) {
    if (type == Type::None) continue;
    if (!full && !(type & AnalysisDeps::kValidForChanges)) continue;

    stateForClassOrTypeAlias(c).match(
      [&] (const ClassState*) { visitClassAsDep(c, false, full, visit); },
      [&] (const UnitState* s) {
        assertx(s);
        if (full || !s->fixed) visitUnitAsDep(s->depState.name, visit);
      },
      [&] { visitClassAsDep(c, false, full, visit); }
    );
  }

  for (auto const [meth, type] : d.methods) {
    if (type == Type::None) continue;
    if (!full && !(type & AnalysisDeps::kValidForChanges)) continue;
    visitClassAsDep(meth.cls, false, full, visit);
  }
  for (auto const [func, type] : d.funcs) {
    if (type == Type::None) continue;
    if (!full && !(type & AnalysisDeps::kValidForChanges)) continue;
    if (Constant::nameFromFuncName(func) &&
        !index.m_data->funcToUnit.contains(func)) {
      continue;
    }
    visitFuncAsDep(func, full, visit);
  }

  for (auto const cns : d.clsConstants) {
    stateForClassOrTypeAlias(cns.cls).match(
      [&] (const ClassState* s) {
        assertx(s);
        if (full || (!s->allCnsFixed &&
                     (cns.idx >= s->cnsFixed.size() ||
                      !s->cnsFixed[cns.idx]))) {
          visitClassAsDep(cns.cls, false, full, visit);
        }
        if (!full) return;
        if (auto const bases = folly::get_ptr(i.classToCnsBases, cns.cls)) {
          for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
        }
      },
      [&] (const UnitState* s) {
        assertx(s);
        if (full || !s->fixed) visitUnitAsDep(s->depState.name, visit);
      },
      [&] () {
        if (!full) return;
        visitClassAsDep(cns.cls, false, true, visit);
        if (auto const bases = folly::get_ptr(i.classToCnsBases, cns.cls)) {
          for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
        }
      }
    );
  }
  for (auto const cls : d.anyClsConstants) {
    stateForClassOrTypeAlias(cls).match(
      [&] (const ClassState* s) {
        assertx(s);
        if (full || !s->allCnsFixed) visitClassAsDep(cls, false, full, visit);
        if (!full) return;
        if (auto const bases = folly::get_ptr(i.classToCnsBases, cls)) {
          for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
        }
      },
      [&] (const UnitState* s) {
        if (full || !s->fixed) visitUnitAsDep(s->depState.name, visit);
      },
      [&] () {
        if (!full) return;
        visitClassAsDep(cls, false, true, visit);
        if (auto const bases = folly::get_ptr(i.classToCnsBases, cls)) {
          for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
        }
      }
    );
  }

  for (auto const cns : d.typeCnsClsConstants) {
    visitClassAsDep(cns.cls, false, full, visit);
    if (!full) continue;
    if (auto const bases = folly::get_ptr(i.classToCnsBases, cns.cls)) {
      for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
    }
  }
  for (auto const cls : d.typeCnsAnyClsConstants) {
    visitClassAsDep(cls, false, full, visit);
    if (!full) continue;
    if (auto const bases = folly::get_ptr(i.classToCnsBases, cls)) {
      for (auto const b : *bases) visitClassAsDep(b, false, full, visit);
    }
  }

  for (auto const cns : d.constants) {
    if (auto const unit = folly::get_ptr(i.constantToUnit, cns)) {
      if (full) visitUnitAsDep(unit->first, visit);
      if (!unit->second || is_native_unit(unit->first)) {
        continue;
      }
      auto const initName = Constant::funcNameFromName(cns);
      if (initName && index.m_data->funcToUnit.contains(initName)) {
        visitFuncAsDep(initName, full, visit);
      }
    } else if (full) {
      visit.onBadConstant(cns);
    }
  }

  for (auto const& prop : d.properties) {
    visitClassAsDep(prop.cls, false, full, visit);
  }
  for (auto const cls : d.anyProperties) {
    visitClassAsDep(cls, false, full, visit);
  }
}

// First pass: initialize all data in TraceStates.
/*
 * PHASE 1: Initialize TraceStates for this scheduling round.
 *
 * Resets all per-round state in each TraceState, preparing for a fresh
 * scheduling pass. This includes:
 * - Clearing successor lists, dependency lists, traces
 * - Resetting eligibility flags
 * - Clearing bucket presence information
 * - Resetting authoritative bucket assignments for DepStates
 *
 * This must be done before each round since the previous round's scheduling
 * information is no longer relevant.
 */
void AnalysisScheduler::initTraceStates() {
  untracked = Untracked{};

  parallel::gen(
    traceNames.size(),
    [&] (size_t idx) {
      auto const n = traceNames[idx];
      auto& state = traceState.at(n);
      assertx(n->tsame(state.name));
      always_assert(state.succs.empty());
      always_assert(state.deps.empty());
      always_assert(state.trace.empty());
      always_assert(state.toProcess.empty());
      always_assert(state.badClasses.empty());
      always_assert(state.badFuncs.empty());
      always_assert(state.badConstants.empty());
      state.eligible = false;
      state.idx = idx;
      state.present.clear();
      for (auto d : state.depStates) d->authBucket.reset();
      return nullptr;
    }
  );
}

/*
 * PHASE 2: Build successor graph and determine eligibility.
 *
 * For each DepState, determines if it's eligible (might produce different
 * results this round). A DepState is eligible if:
 * - It has toSchedule=true (changed last round), OR
 * - Any of its transitive dependencies (up to maxTraceDepth) are eligible
 *
 * Also builds the successor relationship between TraceStates: if TraceState A
 * contains a DepState that depends on entities in TraceState B, then B is a
 * successor of A.
 *
 * The successor graph is the foundation for building traces in the next phase.
 * Successors represent bundles that should potentially be included in this
 * TraceState's trace to short-circuit dependency chains.
 *
 * After building successors, we filter them to only include eligible TraceStates,
 * since ineligible TraceStates won't benefit from being on the trace (they'll
 * produce the same results regardless).
 */
void AnalysisScheduler::findSuccs(size_t maxTraceDepth) {
  // Check if a DepState is eligible by seeing if it or any of its transitive
  // dependencies (up to maxTraceDepth) have toSchedule=true (changed last round).
  // This determines whether the entity might produce different results this round.
  auto const isEligible = [&] (const DepState& start) {
    struct Visit {
      explicit Visit(const AnalysisScheduler& t): thiz{t} {}

      void onCls(SString n) const {
        auto const s = folly::get_ptr(thiz.classState, n);
        if (!s) return;
        if (!seen.emplace(&s->depState).second) return;
        worklist.emplace_back(&s->depState, depth+1);
      }
      void onFunc(SString n) const {
        auto const s = folly::get_ptr(thiz.funcState, n);
        if (!s) return;
        if (!seen.emplace(&s->depState).second) return;
        worklist.emplace_back(&s->depState, depth+1);
      }
      void onUnit(SString n) const {
        auto const s = folly::get_ptr(thiz.unitState, n);
        if (!s) return;
        if (!seen.emplace(&s->depState).second) return;
        worklist.emplace_back(&s->depState, depth+1);
      }
      void onBadCls(SString) const { always_assert(false); }
      void onBadFunc(SString) const { always_assert(false); }
      void onBadConstant(SString) const { always_assert(false); }

      mutable std::deque<std::pair<const DepState*, size_t>> worklist;
      mutable hphp_fast_set<const DepState*> seen;
      size_t depth = 0;

      const AnalysisScheduler& thiz;
    };

    Visit visit{*this};
    visit.seen.emplace(&start);
    visit.worklist.emplace_back(&start, 0);
    while (!visit.worklist.empty()) {
      auto const [d, depth] = visit.worklist.front();
      visit.worklist.pop_front();
      if (d->toSchedule) return true;
      if (depth < maxTraceDepth) {
        visit.depth = depth;
        visitDeps(*d, false, visit);
      }
    }
    return false;
  };

  struct Visit {
    Visit(AnalysisScheduler& t, TraceState& s): thiz{t}, state{s} {}

    void onCls(SString n) const {
      FTRACE(4, "  {} ({}): class {} ({})\n",
             depState->name, state.name,
             n, thiz.index.m_data->classToBundle.at(n));
      bundle(thiz.index.m_data->classToBundle.at(n));
    }
    void onFunc(SString n) const {
      FTRACE(4, "  {} ({}): func {} ({})\n",
             depState->name, state.name,
             n, thiz.index.m_data->funcToBundle.at(n));
      bundle(thiz.index.m_data->funcToBundle.at(n));
    }
    void onUnit(SString n) const {
      FTRACE(4, "  {} ({}): unit {} ({})\n",
             depState->name, state.name,
             n, thiz.index.m_data->unitToBundle.at(n));
      bundle(thiz.index.m_data->unitToBundle.at(n));
    }
    void onBadCls(SString) const { always_assert(false); }
    void onBadFunc(SString) const { always_assert(false); }
    void onBadConstant(SString) const { always_assert(false); }

    void bundle(SString b) const {
      auto const s = folly::get_ptr(thiz.traceState, b);
      if (!s || s == &state) return;
      state.succs.emplace_back(s);
    }

    AnalysisScheduler& thiz;
    TraceState& state;
    const DepState* depState{nullptr};
  };

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      Visit visit{*this, state};

      for (auto const d : state.depStates) {
        auto const UNUSED bump = bumpFor(*d);
        d->eligible = isEligible(*d);
        FTRACE(4, "{} ({}) is {}eligible\n",
               d->name, state.name,
               d->eligible ? "" : "not ");
        if (!d->eligible) continue;
        FTRACE(4, "Getting successors of {} ({}):\n",
               d->name, state.name);
        visit.depState = d;
        visitDeps(*d, false, visit);
      }

      state.eligible = std::any_of(
        state.depStates.begin(), state.depStates.end(),
        [] (const DepState* d) { return d->eligible; }
      );
      if (!state.eligible) {
        assertx(state.succs.empty());
        return;
      }

      std::sort(
        begin(state.succs), end(state.succs),
        [] (const TraceState* a, const TraceState* b) {
          return a->idx < b->idx;
        }
      );
      state.succs.erase(
        std::unique(
          begin(state.succs),
          end(state.succs)
        ),
        end(state.succs)
      );
    }
  );

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      if (!state.eligible) return;
      state.succs.erase(
        std::remove_if(
          begin(state.succs),
          end(state.succs),
          [&] (const TraceState* t) { return !t->eligible; }
        ),
        end(state.succs)
      );
      state.succs.shrink_to_fit();

      if (!state.succs.empty()) {
        auto const UNUSED bump = bumpFor(state);
        using namespace folly::gen;
        FTRACE(
          5, "{} succs:\n  {}\n",
          name,
          from(state.succs)
            | map([] (const TraceState* t) { return t->name->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
    }
  );
}

/*
 * PHASE 3: Compute full dependency sets for each TraceState.
 *
 * Expands each TraceState with its complete dependency set, which includes:
 * - All bundles containing entities this TraceState depends on
 * - All "bad" entities (missing/undefined classes, funcs, constants)
 *
 * This is a superset of what will eventually be in the trace. The trace
 * (built in the next phase) will include only eligible dependencies up to
 * size/depth limits, but this deps set includes ALL dependencies (both
 * eligible and ineligible).
 *
 * The deps set is used to:
 * 1. Provide the complete dependency information for each TraceState
 * 2. Help makeInputs() determine which bundles to include as pureDepBundles
 *    (ineligible dependencies that won't be analyzed but need to be present)
 */
void AnalysisScheduler::findAllDeps() {
  struct Visit {
    Visit(const AnalysisScheduler& t, const TraceState& s): thiz{t}, state{s} {}

    void onCls(SString n) const {
      if (deps.emplace(thiz.index.m_data->classToBundle.at(n)).second) {
        FTRACE(4, "  {} ({}): class {} ({})\n",
               depState->name, state.name,
               n, thiz.index.m_data->classToBundle.at(n));
      }
    }
    void onFunc(SString n) const {
      if (deps.emplace(thiz.index.m_data->funcToBundle.at(n)).second) {
        FTRACE(4, "  {} ({}): func {} ({})\n",
               depState->name, state.name,
               n, thiz.index.m_data->funcToBundle.at(n));
      }
    }
    void onUnit(SString n) const {
      if (deps.emplace(thiz.index.m_data->unitToBundle.at(n)).second) {
        FTRACE(4, "  {} ({}): unit {} ({})\n",
               depState->name, state.name,
               n, thiz.index.m_data->unitToBundle.at(n));
      }
    }
    void onBadCls(SString n) const {
      if (badClasses.emplace(n).second) {
        FTRACE(4, "  {} ({}): bad-class {}\n", depState->name, state.name, n);
      }
    }
    void onBadFunc(SString n) const {
      if (badFuncs.emplace(n).second) {
        FTRACE(4, "  {} ({}): bad-func {}\n", depState->name, state.name, n);
      }
    }
    void onBadConstant(SString n) const {
      if (badConstants.emplace(n).second) {
        FTRACE(4, "  {} ({}): bad-constant {}\n", depState->name, state.name, n);
      }
    }

    mutable SStringSet deps;
    mutable TSStringSet badClasses;
    mutable FSStringSet badFuncs;
    mutable SStringSet badConstants;

    const AnalysisScheduler& thiz;
    const TraceState& state;
    const DepState* depState{nullptr};
  };

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      if (!state.eligible) return;

      Visit visit{*this, state};

      for (auto const d : state.depStates) {
        if (!d->eligible) continue;
        auto const UNUSED bump = bumpFor(*d);
        FTRACE(4, "Getting deps for {} ({}):\n", d->name, state.name);
        visit.depState = d;
        visitDeps(*d, true, visit);
      }

      if constexpr (debug) {
        always_assert(visit.deps.contains(name));
        for (auto const t : state.succs) {
          always_assert(visit.deps.contains(t->name));
        }
      }

      assertx(state.deps.empty());
      assertx(state.badClasses.empty());
      assertx(state.badFuncs.empty());
      assertx(state.badConstants.empty());

      state.deps.insert(
        end(state.deps),
        begin(visit.deps),
        end(visit.deps)
      );
      state.badClasses.insert(
        end(state.badClasses),
        begin(visit.badClasses),
        end(visit.badClasses)
      );
      state.badFuncs.insert(
        end(state.badFuncs),
        begin(visit.badFuncs),
        end(visit.badFuncs)
      );
      state.badConstants.insert(
        end(state.badConstants),
        begin(visit.badConstants),
        end(visit.badConstants)
      );

      std::sort(
        begin(state.deps),
        end(state.deps),
        string_data_lt{}
      );
      std::sort(
        begin(state.badClasses),
        end(state.badClasses),
        string_data_lt_type{}
      );
      std::sort(
        begin(state.badFuncs),
        end(state.badFuncs),
        string_data_lt_func{}
      );
      std::sort(
        begin(state.badConstants),
        end(state.badConstants),
        string_data_lt{}
      );

      state.deps.shrink_to_fit();
      state.badClasses.shrink_to_fit();
      state.badFuncs.shrink_to_fit();
      state.badConstants.shrink_to_fit();

      auto const UNUSED bump = bumpFor(state);

      using namespace folly::gen;

      if (!state.deps.empty()) {
        FTRACE(
          5, "{} deps:\n  {}\n",
          name,
          from(state.deps)
            | map([] (SString n) { return n->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
      if (!state.badClasses.empty()) {
        FTRACE(
          5, "{} bad-classes:\n  {}\n",
          name,
          from(state.badClasses)
            | map([] (SString n) { return n->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
      if (!state.badFuncs.empty()) {
        FTRACE(
          5, "{} bad-funcs:\n  {}\n",
          name,
          from(state.badFuncs)
            | map([] (SString n) { return n->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
      if (!state.badConstants.empty()) {
        FTRACE(
          5, "{} bad-constants:\n  {}\n",
          name,
          from(state.badConstants)
            | map([] (SString n) { return n->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
    }
  );
}

/*
 * PHASE 4: Build traces for each TraceState.
 *
 * A trace is the set of transitive (eligible) dependencies that should be
 * analyzed together with this TraceState to short-circuit dependency chains.
 *
 * Starting from each TraceState, we perform a breadth-first traversal of the
 * successor graph (built in findSuccs), accumulating:
 *
 * - trace: The set of all bundle names to include (up to maxTraceWeight bytes)
 * - toProcess: The set of TraceStates with eligible work to analyze
 *
 * We follow successors up to maxTraceDepth hops and stop adding to the trace
 * once we exceed maxTraceWeight bytes. This prevents traces from spanning the
 * entire codebase while still capturing enough dependencies to significantly
 * reduce analysis rounds.
 *
 * Example: If A depends on B depends on C depends on D (all eligible), and all
 * fit within the size limits, then:
 * - A's trace = [A, B, C, D] (the full dependency chain)
 * - A's toProcess = [A, B, C, D] (all have eligible work)
 * - When A is analyzed, B, C, D are analyzed speculatively
 * - Information propagates through the entire chain in one round
 *
 * In contrast, D's trace would be just [D] (no dependencies), and B's trace
 * would be [B, C, D].
 *
 * After building traces, we perform a second pass to re-check eligibility:
 * A TraceState is only eligible if it or something in its trace has toSchedule=true.
 * This handles the case where all dependencies became ineligible during trace
 * construction.
 *
 * Finally, we clear the succs and deps lists (no longer needed) and remove any
 * ineligible TraceStates from toProcess lists.
 */
void AnalysisScheduler::makeTraces(size_t maxTraceWeight,
                                   size_t maxTraceDepth) {
  parallel::for_each(
    traceNames,
    [&] (SString n) {
      auto& state = traceState.at(n);
      if (!state.eligible) return;

      auto const UNUSED bump = bumpFor(state);

      using Work = std::pair<const TraceState*, size_t>;
      std::deque<Work> worklist;
      TraceState::CSet seen;

      worklist.emplace_back(&state, 0);
      seen.emplace(&state);

      SStringSet trace;
      TraceState::CSet toProcess;
      size_t weight = 0;

      // BFS traversal of successor graph to build the trace.
      // For each TraceState visited:
      // - Add its deps (bundle names) to the trace if they fit within maxTraceWeight
      // - Add the TraceState itself to toProcess (it has eligible work)
      // - Queue its successors (up to maxTraceDepth) for potential inclusion
      //
      // We skip a TraceState if adding its deps would exceed maxTraceWeight,
      // but still consider its successors (they might have better overlap).
      while (!worklist.empty()) {
        auto const [w, depth] = worklist.front();
        assertx(w->eligible);
        worklist.pop_front();

        size_t additional = 0;
        for (auto const d : w->deps) {
          if (trace.contains(d)) continue;
          auto const& r = index.m_data->bundleRefs.at(d);
          additional += r.id().m_size;
        }
        if (weight > 0 && weight + additional > maxTraceWeight) continue;
        weight += additional;
        trace.insert(begin(w->deps), end(w->deps));
        toProcess.emplace(w);

        if (depth >= maxTraceDepth) continue;
        for (auto const s : w->succs) {
          if (!seen.emplace(s).second) continue;
          worklist.emplace_back(s, depth+1);
        }
      }

      if constexpr (debug) {
        always_assert(toProcess.contains(&state));
        for (auto const s : state.toProcess) {
          always_assert(trace.contains(s->name));
          for (auto const w : s->succs) {
            always_assert(trace.contains(w->name));
          }
        }
      }

      assertx(state.trace.empty());
      assertx(state.toProcess.empty());
      state.trace.insert(
        end(state.trace),
        begin(trace),
        end(trace)
      );
      state.toProcess.insert(
        end(state.toProcess),
        begin(toProcess),
        end(toProcess)
      );
      std::sort(begin(state.trace), end(state.trace), string_data_lt{});
      std::sort(
        begin(state.toProcess), end(state.toProcess),
        [] (const TraceState* t1, const TraceState* t2) {
          return t1->idx < t2->idx;
        }
      );
      state.trace.shrink_to_fit();
      state.toProcess.shrink_to_fit();

      using namespace folly::gen;

      if (!state.trace.empty()) {
        FTRACE(
          5, "{} trace:\n  {}\n",
          state.name,
          from(state.trace)
            | map([] (SString n) { return n->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
      if (!state.toProcess.empty()) {
        FTRACE(
          5, "{} to-process:\n  {}\n",
          state.name,
          from(state.toProcess)
            | map([] (const TraceState* t) { return t->name->toCppString(); })
            | unsplit<std::string>("\n  ")
        );
      }
    }
  );

  std::atomic<bool> changed{false};

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      if (!state.eligible) return;

      state.eligible = [&] {
        std::stack<const TraceState*> worklist;
        TraceState::CSet seen;
        seen.emplace(&state);
        worklist.push(&state);
        while (!worklist.empty()) {
          auto const w = worklist.top();
          worklist.pop();
          auto const toSchedule = std::any_of(
            begin(w->depStates), end(w->depStates),
            [] (const DepState* d) { return d->toSchedule; }
          );
          if (toSchedule) return true;
          for (auto const t : w->succs) {
            if (!seen.emplace(t).second) continue;
            worklist.push(t);
          }
        }
        return false;
      }();

      if (!state.eligible) changed.store(true);
    }
  );

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      state.succs.clear();
      state.deps.clear();
      state.succs.shrink_to_fit();
      state.deps.shrink_to_fit();
      if (!state.eligible) {
        state.badClasses.clear();
        state.badFuncs.clear();
        state.badConstants.clear();
        state.badClasses.shrink_to_fit();
        state.badFuncs.shrink_to_fit();
        state.badConstants.shrink_to_fit();
      }
    }
  );

  if (!changed.load()) return;

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      if (!state.eligible) return;
      auto const UNUSED bump = bumpFor(state);
      std::erase_if(
        state.toProcess,
        [&] (const TraceState* t) { return !t->eligible; }
      );
    }
  );
}

/*
 * PHASE 5: Pack TraceStates into size-bounded buckets.
 *
 * This phase takes the traces built in makeTraces() and packs them into buckets
 * that can be processed in parallel, subject to a maximum bucket size constraint.
 *
 * ALGORITHM:
 * ----------
 * We use a greedy bin-packing algorithm with overlap optimization:
 *
 * 1. Build ElemInfo for each unique bundle:
 *    - Frequency: how many traces contain this bundle
 *    - Weight: size in bytes
 *    - Sort by frequency (descending) to prioritize common bundles
 *
 * 2. Convert each TraceState's trace into a bitset for fast intersection:
 *    - present: bitset of all bundles in the trace
 *    - toProcess: bitset of bundles with eligible work
 *
 * 3. Greedy bucket filling:
 *    a. Pick the largest remaining trace as the "seed" for a new bucket
 *    b. Iteratively find traces that maximize overlap with current bucket:
 *       - Calculate additional weight each trace would add (trace weight - overlap)
 *       - Add traces in order of minimum additional weight
 *       - Stop when bucket exceeds maxBucketWeight and no more beneficial additions
 *    c. Repeat until all traces are assigned to buckets
 *
 * OVERLAP OPTIMIZATION:
 * ---------------------
 * The key insight is that multiple traces can share bundles in the same bucket.
 * If trace A needs bundles [1,2,3] and trace B needs [2,3,4], putting both in
 * the same bucket requires bundles [1,2,3,4] (size 4), not (size 6).
 *
 * This dramatically reduces total work while still ensuring each trace has all
 * its dependencies available in the same bucket.
 *
 * EXAMPLE:
 * --------
 * Traces:
 *   T1: [A, B, C] (100 KB)
 *   T2: [B, C, D] (100 KB)
 *   T3: [E, F] (80 KB)
 * Max bucket size: 150 KB
 *
 * Greedy packing:
 *   Bucket 1: T1 + T2  bundles [A,B,C,D] (130 KB, overlap saved 70 KB)
 *   Bucket 2: T3  bundles [E,F] (80 KB)
 *
 * Without overlap awareness, would need 3 buckets or split traces.
 */
std::vector<AnalysisScheduler::Bucket>
AnalysisScheduler::makeBuckets(size_t maxBucketWeight) {
  struct Bin {
    size_t idx{std::numeric_limits<size_t>::max()};
    uint64_t weight{0};
    SparseBitset<> present;
    SparseBitset<> toProcess;
    TraceState::CSet heads;
  };

  struct TraceInfo {
    TraceState* state{nullptr};
    SparseBitset<> present;
    SparseBitset<> toProcess;
    uint64_t weight{0};
    const Bin* bin{nullptr};
  };

  struct ElemInfo {
    SString name;
    size_t idx{0};
    uint64_t freq{0};
    uint64_t weight{0};
  };

  std::vector<std::unique_ptr<TraceInfo>> traceInfos;
  SStringToOneT<TraceInfo*> nameToTraceInfo;
  traceInfos.reserve(traceNames.size());
  nameToTraceInfo.reserve(traceNames.size());

  std::vector<std::unique_ptr<ElemInfo>> elemInfos;
  SStringToOneT<ElemInfo*> nameToElemInfo;

  for (auto const n : traceNames) {
    auto& state = traceState.at(n);
    if (!state.eligible) continue;
    auto tinfo = std::make_unique<TraceInfo>();
    tinfo->state = &state;

    for (auto const t : state.trace) {
      auto einfo = folly::get_default(nameToElemInfo, t);
      if (!einfo) {
        auto info = std::make_unique<ElemInfo>();
        info->name = t;
        info->idx = elemInfos.size();
        auto const& r = index.m_data->bundleRefs.at(t);
        info->weight = r.id().m_size;
        einfo = info.get();
        nameToElemInfo.emplace(t, einfo);
        elemInfos.emplace_back(std::move(info));
      }
      tinfo->weight += einfo->weight;
      ++einfo->freq;
    }
    nameToTraceInfo.emplace(n, tinfo.get());
    traceInfos.emplace_back(std::move(tinfo));
  }

  std::sort(
    begin(elemInfos), end(elemInfos),
    [&] (auto const& e1, auto const& e2) {
      if (e1->freq < e2->freq) return false;
      if (e1->freq > e2->freq) return true;
      if (e1->weight < e2->weight) return false;
      if (e1->weight > e2->weight) return true;
      return e1->idx < e2->idx;
    }
  );

  for (size_t i = 0, size = elemInfos.size(); i < size; ++i) {
    elemInfos[i]->idx = i;
  }

  parallel::for_each(
    traceInfos,
    [&] (auto& info) {
      for (auto const t : info->state->trace) {
        info->present.add(nameToElemInfo.at(t)->idx);
      }
      for (auto const t : info->state->toProcess) {
        info->toProcess.add(nameToElemInfo.at(t->name)->idx);
      }
      assertx(info->toProcess.isSubsetOf(info->present));

      info->state->trace.clear();
      info->state->toProcess.clear();

      info->state->trace.shrink_to_fit();
      info->state->toProcess.shrink_to_fit();
    }
  );

  std::vector<std::unique_ptr<Bin>> bins;

  std::vector<TraceInfo*> worklist;
  worklist.reserve(traceInfos.size());
  for (auto& i : traceInfos) worklist.emplace_back(i.get());

  parallel::thread_pool threadPool;

  while (!worklist.empty()) {
    auto bin = std::make_unique<Bin>();
    bin->idx = bins.size();

    auto const it = std::max_element(
      begin(worklist),
      end(worklist),
      [&] (const TraceInfo* i1, const TraceInfo* i2) {
        if (i1->weight < i2->weight) return true;
        if (i1->weight > i2->weight) return false;
        auto const s1 = i1->present.size();
        auto const s2 = i2->present.size();
        if (s1 < s2) return true;
        if (s1 > s2) return false;
        return i1->state->idx > i2->state->idx;
      }
    );
    assertx(it != end(worklist));
    auto start = *it;
    assertx(!start->bin);
    start->bin = bin.get();
    bin->present = start->present;
    bin->toProcess = start->toProcess;
    bin->weight = start->weight;
    bin->heads.emplace(start->state);

    while (true) {
      auto weights = parallel::map(
        worklist,
        [&] (TraceInfo* info) -> std::pair<TraceInfo*, uint64_t> {
          if (info->bin) return std::make_pair(nullptr, 0);
          auto r = info->weight;
          info->present.forEachIsect(
            bin->present,
            [&] (uint64_t e) {
              assertx(e < elemInfos.size());
              assertx(r >= elemInfos[e]->weight);
              r -= elemInfos[e]->weight;
            }
          );
          if (r >= info->weight) return std::make_pair(nullptr, 0);
          return std::make_pair(info, r);
        },
        &threadPool
      );

      std::sort(
        begin(weights), end(weights),
        [&] (auto const& p1, auto const& p2) {
          if (!p1.first) return false;
          if (!p2.first) return true;
          if (p1.second < p2.second) return true;
          if (p1.second > p2.second) return false;
          if (p1.first->weight < p2.first->weight) return false;
          if (p1.first->weight > p2.first->weight) return true;
          return p1.first->state->idx < p2.first->state->idx;
        }
      );

      auto added = false;
      auto hitLimit = false;

      for (auto const& [info, r] : weights) {
        if (!info) break;
        assertx(r < info->weight);
        if (r > 0 && bin->weight >= maxBucketWeight) {
          hitLimit = true;
          break;
        }
        assertx(!info->bin);
        info->bin = bin.get();
        assertx(info->toProcess.isSubsetOf(info->present));

        bin->present |= info->present;
        bin->toProcess |= info->toProcess;
        bin->weight += r;
        bin->heads.emplace(info->state);
        added |= (r > 0);
      }

      bin->weight = 0;
      bin->present.forEach(
        [&] (uint64_t e) {
          assertx(e < elemInfos.size());
          bin->weight += elemInfos[e]->weight;
        }
      );

      if (!hitLimit || !added) break;
    }

    std::erase_if(
      worklist,
      [&] (const TraceInfo* b) { return (bool)b->bin; }
    );
    bins.emplace_back(std::move(bin));
  }

  std::sort(
    begin(bins), end(bins),
    [&] (auto const& b1, auto const& b2) {
      if (b1->weight < b2->weight) return true;
      if (b1->weight > b2->weight) return false;
      return b1->idx < b2->idx;
    }
  );

  assertx(!bins.empty());
  for (size_t i = 0, size = bins.size(); i < size; ++i) {
    auto& bin = *bins[i];
    assertx(bin.weight > 0);
    if (bin.weight >= maxBucketWeight) break;

    for (size_t j = bins.size(); (j-1) > i; --j) {
      auto& other = *bins[j-1];
      assertx(&other != &bin);
      assertx(other.weight > 0);
      if (other.weight >= maxBucketWeight) continue;

      auto r = bin.weight;
      bin.present.forEachIsect(
        other.present,
        [&] (uint64_t e) {
          assertx(e < elemInfos.size());
          assertx(r >= elemInfos[e]->weight);
          r -= elemInfos[e]->weight;
        }
      );
      if (other.weight + r > maxBucketWeight) continue;

      bin.weight = 0;
      other.present |= bin.present;
      other.toProcess |= bin.toProcess;
      other.weight += r;
      other.heads.insert(begin(bin.heads), end(bin.heads));
      break;
    }
  }

  // Remove any empty bins and sort by weight (largest first)
  std::erase_if(bins, [] (auto const& bin) { return !bin->weight; });
  std::sort(
    begin(bins), end(bins),
    [] (auto const& b1, auto const& b2) {
      if (b1->weight < b2->weight) return false;
      if (b1->weight > b2->weight) return true;
      return b1->idx < b2->idx;
    }
  );

  // Convert Bins to Buckets, categorizing bundles into the three types:
  // - reportBundles: "head" TraceStates (entities that need results reported)
  // - noReportBundles: TraceStates in toProcess but not heads (analyzed speculatively)
  // - pureDepBundles: TraceStates in present but not toProcess (not analyzed, info only)
  auto const buckets = parallel::map(
    bins,
    [&] (auto const& bin) {
      assertx(bin->weight > 0);
      Bucket bucket;

      TSStringSet badClasses;
      FSStringSet badFuncs;
      SStringSet badConstants;

      assertx(bin->toProcess.isSubsetOf(bin->present));

      bin->present.forEach(
        [&] (uint64_t e) {
          assertx(e < elemInfos.size());
          if (bin->toProcess.contains(e)) return;
          bucket.pureDepBundles.emplace_back(elemInfos[e]->name);
        }
      );
      bin->toProcess.forEach(
        [&] (uint64_t e) {
          assertx(e < elemInfos.size());
          assertx(bin->present.contains(e));
          auto const name = elemInfos[e]->name;
          auto const& info = *nameToTraceInfo.at(name);
          auto const& state = *info.state;
          assertx(state.eligible);

          badClasses.insert(begin(state.badClasses), end(state.badClasses));
          badFuncs.insert(begin(state.badFuncs), end(state.badFuncs));
          badConstants.insert(begin(state.badConstants), end(state.badConstants));

          if (bin->heads.contains(&state)) return;
          bucket.noReportBundles.emplace_back(name);
          assertx(info.toProcess.isSubsetOf(info.present));
        }
      );
      for (auto const h : bin->heads) {
        assertx(h->eligible);
        if constexpr (debug) {
          auto const info = nameToElemInfo.at(h->name);
          always_assert(info->idx < elemInfos.size());
          always_assert(bin->present.contains(info->idx));
          always_assert(bin->toProcess.contains(info->idx));
        }
        bucket.reportBundles.emplace_back(h->name);
      }

      bucket.badClasses.insert(
        end(bucket.badClasses), begin(badClasses), end(badClasses)
      );
      bucket.badFuncs.insert(
        end(bucket.badFuncs), begin(badFuncs), end(badFuncs)
      );
      bucket.badConstants.insert(
        end(bucket.badConstants), begin(badConstants), end(badConstants)
      );

      std::sort(
        begin(bucket.reportBundles),
        end(bucket.reportBundles),
        string_data_lt{}
      );
      std::sort(
        begin(bucket.noReportBundles),
        end(bucket.noReportBundles),
        string_data_lt{}
      );
      std::sort(
        begin(bucket.pureDepBundles),
        end(bucket.pureDepBundles),
        string_data_lt{}
      );
      std::sort(
        begin(bucket.badClasses),
        end(bucket.badClasses),
        string_data_lt_type{}
      );
      std::sort(
        begin(bucket.badFuncs),
        end(bucket.badFuncs),
        string_data_lt_func{}
      );
      std::sort(
        begin(bucket.badConstants),
        end(bucket.badConstants),
        string_data_lt{}
      );

      bucket.reportBundles.shrink_to_fit();
      bucket.noReportBundles.shrink_to_fit();
      bucket.pureDepBundles.shrink_to_fit();
      bucket.badClasses.shrink_to_fit();
      bucket.badFuncs.shrink_to_fit();
      bucket.badConstants.shrink_to_fit();
      return bucket;
    }
  );

  parallel::for_each(
    traceNames,
    [&] (SString name) {
      auto& state = traceState.at(name);
      if (!state.eligible) return;
      state.badClasses.clear();
      state.badFuncs.clear();
      state.badConstants.clear();
      state.badClasses.shrink_to_fit();
      state.badFuncs.shrink_to_fit();
      state.badConstants.shrink_to_fit();
    }
  );

  return buckets;
}

/*
 * Helper for makeInputs(): Determine which specific DepStates need to be
 * processed in this bucket.
 *
 * Not every entity in a bundle needs to be analyzed - we only process entities
 * that are:
 * 1. In a reportBundle (primary work items), OR
 * 2. In a noReportBundle AND are transitive dependencies of (1)
 *
 * This function starts from reportBundle DepStates and follows their dependency
 * chains (within the bundles present in this bucket) to find all DepStates that
 * need processing.
 *
 * Returns: Set of DepStates to actively analyze in this bucket
 */
AnalysisScheduler::DepState::CSet
AnalysisScheduler::findToProcess(const Bucket& bucket) const {
  struct Visit {
    explicit Visit(const AnalysisScheduler& t) : thiz{t} {}

    void onCls(SString n) const {
      auto const state = folly::get_ptr(thiz.classState, n);
      if (!state) return;
      enqueue(state->depState, thiz.index.m_data->classToBundle.at(n));
    }
    void onFunc(SString n) const {
      auto const state = folly::get_ptr(thiz.funcState, n);
      if (!state) return;
      enqueue(state->depState, thiz.index.m_data->funcToBundle.at(n));
    }
    void onUnit(SString n) const {
      auto const state = folly::get_ptr(thiz.unitState, n);
      if (!state) return;
      enqueue(state->depState, thiz.index.m_data->unitToBundle.at(n));
    }
    void onBadCls(SString) const { always_assert(false); }
    void onBadFunc(SString) const { always_assert(false); }
    void onBadConstant(SString) const { always_assert(false); }

    void enqueue(const DepState& state, SString bundle) const {
      if (!state.eligible || !relevant.contains(bundle)) return;
      if (!seen.emplace(&state).second) return;
      worklist.push(&state);
    }

    const AnalysisScheduler& thiz;
    mutable DepState::CSet seen;
    mutable std::stack<const DepState*> worklist;
    mutable SStringSet relevant;
  };
  Visit visit{*this};

  for (auto const b : bucket.reportBundles) {
    auto const state = folly::get_ptr(traceState, b);
    if (!state) continue;
    for (auto const d : state->depStates) {
      if (!d->eligible) continue;
      always_assert(visit.seen.emplace(d).second);
      visit.worklist.push(d);
    }
    visit.relevant.emplace(b);
  }
  visit.relevant.insert(
    begin(bucket.noReportBundles),
    end(bucket.noReportBundles)
  );
  visit.relevant.insert(
    begin(bucket.pureDepBundles),
    end(bucket.pureDepBundles)
  );

  while (!visit.worklist.empty()) {
    auto const d = visit.worklist.top();
    visit.worklist.pop();
    visitDeps(*d, false, visit);
  }

  return visit.seen;
}

/*
 * Helper for makeInputs(): Determine which bundles from the bucket are actually
 * needed for the specific DepStates being processed.
 *
 * A bucket may contain many bundles (from trace expansion), but not all are
 * necessarily dependencies of the work items being processed. This function
 * identifies the subset of bundles that are actually required.
 *
 * Starting from the toProcess DepStates (determined by findToProcess()), we
 * follow their dependency chains to find which bundles they actually reference.
 * Only these bundles need to be loaded for the analysis job.
 *
 * Returns: Set of bundle names that are dependencies of toProcess items
 */
SStringSet
AnalysisScheduler::findRelevantBundles(const DepState::CSet& toProcess,
                                       const Bucket& bucket) const {
  struct Visit {
    explicit Visit(const AnalysisScheduler& t) : thiz{t} {}

    void onCls(SString n) const {
      relevant.emplace(thiz.index.m_data->classToBundle.at(n));
    }
    void onFunc(SString n) const {
      relevant.emplace(thiz.index.m_data->funcToBundle.at(n));
    }
    void onUnit(SString n) const {
      relevant.emplace(thiz.index.m_data->unitToBundle.at(n));
    }
    void onBadCls(SString) const {}
    void onBadFunc(SString) const {}
    void onBadConstant(SString) const {}

    const AnalysisScheduler& thiz;
    mutable SStringSet relevant;
  };
  Visit visit{*this};

  for (auto const d : toProcess) visitDeps(*d, true, visit);
  return visit.relevant;
}

/*
 * PHASE 6: Transform Buckets into AnalysisInputs.
 *
 * Takes the buckets produced by makeBuckets() and converts them into
 * AnalysisInput objects that can be dispatched as jobs.
 *
 * KEY OPERATIONS:
 * ===============
 *
 * 1. Refine bundle categorization:
 *    - Uses findToProcess() to determine exactly which DepStates need analysis
 *    - Uses findRelevantBundles() to find which bundles are actually needed
 *    - Recategorizes bundles based on actual usage (a bundle might be in the
 *      bucket but not actually needed by any work items)
 *
 * 2. Add mandatory dependencies:
 *    - Special built-in functions (e.g., array builtins)
 *    - Core classes (Awaitable, Traversable)
 *    - Classes/units required by processed entities
 *
 * 3. Build metadata for the job:
 *    - List of classes, funcs, units to process
 *    - Dependency information for each entity
 *    - "Bad" (missing) entities
 *    - Interface slot mapping
 *
 * 4. Assign authoritative buckets:
 *    - Each DepState with toSchedule=true gets assigned to exactly one
 *      bucket as its "authoritative" bucket (where results will be reported)
 *    - Other buckets may analyze it speculatively but won't report results
 *
 * BUNDLE RECATEGORIZATION:
 * ========================
 * A bundle's role can be refined from what makeBuckets() determined:
 *
 * - reportBundle  stays reportBundle (contains authoritative work)
 * - noReportBundle  might become pureDepBundle if none of its entities
 *   are in the toProcess set for this bucket
 * - pureDepBundle  stays pureDepBundle (unchanged)
 *
 * This ensures each job only analyzes what's necessary, minimizing redundant work.
 */
std::vector<AnalysisInput>
AnalysisScheduler::makeInputs(const std::vector<Bucket>& buckets,
                              Mode mode) {
  auto const& i = *index.m_data;

  auto const addMandatoryBundle = [&] (AnalysisInput& input,
                                       SString from,
                                       auto const& map,
                                       SStringSet& added) {
    auto const b = folly::get_default(map, from);
    always_assert(b);
    if (!added.emplace(b).second) return;
    input.noReportBundles.emplace_back(
      i.bundleRefs.at(b).template cast<AnalysisIndexBundle>()
    );
    input.meta.bundleNames.emplace_back(b);
  };

  auto const addMandatoryClass = [&] (AnalysisInput& input,
                                      SString cls,
                                      SStringSet& added) {
    assertx(IMPLIES(is_closure_name(cls), i.closureToFunc.contains(cls)));
    assertx(!i.typeAliasToUnit.contains(cls));
    auto const u = folly::get_default(i.classToUnit, cls);
    always_assert(u);
    addMandatoryBundle(input, u, i.unitToBundle, added);
    addMandatoryBundle(input, cls, i.classToBundle, added);
  };

  auto const addMandatoryFunc = [&] (AnalysisInput& input,
                                     SString func,
                                     SStringSet& added) {
    auto const u = folly::get_default(i.funcToUnit, func);
    always_assert(u);
    addMandatoryBundle(input, u, i.unitToBundle, added);
    addMandatoryBundle(input, func, i.funcToBundle, added);
    auto const& closures = folly::get_default(i.funcToClosures, func);
    for (auto const clo : closures) addMandatoryClass(input, clo, added);
  };

  auto const addMandatory = [&] (AnalysisInput& input, SStringSet& added) {
    for (auto const f : special_builtins()) addMandatoryFunc(input, f, added);
    addMandatoryClass(input, s_Awaitable.get(), added);
    addMandatoryClass(input, s_Traversable.get(), added);
  };

  auto const addMandatoryUnit = [&] (AnalysisInput& input,
                                     SString unit,
                                     SStringSet& added) {
    addMandatoryBundle(input, unit, i.unitToBundle, added);
  };

  return parallel::gen(
    buckets.size(),
    [&] (size_t idx) {
      auto const& bucket = buckets[idx];

      AnalysisInput input;
      input.meta.bucketIdx = idx;

      auto const toProcess = findToProcess(bucket);
      auto const relevant = findRelevantBundles(toProcess, bucket);

      SStringSet added;

      auto const add = [&] (SString b, auto& bundles, bool auth) {
        bundles.emplace_back(i.bundleRefs.at(b).cast<AnalysisIndexBundle>());
        input.meta.bundleNames.emplace_back(b);
        always_assert(added.emplace(b).second);

        auto const state = folly::get_ptr(traceState, b);
        if (!state) return;

        assertx(!state->depStates.empty());
        for (auto const d : state->depStates) {
          if (!d->eligible || !toProcess.contains(d)) continue;

          if (auth) {
            assertx(!d->authBucket);
            d->authBucket = idx;
          }

          if (d->toSchedule) {
            switch (d->kind) {
              case DepState::Func:
                input.meta.startFunc.emplace(d->name);
                break;
              case DepState::Class:
                input.meta.startCls.emplace(d->name);
                break;
              case DepState::Unit:
                input.meta.startUnit.emplace(d->name);
                break;
            }
          } else {
            switch (d->kind) {
              case DepState::Func:
                input.meta.funcDeps.emplace(d->name, d->deps);
                break;
              case DepState::Class:
                input.meta.classDeps.emplace(d->name, d->deps);
                break;
              case DepState::Unit:
                input.meta.unitDeps.emplace(d->name, d->deps);
                break;
            }
          }
        }
      };

      for (auto const b : bucket.reportBundles) {
        assertx(relevant.contains(b));
        add(b, input.reportBundles, true);
      }
      for (auto const b : bucket.noReportBundles) {
        if (!relevant.contains(b)) continue;
        add(b, input.noReportBundles, false);
      }
      for (auto const b : bucket.pureDepBundles) {
        if (!relevant.contains(b)) continue;
        input.noReportBundles.emplace_back(
          i.bundleRefs.at(b).cast<AnalysisIndexBundle>()
        );
        input.meta.bundleNames.emplace_back(b);
        always_assert(added.emplace(b).second);
      }

      addMandatory(input, added);

      input.meta.badClasses.insert(
        begin(bucket.badClasses),
        end(bucket.badClasses)
      );
      input.meta.badFuncs.insert(
        begin(bucket.badFuncs),
        end(bucket.badFuncs)
      );
      input.meta.badConstants.insert(
        begin(bucket.badConstants),
        end(bucket.badConstants)
      );

      assertx(!input.reportBundles.empty());
      assertx(!input.meta.bundleNames.empty());
      input.m_key = input.meta.bundleNames.front();

      if (mode != Mode::Final) return input;

      for (auto const b : bucket.reportBundles) {
        auto const state = folly::get_ptr(traceState, b);
        if (!state) continue;
        for (auto const d : state->depStates) {
          switch (d->kind) {
            case DepState::Class: {
              if (is_closure_name(d->name)) break;
              auto const slot = folly::get_default(
                index.m_data->ifaceSlotMap,
                d->name,
                kInvalidSlot
              );
              if (slot == kInvalidSlot) break;
              input.meta.ifaceSlotMap.emplace(d->name, slot);
              break;
            }
            case DepState::Unit: {
              auto const orig =
                folly::get_ptr(index.m_data->unitToOriginalUnits, d->name);
              if (!orig) break;
              std::vector<SString> sorted{begin(*orig), end(*orig)};
              std::sort(begin(sorted), end(sorted), string_data_lt{});
              for (auto const o : sorted) addMandatoryUnit(input, o, added);
              break;
            }
            case DepState::Func:
              break;
          }
        }
      }
      return input;
    }
  );
}

// Sixth pass: Calculate BucketSets for each TraceSet. This will
// determine how different items can utilize information from another.
void AnalysisScheduler::calcBucketSets(std::vector<AnalysisInput>& inputs) {
  struct Shards {
    struct Shard {
      SStringToOneT<BucketSet> presentBundles;
      TSStringToOneT<BucketSet> badClasses;
      FSStringToOneT<BucketSet> badFuncs;
      SStringToOneT<BucketSet> badConstants;

      std::mutex lock;
    };

    struct Lock {
      explicit Lock(Shard& s): s{&s} { s.lock.lock(); }
      Lock(const Lock&) = delete;
      Lock(Lock&&) = default;
      Lock& operator=(const Lock&) = delete;
      Lock& operator=(Lock&&) = default;
      ~Lock() { s->lock.unlock(); }
      Shard* s;
    };

    Lock lock(SString n) {
      auto& shard = shards[n->hashStatic() % shards.size()];
      return Lock{shard};
    }

    const Shard& get(SString n) const {
      return shards[n->hashStatic() % shards.size()];
    }

    std::array<Shard, 4096> shards;
  };
  Shards shards;

  parallel::gen(
    inputs.size(),
    [&] (size_t bucketIdx) {
      auto& input = inputs[bucketIdx];
      for (auto const n : input.meta.bundleNames) {
        auto const l = shards.lock(n);
        l.s->presentBundles[n].emplace(bucketIdx);
      }

      for (auto const c : input.meta.badClasses) {
        auto const l = shards.lock(c);
        l.s->badClasses[c].emplace(bucketIdx);
      }
      for (auto const f : input.meta.badFuncs) {
        auto const l = shards.lock(f);
        l.s->badFuncs[f].emplace(bucketIdx);
      }
      for (auto const c : input.meta.badConstants) {
        auto const l = shards.lock(c);
        l.s->badConstants[c].emplace(bucketIdx);
      }

      return nullptr;
    }
  );

  parallel::for_each(
    traceNames,
    [&] (SString n) {
      auto& state = traceState.at(n);
      assertx(state.present.empty());
      auto const p = folly::get_ptr(shards.get(n).presentBundles, n);
      if (!p) return;
      state.present = std::move(*p);
    }
  );

  for (auto const& shard : shards.shards) {
    for (auto& [n, b] : shard.presentBundles) {
      if (traceState.contains(n)) continue;
      untracked.untrackedBundles.emplace(n, std::move(b));
    }

    for (auto& [n, b] : shard.badFuncs) {
      untracked.badFuncs.emplace(n, std::move(b));
    }
    for (auto& [n, b] : shard.badClasses) {
      untracked.badClasses.emplace(n, std::move(b));
    }
    for (auto& [n, b] : shard.badConstants) {
      untracked.badConstants.emplace(n, std::move(b));
    }
  }
}

// Seventh pass: Strictly debug. Check various invariants.
void AnalysisScheduler::checkInputInvariants(
  const std::vector<AnalysisInput>& inputs
) {
  if (!debug) return;

  SStringToOneT<size_t> allReportBundles;

  for (auto const& i : inputs) {
    always_assert_flog(
      !i.reportBundles.empty(),
      "Bucket {} has empty report set!",
      i.meta.bucketIdx
    );

    always_assert_flog(
      i.meta.bucketIdx < inputs.size(),
      "Bucket {} has an invalid index! ({})",
      i.meta.bucketIdx, inputs.size()
    );

    always_assert_flog(
      i.meta.bundleNames.size() ==
      i.reportBundles.size() + i.noReportBundles.size(),
      "Bucket {} has mismatched bundle names!",
      i.meta.bucketIdx
    );

    SStringSet reportBundles;
    SStringSet noReportBundles;
    for (size_t idx = 0, size = i.reportBundles.size(); idx < size; ++idx) {
      auto const name = i.meta.bundleNames[idx];
      always_assert_flog(
        reportBundles.emplace(name).second,
        "Bundle {} appears twice in report set for bucket {}!",
        name, i.meta.bucketIdx
      );
    }
    for (size_t idx = 0, size = i.noReportBundles.size(); idx < size; ++idx) {
      auto const name = i.meta.bundleNames[i.reportBundles.size() + idx];
      always_assert_flog(
        noReportBundles.emplace(name).second,
        "Bundle {} appears twice in no-report set for bucket {}!",
        name, i.meta.bucketIdx
      );
    }

    for (auto const n : reportBundles) {
      always_assert_flog(
        !noReportBundles.contains(n),
        "Bundle {} appears in both sets for bucket {}!",
        n, i.meta.bucketIdx
      );

      auto const [it, e] = allReportBundles.try_emplace(n, i.meta.bucketIdx);
      always_assert_flog(
        e,
        "Bundle {} is in report set for both bucket {} and {}!",
        n, it->second, i.meta.bucketIdx
      );
    }

    SStringSet startOrDepBundles;

    auto const checkStart = [&] (const char* type,
                                 auto const& set,
                                 auto const& toBundle,
                                 auto const& deps,
                                 auto const& toState) {
      for (auto const n : set) {
        auto const bundle = folly::get_default(toBundle, n);
        always_assert_flog(
          bundle,
          "{} {} is on start-set for bucket {}, but has no bundle!",
          type, n, i.meta.bucketIdx
        );

        always_assert_flog(
          reportBundles.contains(bundle) || noReportBundles.contains(bundle),
          "{} {} is on start-set for bucket {}, "
          "but bundle {} is not on input!",
          type, n, i.meta.bucketIdx, bundle
        );
        startOrDepBundles.emplace(bundle);

        always_assert_flog(
          !deps.contains(n),
          "{} {} is on both start-set and dep-set for bucket {}!",
          type, n, i.meta.bucketIdx
        );

        auto const state = folly::get_ptr(toState, n);
        always_assert_flog(
          state,
          "{} {} is on start-set for bucket {}, but has no state!",
          type, n, i.meta.bucketIdx
        );
        always_assert_flog(
          state->depState.eligible && state->depState.toSchedule,
          "{} {} is on start-set for bucket {}, but is not scheduled!",
          type, n, i.meta.bucketIdx
        );

        auto const tstate = folly::get_ptr(traceState, bundle);
        always_assert_flog(
          tstate,
          "{} {} is on start-set for bucket {}, "
          "but bundle {} has no state!",
          type, n, i.meta.bucketIdx, bundle
        );
        always_assert_flog(
          tstate->eligible,
          "{} {} is on start-set for bucket {}, "
          "but bundle {} is not eligible!",
          type, n, i.meta.bucketIdx, bundle
        );
        always_assert_flog(
          std::find(
            begin(tstate->depStates), end(tstate->depStates),
            &state->depState
          ) != end(tstate->depStates),
          "{} {} is on start-set for bucket {}, "
          "but bundle {} doesn't have correct dep-state!",
          type, n, i.meta.bucketIdx, bundle
        );
      }
    };
    checkStart(
      "Class", i.meta.startCls, index.m_data->classToBundle,
      i.meta.classDeps, classState
    );
    checkStart(
      "Func", i.meta.startFunc, index.m_data->funcToBundle,
      i.meta.funcDeps, funcState
    );
    checkStart(
      "Unit", i.meta.startUnit, index.m_data->unitToBundle,
      i.meta.unitDeps, unitState
    );

    auto const checkDep = [&] (const char* type,
                               auto const& set,
                               auto const& toBundle,
                               auto const& starts,
                               auto const& toState) {
      for (auto const& [n, _] : set) {
        auto const bundle = folly::get_default(toBundle, n);
        always_assert_flog(
          bundle,
          "{} {} is on dep-set for bucket {}, but has no bundle!",
          type, n, i.meta.bucketIdx
        );

        always_assert_flog(
          reportBundles.contains(bundle) || noReportBundles.contains(bundle),
          "{} {} is on dep-set for bucket {}, "
          "but bundle {} is not on input!",
          type, n, i.meta.bucketIdx, bundle
        );
        startOrDepBundles.emplace(bundle);

        always_assert_flog(
          !starts.contains(n),
          "{} {} is on both start-set and dep-set for bucket {}!",
          type, n, i.meta.bucketIdx
        );

        auto const state = folly::get_ptr(toState, n);
        always_assert_flog(
          state,
          "{} {} is on dep-set for bucket {}, but has no state!",
          type, n, i.meta.bucketIdx
        );
        always_assert_flog(
          state->depState.eligible && !state->depState.toSchedule,
          "{} {} is on dep-set for bucket {}, but is scheduled!",
          type, n, i.meta.bucketIdx
        );

        auto const tstate = folly::get_ptr(traceState, bundle);
        always_assert_flog(
          tstate,
          "{} {} is on dep-set for bucket {}, "
          "but bundle {} has no state!",
          type, n, i.meta.bucketIdx, bundle
        );
        always_assert_flog(
          tstate->eligible,
          "{} {} is on dep-set for bucket {}, "
          "but bundle {} is not eligible!",
          type, n, i.meta.bucketIdx, bundle
        );
        always_assert_flog(
          std::find(
            begin(tstate->depStates), end(tstate->depStates),
            &state->depState
          ) != end(tstate->depStates),
          "{} {} is on dep-set for bucket {}, "
          "but bundle {} doesn't have correct dep-state!",
          type, n, i.meta.bucketIdx, bundle
        );
      }
    };
    checkDep(
      "Class", i.meta.classDeps, index.m_data->classToBundle,
      i.meta.startCls, classState
    );
    checkDep(
      "Func", i.meta.funcDeps, index.m_data->funcToBundle,
      i.meta.startFunc, funcState
    );
    checkDep(
      "Unit", i.meta.unitDeps, index.m_data->unitToBundle,
      i.meta.startUnit, unitState
    );

    auto const checkBad = [&] (const char* type,
                               auto const& set,
                               auto const& toBundle,
                               auto const& start,
                               auto const& deps,
                               auto const& toState) {
      for (auto const n : set) {
        always_assert_flog(
          !toBundle.contains(n),
          "{} {} is marked as bad on bucket {}, yet exists!",
          type, n, i.meta.bucketIdx
        );
        always_assert_flog(
          !start.contains(n) && !deps.contains(n),
          "{} {} is marked as bad on bucket {}, yet is on start/dep set!",
          type, n, i.meta.bucketIdx
        );
        always_assert_flog(
          !toState.contains(n),
          "{} {} is marked as bad on bucket {}, yet has state!",
          type, n, i.meta.bucketIdx
        );
      }
    };
    checkBad(
      "Class", i.meta.badClasses, index.m_data->classToBundle,
      i.meta.startCls, i.meta.classDeps, classState
    );
    checkBad(
      "Func", i.meta.badFuncs, index.m_data->funcToBundle,
      i.meta.startFunc, i.meta.funcDeps, funcState
    );

    for (auto const n : i.meta.badConstants) {
      always_assert_flog(
        !index.m_data->constantToUnit.contains(n),
        "Constant {} is marked as bad on bucket {}, yet exists!",
        n, i.meta.bucketIdx
      );
    }

    for (auto const bundle : reportBundles) {
      always_assert_flog(
        startOrDepBundles.contains(bundle),
        "Bundle {} on bucket {} is on report-list, "
        "but nothing is assigned to be processed!",
        bundle, i.meta.bucketIdx
      );
    }
  }
}

// Group the work that needs to run into buckets of the given size.
/*
 * Main scheduling entry point.
 *
 * Orchestrates all phases of the scheduling algorithm to transform changed
 * entities and their dependencies into optimally-packed work buckets.
 *
 * INPUTS:
 * - mode: Analysis mode (e.g., Constants, Final)
 * - maxTraceWeight: Maximum total bytes for a trace (limits speculative work)
 * - maxTraceDepth: Maximum hops to follow in dependency chains
 * - maxBucketWeight: Maximum total bytes for a bucket (parallelism granularity)
 *
 * OUTPUTS:
 * - Vector of AnalysisInput objects, one per bucket, ready to dispatch as jobs
 *
 * ALGORITHM PHASES (see individual methods for details):
 * 1. initTraceStates()   - Reset per-round state
 * 2. findSuccs()         - Build successor graph, determine eligibility
 * 3. findAllDeps()       - Compute full dependency sets
 * 4. makeTraces()        - Build traces to short-circuit dependency chains
 * 5. makeBuckets()       - Pack traces into size-bounded buckets
 * 6. makeInputs()        - Transform buckets into AnalysisInput jobs
 * 7. calcBucketSets()    - Finalize bucket metadata
 * 8. checkInputInvariants() - Validate the resulting inputs
 *
 * After scheduling, totalWorkItems is reset and the round counter increments.
 */
std::vector<AnalysisInput>
AnalysisScheduler::schedule(Mode mode,
                            size_t maxTraceWeight,
                            size_t maxTraceDepth,
                            size_t maxBucketWeight) {
  FTRACE(2, "AnalysisScheduler: scheduling {} items into buckets "
         "with max size {}, using traces with max weight {} and max depth {}\n",
         workItems(), format_bytes(maxBucketWeight),
         format_bytes(maxTraceWeight), maxTraceDepth);

  sortNames();

  initTraceStates();
  findSuccs(maxTraceDepth);
  findAllDeps();
  makeTraces(maxTraceWeight, maxTraceDepth);
  auto buckets = makeBuckets(maxBucketWeight);
  auto inputs = makeInputs(buckets, mode);
  calcBucketSets(inputs);
  checkInputInvariants(inputs);

  totalWorkItems.store(0);
  FTRACE(2, "AnalysisScheduler: scheduled {} buckets\n", inputs.size());
  ++round;
  return inputs;
}

//////////////////////////////////////////////////////////////////////

namespace {

//////////////////////////////////////////////////////////////////////

// If we optimized a top-level constant's value to a scalar, we no
// longer need the associated 86cinit function. This fixes up the
// metadata to remove it.
FSStringSet strip_unneeded_constant_inits(AnalysisIndex::IndexData& index) {
  FSStringSet stripped;

  for (auto const [n, f] : index.funcs) {
    if (!index.toReport.contains(f)) continue;
    auto const cnsName = Constant::nameFromFuncName(n);
    if (!cnsName) continue;
    auto const cns = folly::get_ptr(index.constants, cnsName);
    if (!cns) continue;
    if (type(cns->first->val) == KindOfUninit) continue;
    auto const p = folly::get_ptr(index.funcToClosures, f);
    always_assert(!p || p->empty());
    stripped.emplace(n);
  }
  if (stripped.empty()) return stripped;

  for (auto const name : stripped) {
    index.deps->getChanges().remove(*index.funcs.at(name));
    index.funcs.erase(name);
    index.finfos.erase(name);
  }

  folly::erase_if(
    index.toReport,
    [&] (FuncClsUnit fc) {
      auto const f = fc.func();
      return f && stripped.contains(f->name);
    }
  );

  for (auto& [_, unit] : index.units) {
    unit->funcs.erase(
      std::remove_if(
        begin(unit->funcs), end(unit->funcs),
        [&, unit=unit] (SString f) {
          if (!stripped.contains(f)) return false;
          assertx(
            index.reportBundleNames.contains(index.unitToBundle.at(unit->filename))
          );
          return true;
        }
      ),
      end(unit->funcs)
    );
  }

  for (auto& b : index.reportBundles) {
    b->funcs.erase(
      std::remove_if(
        begin(b->funcs), end(b->funcs),
        [&] (const std::unique_ptr<php::Func>& f) {
          return stripped.contains(f->name);
        }
      ),
      end(b->funcs)
    );
    b->funcInfos.erase(
      std::remove_if(
        begin(b->funcInfos), end(b->funcInfos),
        [&] (const std::unique_ptr<FuncInfo2>& f) {
          return stripped.contains(f->name);
        }
      ),
      end(b->funcInfos)
    );
    b->funcBytecode.erase(
      std::remove_if(
        begin(b->funcBytecode), end(b->funcBytecode),
        [&] (const std::unique_ptr<php::FuncBytecode>& bc) {
          return stripped.contains(bc->name);
        }
      ),
      end(b->funcBytecode)
    );
  }

  return stripped;
}

// Record the new set of classes which this class has inherited
// constants from. This set can change (always shrink) due to
// optimizations rewriting constants.
TSStringSet record_cns_bases(const php::Class& cls,
                             const AnalysisIndex::IndexData& index) {
  TSStringSet out;
  if (!cls.cinfo) return out;
  for (auto const& [_, idx] : cls.cinfo->clsConstants) {
    if (!cls.name->tsame(idx.idx.cls)) out.emplace(idx.idx.cls);
    if (auto const cnsCls = folly::get_default(index.classes, idx.idx.cls)) {
      assertx(idx.idx.idx < cnsCls->constants.size());
      auto const& cns = cnsCls->constants[idx.idx.idx];
      if (!cls.name->tsame(cns.cls)) out.emplace(cns.cls);
    }
  }
  return out;
}

// Mark all "fixed" class constants. A class constant is fixed if it's
// value can't change going forward. This transforms any dependency on
// that constant from a transitive one (that gets put on the trace) to
// a strict dependency (which only goes on the dep list).
void mark_fixed_class_constants(const php::Class& cls,
                                AnalysisIndex::IndexData& index) {
  auto& changes = index.deps->getChanges();

  auto all = true;
  for (size_t i = 0, size = cls.constants.size(); i < size; ++i) {
    auto const& cns = cls.constants[i];
    auto const fixed = [&] {
      if (!cns.val) return true;
      if (cns.kind == ConstModifierFlags::Kind::Type) {
        // A type-constant is fixed if it's been resolved.
        return cns.resolvedTypeStructure && cns.contextInsensitive;
      } else if (cns.kind == ConstModifierFlags::Kind::Value) {
        // A normal constant is fixed if it's a scalar.
        return type(*cns.val) != KindOfUninit;
      } else {
        // Anything else never changes.
        return true;
      }
    }();
    if (fixed) {
      changes.fixed(ConstIndex { cls.name, (unsigned int)i });
    } else {
      all = false;
    }
  }
  if (all) changes.fixed(cls);

  // While we're at it, record all the names present in this class'
  // type-constants. This will be used in forming dependencies.
  for (auto const& [_, idx] :
         index.index.lookup_flattened_class_type_constants(cls)) {
    auto const cinfo = folly::get_default(index.cinfos, idx.cls);
    if (!cinfo) continue;
    assertx(idx.idx < cinfo->cls->constants.size());
    auto const& cns = cinfo->cls->constants[idx.idx];
    if (cns.kind != ConstModifierFlags::Kind::Type) continue;
    if (!cns.val.has_value()) continue;
    auto const ts = [&] () -> SArray {
      if (cns.resolvedTypeStructure &&
          (cns.contextInsensitive || cls.name->tsame(cns.cls))) {
        return cns.resolvedTypeStructure;
      }
      assertx(tvIsDict(*cns.val));
      return val(*cns.val).parr;
    }();
    auto const name = type_structure_name(ts);
    if (!name) continue;
    changes.typeCnsName(cls, AnalysisChangeSet::Class { name });
  }
}

// Mark whether this unit is "fixed". An unit is fixed if all the
// type-aliases in it are resolved.
void mark_fixed_unit(const php::Unit& unit,
                     AnalysisChangeSet& changes) {
  auto all = true;
  for (auto const& ta : unit.typeAliases) {
    if (!ta->resolvedTypeStructure) all = false;
    auto const ts = [&] {
      if (ta->resolvedTypeStructure) return ta->resolvedTypeStructure;
      return ta->typeStructure;
    }();
    auto const name = type_structure_name(ts);
    if (!name) continue;
    changes.typeCnsName(unit, AnalysisChangeSet::Class { name });
  }
  if (all) changes.fixed(unit);
}

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

AnalysisIndex::AnalysisIndex(
  AnalysisWorklist& worklist,
  V<AnalysisIndexBundle> reportBundles,
  V<AnalysisIndexBundle> noReportBundles,
  AnalysisInput::Meta meta,
  Mode mode
) : m_data{std::make_unique<IndexData>(*this, worklist, mode)}
{
  m_data->bucketIdx = meta.bucketIdx;

  m_data->badClasses = std::move(meta.badClasses);
  m_data->badFuncs = std::move(meta.badFuncs);
  m_data->badConstants = std::move(meta.badConstants);

  auto const assignFInfoIdx = [&] (php::Func& func, FuncInfo2& finfo) {
    always_assert(func.idx == std::numeric_limits<uint32_t>::max());
    always_assert(!finfo.func);
    finfo.func = &func;
    func.idx = m_data->finfosByIdx.size();
    m_data->finfosByIdx.emplace_back(&finfo);
  };

  auto const onCInfo = [&] (ClassInfo2* cinfo) {
    auto cls = folly::get_default(m_data->classes, cinfo->name);
    always_assert(cls);
    always_assert(!cls->cinfo);
    always_assert(!cinfo->cls);
    cls->cinfo = cinfo;
    cinfo->cls = cls;

    // Serialized property info won't have the TC pointer set, so do
    // it here.
    for (auto const& prop : cls->properties) {
      if (!(prop.attrs & AttrPrivate)) continue;
      auto& map = (prop.attrs & AttrStatic)
        ? cinfo->inferred.privateStatics
        : cinfo->inferred.privateProps;
      auto elem = folly::get_ptr(map, prop.name);
      if (!elem) continue;
      assertx(!elem->tc);
      elem->tc = &prop.typeConstraints;
    }

    auto const numMethods = cls->methods.size();
    always_assert(cinfo->funcInfos.size() == numMethods);
    for (size_t i = 0; i < numMethods; ++i) {
      assignFInfoIdx(*cls->methods[i], *cinfo->funcInfos[i]);
    }
    always_assert(m_data->cinfos.emplace(cinfo->name, cinfo).second);
  };

  auto const onBundle = [&] (ClassBundle& bundle, SString bundleName) {
    for (auto& func : bundle.funcs) {
      always_assert(m_data->funcs.emplace(func->name, func.get()).second);
      always_assert(
        m_data->funcToBundle.emplace(func->name, bundleName).second
      );
    }

    for (auto& cls : bundle.classes) {
      if (is_closure(*cls)) {
        assertx(!cls->closureContextCls);
        assertx(cls->closureDeclFunc);
        auto const f = folly::get_default(m_data->funcs, cls->closureDeclFunc);
        always_assert(f);
        m_data->funcToClosures[f].emplace(cls.get());
      }

      for (auto& clo : cls->closures) {
        always_assert(
          m_data->classes.emplace(clo->name, clo.get()).second
        );
        always_assert(
          m_data->classToBundle.emplace(clo->name, bundleName).second
        );
      }

      always_assert(m_data->classes.emplace(cls->name, cls.get()).second);
      always_assert(
        m_data->classToBundle.emplace(cls->name, bundleName).second
      );
    }

    for (auto& cinfo : bundle.classInfos) {
      for (auto& clo : cinfo->closures) onCInfo(clo.get());
      onCInfo(cinfo.get());
    }
    for (auto& finfo : bundle.funcInfos) {
      auto func = folly::get_default(m_data->funcs, finfo->name);
      always_assert(func);
      assignFInfoIdx(*func, *finfo);
      always_assert(m_data->finfos.emplace(func->name, finfo.get()).second);
    }
    for (auto& minfo : bundle.methInfos) {
      auto cls = folly::get_default(m_data->classes, minfo->cls);
      always_assert(cls);
      always_assert(!cls->cinfo);

      auto const numMethods = cls->methods.size();
      always_assert(minfo->finfos.size() == numMethods);
      for (size_t i = 0; i < numMethods; ++i) {
        assignFInfoIdx(*cls->methods[i], *minfo->finfos[i]);
      }
      auto const numClosures = cls->closures.size();
      always_assert(minfo->closureInvokes.size() == numClosures);
      for (size_t i = 0; i < numClosures; ++i) {
        auto& clo = cls->closures[i];
        auto& finfo = minfo->closureInvokes[i];
        assertx(clo->methods.size() == 1);
        assignFInfoIdx(*clo->methods[0], *finfo);
      }
      always_assert(m_data->minfos.emplace(minfo->cls, minfo.get()).second);
    }

    for (auto& bc : bundle.classBytecode) {
      auto cls = folly::get_default(m_data->classes, bc->cls);
      always_assert(cls);

      size_t idx = 0;
      for (auto& meth : cls->methods) {
        assertx(idx < bc->methodBCs.size());
        auto& methBC = bc->methodBCs[idx++];
        always_assert(methBC.name == meth->name);
        meth->rawBlocks = std::move(methBC.bc);
      }
      for (auto& clo : cls->closures) {
        assertx(idx < bc->methodBCs.size());
        auto& methBC = bc->methodBCs[idx++];
        assertx(clo->methods.size() == 1);
        always_assert(methBC.name == clo->methods[0]->name);
        clo->methods[0]->rawBlocks = std::move(methBC.bc);
      }
      assertx(idx == bc->methodBCs.size());
    }
    for (auto& bc : bundle.funcBytecode) {
      auto func = folly::get_default(m_data->funcs, bc->name);
      always_assert(func);
      func->rawBlocks = std::move(bc->bc);
    }

    for (auto& unit : bundle.units) {
      auto const isNative = is_native_unit(*unit);
      for (auto& cns : unit->constants) {
        always_assert(
          m_data->constants.try_emplace(cns->name, cns.get(), unit.get()).second
        );
        assertx(!m_data->badConstants.contains(cns->name));
        if (isNative && type(cns->val) == KindOfUninit) {
          m_data->dynamicConstants.emplace(cns->name);
        }
      }
      for (auto& typeAlias : unit->typeAliases) {
        always_assert(
          m_data->typeAliases.try_emplace(
            typeAlias->name,
            typeAlias.get(),
            unit.get()
          ).second
        );
        assertx(!m_data->badClasses.contains(typeAlias->name));
      }
      always_assert(m_data->units.emplace(unit->filename, unit.get()).second);
      always_assert(
        m_data->unitToBundle.emplace(unit->filename, bundleName).second
      );
    }
  };

  always_assert(
    meta.bundleNames.size() == reportBundles.size() + noReportBundles.size()
  );
  size_t nameIdx = 0;
  for (auto& wrapper : reportBundles) {
    auto const name = meta.bundleNames[nameIdx++];
    onBundle(*wrapper.ptr, name);
    m_data->reportBundles.emplace_back(wrapper.ptr.release());
    m_data->reportBundleNames.emplace(name);
  }
  for (auto& wrapper : noReportBundles) {
    auto const name = meta.bundleNames[nameIdx++];
    onBundle(*wrapper.ptr, name);
    m_data->noReportBundles.emplace_back(wrapper.ptr.release());
  }

  for (auto const& [n, cls] : m_data->classes) {
    if (!cls->cinfo) continue;
    always_assert(cls == cls->cinfo->cls);
  }

  for (auto const& [_, cinfo]: m_data->cinfos) {
    always_assert(cinfo->cls);
    always_assert(cinfo == cinfo->cls->cinfo);
    if (debug &&
        (meta.startCls.contains(cinfo->name) ||
         meta.classDeps.contains(cinfo->name))) {
      for (auto const aux : cinfo->auxClassGraphs.withChildren) {
        always_assert(aux.hasCompleteChildren() || aux.isConservative());
      }
    }
  }

  for (size_t i = 0, size = m_data->finfosByIdx.size(); i < size; ++i) {
    auto finfo = m_data->finfosByIdx[i];
    always_assert(finfo->func);
    always_assert(finfo->func->idx == i);
    if (debug && finfo->auxClassGraphs &&
        (meta.startFunc.contains(finfo->name) ||
         meta.funcDeps.contains(finfo->name))) {
      for (auto const aux : finfo->auxClassGraphs->withChildren) {
        always_assert(aux.hasCompleteChildren() || aux.isConservative());
      }
    }
  }

  assertx(IMPLIES(mode != Mode::Final, meta.ifaceSlotMap.empty()));
  m_data->ifaceSlotMap = std::move(meta.ifaceSlotMap);

  ClassGraph::setAnalysisIndex(*m_data);
  remember_to_report(meta);
  initialize_worklist(meta);
}

AnalysisIndex::~AnalysisIndex() {
  ClassGraph::clearAnalysisIndex();
}

// Initialize the worklist with the items we know we must
// process. Also add dependency information for the items which *may*
// run.
void AnalysisIndex::initialize_worklist(const AnalysisInput::Meta& meta) {
  if (m_data->mode == Mode::Final) {
    for (auto const& b : m_data->reportBundles) {
      for (auto const& f : b->funcs)   m_data->worklist.schedule(f.get());
      for (auto const& c : b->classes) {
        if (is_closure(*c)) continue;
        m_data->worklist.schedule(c.get());
      }
      for (auto const& u : b->units)   m_data->worklist.schedule(u.get());
    }
    m_data->worklist.sort();
    return;
  }

  auto const add = [&] (FuncClsUnit src,
                        const AnalysisDeps& deps,
                        bool isUnit = false) {
    if (!isUnit) {
      for (auto const [name, t] : deps.funcs) {
        m_data->deps->preadd(src, DepTracker::Func { name }, t);
      }
      for (auto const [meth, t] : deps.methods) {
        m_data->deps->preadd(src, meth, t);
      }
    }
    for (auto const cns : deps.clsConstants) {
      m_data->deps->preadd(src, cns);
    }
    for (auto const cls : deps.anyClsConstants) {
      m_data->deps->preadd(src, DepTracker::AnyClassConstant { cls });
    }
    if (!isUnit) {
      for (auto const cns : deps.constants) {
        m_data->deps->preadd(src, DepTracker::Constant { cns });
      }
    }
  };

  auto firstBump = true;
  auto const addBump = [&] (FuncClsUnit fc) {
    auto const b = fc.traceBump();
    if (firstBump) {
      m_data->traceBump = b;
      firstBump = false;
    } else {
      m_data->traceBump = std::min(m_data->traceBump, b);
    }
    return Trace::Bump{Trace::hhbbc_index, b};
  };

  for (auto const n : meta.startCls) {
    auto const c = folly::get_default(m_data->classes, n);
    always_assert(c);
    auto const UNUSED bump = addBump(c);
    FTRACE(4, "Class {} will be unconditionally processed{}\n",
           n, m_data->toReport.contains(c) ? "" : " as a dep");
    assertx(!is_closure(*c));
    m_data->worklist.schedule(c);
  }

  for (auto const& [n, d] : meta.classDeps) {
    auto const c = folly::get_default(m_data->classes, n);
    always_assert(c);
    auto const UNUSED bump = addBump(c);
    FTRACE(4, "Class {} will be conditionally processed{}\n",
           n, m_data->toReport.contains(c) ? "" : " as a dep");
    assertx(!is_closure(*c));
    add(c, d);
  }

  for (auto const n : meta.startFunc) {
    auto const f = folly::get_default(m_data->funcs, n);
    always_assert(f);
    auto const UNUSED bump = addBump(f);
    FTRACE(4, "Func {} will be unconditionally processed{}\n",
           n, m_data->toReport.contains(f) ? "" : " as a dep");
    m_data->worklist.schedule(f);
  }

  for (auto const& [n, d] : meta.funcDeps) {
    auto const f = folly::get_default(m_data->funcs, n);
    always_assert(f);
    auto const UNUSED bump = addBump(f);
    FTRACE(4, "Func {} will be conditionally processed{}\n",
           n, m_data->toReport.contains(f) ? "" : " as a dep");
    add(f, d);
  }

  for (auto const n : meta.startUnit) {
    auto const u = folly::get_default(m_data->units, n);
    always_assert(u);
    auto const UNUSED bump = addBump(u);
    FTRACE(4, "Unit {} will be unconditionally processed{}\n",
           n, m_data->toReport.contains(u) ? "" : " as a dep");
    m_data->worklist.schedule(u);
  }

  for (auto const& [n, d] : meta.unitDeps) {
    auto const u = folly::get_default(m_data->units, n);
    always_assert(u);
    auto const UNUSED bump = addBump(u);
    FTRACE(4, "Unit {} will be conditionally processed{}\n",
           n, m_data->toReport.contains(u) ? "" : " as a dep");
    add(u, d, true);
  }

  // The worklist is not in deterministic order, so sort it.
  m_data->worklist.sort();
}

void AnalysisIndex::remember_to_report(const AnalysisInput::Meta& meta) {
  if (m_data->mode == Mode::Final) return;

  for (auto const& b : m_data->reportBundles) {
    for (auto const& c : b->classes) {
      if (!meta.startCls.contains(c->name) && !meta.classDeps.contains(c->name)) {
        continue;
      }
      m_data->toReport.emplace(c.get());
    }
    for (auto const& f : b->funcs) {
      if (!meta.startFunc.contains(f->name) && !meta.funcDeps.contains(f->name)) {
        continue;
      }
      m_data->toReport.emplace(f.get());
    }
    for (auto const& u : b->units) {
      if (!meta.startUnit.contains(u->filename) &&
          !meta.unitDeps.contains(u->filename)) {
        continue;
      }
      m_data->toReport.emplace(u.get());
    }
  }
}

void AnalysisIndex::start() { ClassGraph::init(); }
void AnalysisIndex::stop()  { ClassGraph::destroy(); }

void AnalysisIndex::push_context(const Context& ctx) {
  m_data->contexts.emplace_back(ctx);
}

void AnalysisIndex::pop_context() {
  assertx(!m_data->contexts.empty());
  m_data->contexts.pop_back();
}

bool AnalysisIndex::set_in_type_cns(bool b) {
  auto const was = m_data->inTypeCns;
  m_data->inTypeCns = b;
  return was;
}

void AnalysisIndex::freeze() {
  {
    Trace::Bump bump{Trace::hhbbc_index, m_data->traceBump};
    FTRACE(2, "Freezing index...\n");
  }
  assertx(!m_data->frozen);
  assertx(m_data->mode != Mode::Final);
  m_data->frozen = true;

  // We're going to be recording dependencies, so we must force actual
  // unserialization again.
  m_data->unserializeCache.clear();

  // We're going to do a final set of analysis to record dependencies,
  // so we must re-add the original set of items to the worklist.
  assertx(!m_data->worklist.next());
  for (auto const fc : m_data->toReport) {
    Trace::Bump bump{Trace::hhbbc_index, fc.traceBump()};
    m_data->deps->reset(fc);
    m_data->worklist.schedule(fc);
  }

  // toReport is unordered so the worklist can be in indeterminate
  // order. Sort it to make it deterministic.
  m_data->worklist.sort();
}

bool AnalysisIndex::frozen() const { return m_data->frozen; }

AnalysisMode AnalysisIndex::mode() const { return m_data->mode; }

std::vector<php::Unit*> AnalysisIndex::units_to_emit() const {
  assertx(m_data->mode == Mode::Final);

  hphp_fast_set<php::Unit*> units;
  for (auto const& b : m_data->reportBundles) {
    for (auto& u : b->units) {
      if (is_native_unit(*u)) continue;
      units.emplace(u.get());
    }
  }

  for (auto const& b : m_data->reportBundles) {
    for (auto const& c : b->classes) {
      auto u = folly::get_default(m_data->units, c->unit);
      always_assert(u && units.contains(u));
    }
    for (auto const& f : b->funcs) {
      auto u = folly::get_default(m_data->units, f->unit);
      always_assert(u && units.contains(u));
    }
  }

  std::vector<php::Unit*> out{begin(units), end(units)};
  std::sort(
    begin(out), end(out),
    [] (const php::Unit* u1, const php::Unit* u2) {
      return string_data_lt{}(u1->filename, u2->filename);
    }
  );
  return out;
}

const php::Unit& AnalysisIndex::lookup_func_unit(const php::Func& f) const {
  auto const it = m_data->units.find(f.unit);
  always_assert_flog(
    it != end(m_data->units),
    "Attempting to access missing unit {} for func {}",
    f.unit, func_fullname(f)
  );
  return *it->second;
}

const php::Unit& AnalysisIndex::lookup_func_original_unit(const php::Func& f) const {
  auto const unit = f.originalUnit ? f.originalUnit : f.unit;
  auto const it = m_data->units.find(unit);
  always_assert_flog(
    it != end(m_data->units),
    "Attempting to access missing original unit {} for func {}",
    unit, func_fullname(f)
  );
  return *it->second;
}

const php::Unit& AnalysisIndex::lookup_class_unit(const php::Class& c) const {
  auto const it = m_data->units.find(c.unit);
  always_assert_flog(
    it != end(m_data->units),
    "Attempting to access missing unit {} for class {}",
    c.unit, c.name
  );
  return *it->second;
}

const php::Unit& AnalysisIndex::lookup_unit(SString n) const {
  auto const it = m_data->units.find(n);
  always_assert_flog(
    it != end(m_data->units),
    "Attempting to access missing unit {}",
    n
  );
  return *it->second;
}

const php::Class*
AnalysisIndex::lookup_const_class(const php::Const& cns) const {
  m_data->deps->add(AnalysisDeps::Class { cns.cls });
  return folly::get_default(m_data->classes, cns.cls);
}

const php::Class* AnalysisIndex::lookup_class(SString name) const {
  return folly::get_default(m_data->classes, name);
}

void AnalysisIndex::for_each_unit_func(
  const php::Unit& unit,
  std::function<void(const php::Func&)> f
) const {
  for (auto const func : unit.funcs) {
    f(*m_data->funcs.at(func));
  }
}

void AnalysisIndex::for_each_unit_func_mutable(
  php::Unit& unit,
  std::function<void(php::Func&)> f
) {
  for (auto const func : unit.funcs) {
    f(*m_data->funcs.at(func));
  }
}

void AnalysisIndex::for_each_unit_class(
  const php::Unit& unit,
  std::function<void(const php::Class&)> f
) const {
  for (auto const cls : unit.classes) {
    f(*m_data->classes.at(cls));
  }
}

void AnalysisIndex::for_each_unit_class_mutable(
  php::Unit& unit,
  std::function<void(php::Class&)> f
) {
  for (auto const cls : unit.classes) {
    f(*m_data->classes.at(cls));
  }
}

const php::Class&
AnalysisIndex::lookup_closure_context(const php::Class& cls) const {
  if (!cls.closureContextCls) return cls;
  auto const p = folly::get_default(m_data->classes, cls.closureContextCls);
  always_assert_flog(
    p,
    "Closure {} is missing its context class {}",
    cls.name, cls.closureContextCls
  );
  return *p;
}

CompactVector<const php::Func*>
AnalysisIndex::lookup_extra_methods(const php::Class& cls) const {
  CompactVector<const php::Func*> out;
  if (cls.attrs & AttrNoExpandTrait) return out;
  if (!cls.cinfo) return out;

  out.reserve(cls.cinfo->extraMethods.size());
  for (auto const meth : cls.cinfo->extraMethods) {
    auto const f = func_from_meth_ref(*m_data, meth);
    always_assert_flog(
      f,
      "Class {} is missing one of its extra methods {}",
      cls.name,
      show(meth)
    );
    out.emplace_back(f);
  }

  std::sort(
    begin(out), end(out),
    [] (const php::Func* f1, const php::Func* f2) {
      assertx(f1->cls);
      assertx(f2->cls);
      if (f1->cls != f2->cls) {
        return string_data_lt_type{}(f1->cls->name, f2->cls->name);
      }
      return string_data_lt_func{}(f1->name, f2->name);
    }
  );
  return out;
}

CompactVector<php::Func*>
AnalysisIndex::lookup_func_closure_invokes(const php::Func& f) const {
  CompactVector<php::Func*> out;
  auto const& closures = folly::get_default(m_data->funcToClosures, &f);
  out.reserve(closures.size());
  for (auto const c : closures) {
    assertx(c->methods.size() == 1);
    out.emplace_back(c->methods[0].get());
  }
  std::sort(
    begin(out), end(out),
    [] (const php::Func* f1, const php::Func* f2) {
      return string_data_lt{}(f1->cls->name, f2->cls->name);
    }
  );
  return out;
}

res::Func AnalysisIndex::resolve_func(SString n) const {
  n = normalizeNS(n);
  if (m_data->mode == Mode::Constants) {
    return res::Func { res::Func::FuncName { n } };
  }
  m_data->deps->add(AnalysisDeps::Func { n });
  if (auto const finfo = folly::get_default(m_data->finfos, n)) {
    return res::Func { res::Func::Fun2 { finfo } };
  }
  if (m_data->badFuncs.contains(n)) {
    return res::Func { res::Func::MissingFunc { n } };
  }
  return res::Func { res::Func::FuncName { n } };
}

Optional<res::Class> AnalysisIndex::resolve_class(SString n) const {
  n = normalizeNS(n);
  m_data->deps->add(AnalysisDeps::Class { n });
  if (auto const cinfo = folly::get_default(m_data->cinfos, n)) {
    return res::Class::get(*cinfo);
  }
  if (m_data->typeAliases.contains(n)) return std::nullopt;
  // A php::Class should always be accompanied by it's ClassInfo,
  // unless if it's uninstantiable. So, if we have a php::Class here,
  // we know it's uninstantiable.
  if (m_data->badClasses.contains(n) || m_data->classes.contains(n)) {
    return std::nullopt;
  }
  return res::Class::getOrCreate(n);
}

Optional<res::Class> AnalysisIndex::resolve_class(const php::Class& cls) const {
  m_data->deps->add(AnalysisDeps::Class { cls.name });
  if (cls.cinfo) return res::Class::get(*cls.cinfo);
  return std::nullopt;
}

res::Func AnalysisIndex::resolve_func_or_method(const php::Func& f) const {
  m_data->deps->add(f);
  if (!f.cls) return res::Func { res::Func::Fun2 { &func_info(*m_data, f) } };
  return res::Func { res::Func::Method2 { &func_info(*m_data, f) } };
}

Type AnalysisIndex::lookup_constant(SString name) const {
  m_data->deps->add(AnalysisDeps::Constant { name });

  if (auto const p = folly::get_ptr(m_data->constants, name)) {
    auto const cns = p->first;
    if (type(cns->val) != KindOfUninit) return from_cell(cns->val);
    if (m_data->dynamicConstants.contains(name)) return TInitCell;
    auto const fname = Constant::funcNameFromName(name);
    auto const fit = m_data->finfos.find(fname);
    // We might have the unit present by chance, but without an
    // explicit dependence on the constant, we might not have the init
    // func present.
    if (fit == end(m_data->finfos)) return TInitCell;
    return return_type_for_func(*fit->second->func).t;
  }
  return m_data->badConstants.contains(name) ? TBottom : TInitCell;
}

std::vector<std::pair<SString, ConstIndex>>
AnalysisIndex::lookup_flattened_class_type_constants(
  const php::Class& cls
) const {
  std::vector<std::pair<SString, ConstIndex>> out;

  auto const cinfo = cls.cinfo;
  if (!cinfo) return out;

  out.reserve(cinfo->clsConstants.size());
  for (auto const& [name, idx] : cinfo->clsConstants) {
    if (idx.kind != ConstModifierFlags::Kind::Type) continue;
    out.emplace_back(name, idx.idx);
  }
  std::sort(
    begin(out), end(out),
    [] (auto const& p1, auto const& p2) {
      return string_data_lt{}(p1.first, p2.first);
    }
  );
  return out;
}

std::vector<std::pair<SString, ClsConstInfo>>
AnalysisIndex::lookup_class_constants(const php::Class& cls) const {
  std::vector<std::pair<SString, ClsConstInfo>> out;
  out.reserve(cls.constants.size());
  for (auto const& cns : cls.constants) {
    if (cns.kind != ConstModifierFlags::Kind::Value) continue;
    if (!cns.val) continue;
    if (cns.val->m_type != KindOfUninit) {
      out.emplace_back(cns.name, ClsConstInfo{ from_cell(*cns.val), 0 });
    } else if (!cls.cinfo) {
      out.emplace_back(cns.name, ClsConstInfo{ TInitCell, 0 });
    } else {
      out.emplace_back(cns.name, info_for_class_constant(cls, cns));
    }
  }
  return out;
}

ClsConstLookupResult
AnalysisIndex::lookup_class_constant(const Type& cls,
                                     const Type& name) const {
  ITRACE(4, "lookup_class_constant: ({}) {}::{}\n",
         show(current_context(*m_data)), show(cls), show(name));
  Trace::Indent _;

  AnalysisIndexAdaptor adaptor{*this};
  InTypeCns _2{adaptor, false};

  using R = ClsConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R{ TInitCell, TriBool::Maybe, true };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R{ TBottom, TriBool::No, false };
  };

  if (!is_specialized_cls(cls)) return conservative();

  // We could easily support the case where we don't know the constant
  // name, but know the class (like we do for properties), by unioning
  // together all possible constants. However it very rarely happens,
  // but when it does, the set of constants to union together can be
  // huge and it becomes very expensive.
  if (!is_specialized_string(name)) return conservative();
  auto const sname = sval_of(name);

  Optional<R> result;
  Optional<R> iresult;
  auto const addResult = [&] (R r) {
    if (!result.has_value()) {
      result.emplace(std::move(r));
    } else {
      *result |= r;
      ITRACE(4, "  -> {}\n", show(*result));
    }
  };

  auto const process = [&] (const ClassInfo2& cinfo) {
    ITRACE(4, "{}:\n", cinfo.name);
    Trace::Indent _;

    // Does the constant exist on this class?
    auto const idx = folly::get_ptr(cinfo.clsConstants, sname);
    if (!idx) {
      addResult(notFound());
      return;
    }

    // Is it an actual non-type-constant?
    assertx(!m_data->badClasses.contains(idx->idx.cls));
    if (idx->kind != ConstModifierFlags::Kind::Value) {
      addResult(notFound());
      return;
    }

    m_data->deps->add(idx->idx);

    auto const cnsCls = folly::get_default(m_data->classes, idx->idx.cls);
    if (!cnsCls) {
      addResult(conservative());
      return;
    }

    assertx(idx->idx.idx < cnsCls->constants.size());
    auto const& cns = cnsCls->constants[idx->idx.idx];
    assertx(cns.kind == ConstModifierFlags::Kind::Value);
    if (!cns.val.has_value()) {
      addResult(notFound());
      return;
    }

    auto r = [&] {
      if (type(*cns.val) != KindOfUninit) {
        // Fully resolved constant with a known value. We don't need
        // to register a dependency on the constant because the value
        // will never change.
        auto mightThrow = bool(cinfo.cls->attrs & AttrInternal);
        return R{ from_cell(*cns.val), TriBool::Yes, mightThrow };
      }

      ITRACE(4, "(dynamic)\n");

      auto info = info_for_class_constant(*cinfo.cls, cns);
      return R{
        std::move(info.type),
        TriBool::Yes,
        true
      };
    }();
    ITRACE(4, "-> {}\n", show(r));
    addResult(std::move(r));
  };

  auto const onSub = [&] (res::Class rcls) {
    m_data->deps->add(AnalysisDeps::Class { rcls.name() });
    auto const c = rcls.cls();
    if (!c || !c->cinfo) {
      addResult(conservative());
      return;
    }
    auto const sinfo = folly::get_ptr(c->cinfo->cnsSubInfo, sname);
    if (!sinfo) {
      addResult(notFound());
      return;
    }

    addResult(sinfo->result);
    for (auto const dynamic : sinfo->dynamic) {
      m_data->deps->add(AnalysisDeps::Class { dynamic });
      auto const childInfo = folly::get_default(m_data->cinfos, dynamic);
      if (childInfo) {
        process(*childInfo);
      } else {
        addResult(conservative());
      }
    }
  };

  auto const& dcls = dcls_of(cls);
  if (dcls.isExact() || dcls.isIsectAndExact()) {
    auto const rcls = dcls.smallestCls();
    m_data->deps->add(AnalysisDeps::Class { rcls.name() });
    auto const c = rcls.cls();
    if (!c || !c->cinfo) return conservative();
    process(*c->cinfo);
  } else if (dcls.isSub()) {
    onSub(dcls.smallestCls());
  } else {
    assertx(dcls.isIsect());
    auto const& isect = dcls.isect();
    assertx(isect.size() > 1);
    for (auto const rcls : isect) {
      onSub(rcls);
      if (!result.has_value()) {
        if (!iresult.has_value()) {
          iresult.emplace(notFound());
        } else {
          *iresult &= notFound();
        }
      } else if (!iresult.has_value()) {
        iresult = std::move(result);
      } else {
        *iresult &= *result;
      }
      result.reset();
    }
  }

  if (iresult.has_value()) {
    assertx(!result.has_value());
    ITRACE(4, "union -> {}\n", show(*iresult));
    return *iresult;
  }
  if (result.has_value()) {
    ITRACE(4, "union -> {}\n", show(*result));
    return *result;
  }

  return notFound();
}

ClsTypeConstLookupResult
AnalysisIndex::lookup_class_type_constant(
  const Type& cls,
  const Type& name,
  const Index::ClsTypeConstLookupResolver& resolver
) const {
  ITRACE(4, "lookup_class_type_constant: ({}) {}::{}\n",
         show(current_context(*m_data)), show(cls), show(name));
  Trace::Indent _;

  using R = ClsTypeConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R{
      TypeStructureResolution { TSDictN, true },
      TriBool::Maybe,
      TriBool::Maybe
    };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::No
    };
  };

  // Unlike lookup_class_constant, we distinguish abstract from
  // not-found, as the runtime sometimes treats them differently.
  auto const abstract = [] {
    ITRACE(4, "abstract\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::Yes
    };
  };

  if (!is_specialized_cls(cls)) return conservative();

  // As in lookup_class_constant, we could handle this, but it's not
  // worth it.
  if (!is_specialized_string(name)) return conservative();
  auto const sname = sval_of(name);

  Optional<R> result;
  auto const addResult = [&] (R r) {
    if (!result.has_value()) {
      result.emplace(std::move(r));
    } else {
      *result |= r;
      ITRACE(4, "  -> {}\n", show(*result));
    }
  };

  auto const process = [&] (const ClassInfo2& cinfo) {
    ITRACE(4, "{}:\n", cinfo.name);
    Trace::Indent _;

    // Does the type constant exist on this class?
    auto const idx = folly::get_ptr(cinfo.clsConstants, sname);
    if (!idx) {
      addResult(notFound());
      return;
    }

    // Is it an actual non-abstract type-constant?
    assertx(!m_data->badClasses.contains(idx->idx.cls));
    if (idx->kind != ConstModifierFlags::Kind::Type) {
      addResult(notFound());
      return;
    }

    m_data->deps->add(idx->idx);

    auto const cnsCls = folly::get_default(m_data->classes, idx->idx.cls);
    if (!cnsCls) {
      addResult(conservative());
      return;
    }

    assertx(idx->idx.idx < cnsCls->constants.size());
    auto const& cns = cnsCls->constants[idx->idx.idx];
    assertx(cns.kind == ConstModifierFlags::Kind::Type);
    if (!cns.val.has_value()) {
      addResult(abstract());
      return;
    }
    assertx(tvIsDict(*cns.val));

    ITRACE(4, "({}) {}\n", cns.cls, show(dict_val(val(*cns.val).parr)));

    // If we've been given a resolver, use it. Otherwise resolve it in
    // the normal way.
    auto resolved = resolver
      ? resolver(cns, *cinfo.cls)
      : resolve_type_structure(
        AnalysisIndexAdaptor { *this }, cns, *cinfo.cls
      );

    // The result of resolve_type_structure isn't, in general,
    // static. However a type-constant will always be, so force that
    // here.
    assertx(resolved.type.is(BBottom) || resolved.type.couldBe(BUnc));
    resolved.type &= TUnc;
    auto r = R{
      std::move(resolved),
      TriBool::Yes,
      TriBool::No
    };
    ITRACE(4, "-> {}\n", show(r));
    addResult(std::move(r));
  };

  // Can't use what we use for properties here because of the
  // possibility of contextual sensitive type-constants (we must visit
  // all subclasses).
  auto const& dcls = dcls_of(cls);
  auto const rcls = dcls.smallestCls();
  if (dcls.isExact() || dcls.isIsectAndExact()) {
    m_data->deps->add(AnalysisDeps::Class { rcls.name() });
    auto const c = rcls.cls();
    if (!c || !c->cinfo) return conservative();
    process(*c->cinfo);
  } else {
    auto const full = rcls.forEachSubclass(
      [&] (SString c, Attr) {
        m_data->deps->add(AnalysisDeps::Class { c });
        auto const childInfo = folly::get_default(m_data->cinfos, c);
        if (childInfo) {
          process(*childInfo);
        } else {
          addResult(conservative());
        }
      }
    );
    if (!full) return conservative();
  }

  if (result.has_value()) {
    ITRACE(4, "union -> {}\n", show(*result));
    return *result;
  }
  return notFound();
}

ClsTypeConstLookupResult
AnalysisIndex::lookup_class_type_constant(const php::Class& ctx,
                                          SString name,
                                          ConstIndex idx) const {
  ITRACE(4, "lookup_class_type_constant: ({}) {}::{} (from {})\n",
         show(current_context(*m_data)),
         ctx.name, name,
         show(idx, AnalysisIndexAdaptor{ *this }));
  Trace::Indent _;

  assertx(!m_data->badClasses.contains(idx.cls));

  using R = ClsTypeConstLookupResult;

  auto const conservative = [] {
    ITRACE(4, "conservative\n");
    return R {
      TypeStructureResolution { TSDictN, true },
      TriBool::Maybe,
      TriBool::Maybe
    };
  };

  auto const notFound = [] {
    ITRACE(4, "not found\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::No
    };
  };

  // Unlike lookup_class_constant, we distinguish abstract from
  // not-found, as the runtime sometimes treats them differently.
  auto const abstract = [] {
    ITRACE(4, "abstract\n");
    return R {
      TypeStructureResolution { TBottom, false },
      TriBool::No,
      TriBool::Yes
    };
  };

  m_data->deps->add(idx);
  auto const cinfo = folly::get_default(m_data->cinfos, idx.cls);
  if (!cinfo) return conservative();

  assertx(idx.idx < cinfo->cls->constants.size());
  auto const& cns = cinfo->cls->constants[idx.idx];
  if (cns.kind != ConstModifierFlags::Kind::Type) return notFound();
  if (!cns.val.has_value()) return abstract();

  assertx(tvIsDict(*cns.val));

  ITRACE(4, "({}) {}\n", cns.cls, show(dict_val(val(*cns.val).parr)));

  auto resolved = resolve_type_structure(
    AnalysisIndexAdaptor { *this }, cns, ctx
  );

  // The result of resolve_type_structure isn't, in general,
  // static. However a type-constant will always be, so force that
  // here.
  assertx(resolved.type.is(BBottom) || resolved.type.couldBe(BUnc));
  resolved.type &= TUnc;
  auto const r = R{
    std::move(resolved),
    TriBool::Yes,
    TriBool::No
  };
  ITRACE(4, "-> {}\n", show(r));
  return r;
}

PropState AnalysisIndex::lookup_private_props(const php::Class& cls) const {
  if (cls.cinfo && !cls.cinfo->inferred.privateProps.empty()) {
    auto cleaned = cls.cinfo->inferred.privateProps;
    for (auto& [n, e] : cleaned) {
      m_data->deps->add(AnalysisDeps::Property{ cls.name, n });
      e.ty = unserialize_type(std::move(e.ty));
    }
    return cleaned;
  }
  m_data->deps->add(AnalysisDeps::AnyProperty { cls.name });
  return make_unknown_propstate(
    *m_data,
    cls,
    [&] (const php::Prop& prop) {
      return (prop.attrs & AttrPrivate) && !(prop.attrs & AttrStatic);
    }
  );
}

PropState AnalysisIndex::lookup_private_statics(const php::Class& cls) const {
  if (cls.cinfo && !cls.cinfo->inferred.privateStatics.empty()) {
    auto cleaned = cls.cinfo->inferred.privateStatics;
    for (auto& [n, e] : cleaned) {
      m_data->deps->add(AnalysisDeps::Property{ cls.name, n });
      e.ty = unserialize_type(std::move(e.ty));
    }
    return cleaned;
  }
  m_data->deps->add(AnalysisDeps::AnyProperty { cls.name });
  return make_unknown_propstate(
    *m_data,
    cls,
    [&] (const php::Prop& prop) {
      return (prop.attrs & AttrPrivate) && (prop.attrs & AttrStatic);
    }
  );
}

PropState AnalysisIndex::lookup_public_statics(const php::Class& cls) const {
  // Public static property tracking not yet implemented, so be
  // conservative.
  return make_unknown_propstate(
    *m_data,
    cls,
    [&] (const php::Prop& prop) {
      return
        (prop.attrs & (AttrPublic|AttrProtected)) &&
        (prop.attrs & AttrStatic);
    }
  );
}

// Visit all classes that declare a given property without traversing
// the full class hierarchy.
//
// Parameters:
//   dcls - The class representing where the property is accessed
//        - from (which might not be the class which declares it).
//   prop - Property name to search for (nullptr to visit all properties)
//   f    - Invoked for each class that declares the property
//   i    - Invoked when merging results from intersection types
//   m    - Invoked when property has no declaring class
//
// Returns true if all relevant information was available, false if
// any were missing (indicating incomplete information requiring
// conservative fallback). Automatically registers dependencies on
// all declaring classes.
template <typename F, typename I, typename M>
bool AnalysisIndex::visit_prop_decls(const DCls& dcls,
                                     SString prop,
                                     const F& f,
                                     const I& i,
                                     const M& m) const {
  auto const addDep = [&] (SString cls) {
    if (prop) {
      m_data->deps->add(AnalysisDeps::Property { cls, prop });
    } else {
      m_data->deps->add(AnalysisDeps::AnyProperty { cls });
    }
  };

  auto const onDInfo = [&] (const PropDeclInfo& dinfo, bool sub) {
    auto success = true;

    if (dinfo.decl) {
      addDep(dinfo.decl);
      if (auto const cinfo = folly::get_default(m_data->cinfos, dinfo.decl)) {
        f(*cinfo);
      } else {
        success = false;
      }
    } else {
      m();
    }

    if (!sub) return success;
    // If the dinfo is empty, it means the property might exist in a
    // subclass, but isn't present in this class. Be pessimistic in
    // those cases. This is distinguished from having no dinfo at all,
    // which means it doesn't exist here or in any subclasses.
    if (!dinfo.decl && dinfo.subDecls.empty()) return false;

    for (auto const c : dinfo.subDecls) {
      addDep(c);
      if (auto const cinfo = folly::get_default(m_data->cinfos, c)) {
        f(*cinfo);
      } else {
        success = false;
      }
    }

    return success;
  };

  auto const onRCls = [&] (res::Class rcls, bool sub) {
    rcls.graph().ensureCInfo();
    auto const cinfo = rcls.cinfo2();
    if (!cinfo) return false;
    if (prop) {
      if (auto const dinfo = folly::get_ptr(cinfo->propDeclInfo, prop)) {
        return onDInfo(*dinfo, sub);
      }
      // No entry means it definitely doesn't exist.
      m();
      return true;
    } else {
      auto success = true;
      for (auto const& [n, dinfo] : cinfo->propDeclInfo) {
        success &= onDInfo(dinfo, sub);
      }
      return success;
    }
  };

  if (dcls.isExact()) {
    return onRCls(dcls.cls(), false);
  } else if (dcls.isSub()) {
    return onRCls(dcls.cls(), true);
  } else if (dcls.isIsect()) {
    auto const& isect = dcls.isect();
    assertx(isect.size() > 1);
    auto success = true;
    for (auto const r : isect) {
      success &= onRCls(r, true);
      i();
    }
    return success;
  } else {
    // Even though this has an intersection list, it must be the exact
    // class, so it's sufficient to process that.
    assertx(dcls.isIsectAndExact());
    return onRCls(dcls.isectAndExact().first, false);
  }
}

PropLookupResult
AnalysisIndex::lookup_static(Context ctx,
                             const PropertiesInfo& privateProps,
                             const Type& cls,
                             const Type& name) const {
  ITRACE(4, "lookup_static: {} {}::${}\n", show(ctx), show(cls), show(name));
  Trace::Indent _;

  using R = PropLookupResult;

  // First try to obtain the property name as a static string
  auto const sname = [&] () -> SString {
    // Treat non-string names conservatively, but the caller should be
    // checking this.
    if (!is_specialized_string(name)) return nullptr;
    return sval_of(name);
  }();

  // Conservative result when we can't do any better. The type can be
  // anything, and anything might throw.
  auto const conservative = [&] {
    ITRACE(4, "conservative\n");
    return R{
      TInitCell,
      sname,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      TriBool::Maybe,
      true
    };
  };

  // The property definitely wasn't found.
  auto const notFound = [&] {
    ITRACE(4, "nothing found\n");
    return PropLookupResult{
      TBottom,
      sname,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      TriBool::No,
      false
    };
  };

  // If we don't know what `cls' is, there's not much we can do.
  if (!is_specialized_cls(cls)) return conservative();

  // Turn the context class into a ClassInfo2* for convenience.
  if (ctx.cls && !ctx.cls->cinfo) return conservative();
  auto const ctxCls = ctx.cls ? ctx.cls->cinfo : nullptr;

  auto const initMightRaise =
    class_init_might_raise(*m_data, dcls_of(cls));

  auto const type = [&] (const php::Prop& prop,
                         const ClassInfo2& cinfo) {
    switch (prop.attrs & (AttrPublic|AttrProtected|AttrPrivate)) {
      case AttrPublic:
      case AttrProtected: {
        // Tracking of public statics not yet implemented. Use what we
        // can infer from the type-constraint.
        return union_of(
          adjust_type_for_prop(
            AnalysisIndexAdaptor { *this },
            *cinfo.cls,
            &prop.typeConstraints,
            TInitCell
          ),
          initial_type_for_public_sprop(
            *m_data,
            *cinfo.cls,
            prop
          )
        );
      }
      case AttrPrivate: {
        assertx(ctxCls == &cinfo);
        auto const elem = privateProps.readPrivateStatic(prop.name);
        if (!elem) return TInitCell;
        return remove_uninit(elem->ty);
      }
    }
    always_assert(false);
  };

  auto const fromProp = [&] (const php::Prop& prop,
                             const ClassInfo2& cinfo) {
    // The property was definitely found. Compute its attributes
    // from the prop metadata.
    return PropLookupResult{
      type(prop, cinfo),
      sname,
      TriBool::Yes,
      yesOrNo(prop.attrs & AttrIsConst),
      yesOrNo(prop.attrs & AttrIsReadonly),
      yesOrNo(prop.attrs & AttrLateInit),
      yesOrNo(prop.attrs & AttrInternal),
      initMightRaise
    };
  };

  Optional<R> result;
  auto const addResult = [&] (R r) {
    if (!result.has_value()) {
      result.emplace(std::move(r));
    } else {
      *result |= r;
      ITRACE(4, "  -> {}\n", show(*result));
    }
  };
  Optional<R> iresult;

  auto const full = visit_prop_decls(
    dcls_of(cls),
    sname,
    [&] (const ClassInfo2& cinfo) {
      if (sname) {
        for (auto const& prop : cinfo.cls->properties) {
          if (prop.name != sname) continue;
          // We have a matching prop. If its not static or not
          // accessible, the access will not succeed.
          if (!(prop.attrs & AttrStatic) ||
              !static_is_accessible(ctxCls, &cinfo, prop)) {
            ITRACE(
              6, "{}::${} found but inaccessible\n",
              cinfo.cls->name, sname
            );
            addResult(notFound());
            return;
          }

          // Otherwise its a match
          auto r = fromProp(prop, cinfo);
          ITRACE(6, "found {}:${} {}\n", cinfo.cls->name, prop.name, show(r));
          addResult(std::move(r));
          return;
        }

        addResult(notFound());
        return;
      }

      addResult(notFound());
      for (auto const& prop : cinfo.cls->properties) {
        if (!(prop.attrs & AttrStatic) ||
            !static_is_accessible(ctxCls, &cinfo, prop)) {
          ITRACE(
            6, "skipping inaccessible {}::${}\n",
            cinfo.cls->name, prop.name
          );
          continue;
        }

        auto r = fromProp(prop, cinfo);
        ITRACE(6, "including {}:${} {}\n", cinfo.cls->name, prop.name, show(r));
        addResult(std::move(r));
      }
    },
    [&] {
      if (!result.has_value()) {
        if (!iresult.has_value()) {
          iresult.emplace(notFound());
        } else {
          *iresult &= notFound();
        }
      } else if (!iresult.has_value()) {
        iresult = std::move(result);
      } else {
        *iresult &= *result;
      }
      result.reset();
    },
    [&] { addResult(notFound()); }
  );
  if (!full) return conservative();

  if (iresult.has_value()) {
    assertx(!result.has_value());
    ITRACE(4, "union -> {}\n", show(*iresult));
    return *iresult;
  }
  if (result.has_value()) {
    ITRACE(4, "union -> {}\n", show(*result));
    return *result;
  }
  return notFound();
}

Type AnalysisIndex::lookup_public_prop(const Type& obj,
                                       const Type& name) const {
  ITRACE(4, "lookup_public_prop: {}::${}\n", show(obj), show(name));
  Trace::Indent _;

  if (!is_specialized_obj(obj)) return TCell;

  if (!is_specialized_string(name)) return TCell;
  auto const sname = sval_of(name);

  Optional<Type> type;
  auto const addType = [&] (Type t) {
    if (!type.has_value()) {
      type.emplace(std::move(t));
    } else {
      *type |= t;
      ITRACE(4, "  -> {}\n", show(*type));
    }
  };
  Optional<Type> itype;

  auto const full = visit_prop_decls(
    dobj_of(obj),
    sname,
    [&] (const ClassInfo2& cinfo) {
      for (auto const& prop : cinfo.cls->properties) {
        if (prop.name != sname) continue;
        // Make sure its non-static and public. Otherwise its another
        // function's problem.
        if (prop.attrs & (AttrStatic | AttrPrivate)) {
          ITRACE(
            6, "{}::${} found but ineligible\n",
            cinfo.cls->name, sname
          );
          break;
        }

        // Otherwise its a match

        // Get a type corresponding to its declared type-hint (if any).
        auto ty = adjust_type_for_prop(
          AnalysisIndexAdaptor { *this },
          *cinfo.cls,
          &prop.typeConstraints,
          TCell
        );
        // We might have to include the initial value which might be
        // outside of the type-hint.
        auto initialTy = loosen_all(from_cell(prop.val));
        if (initialTy.subtypeOf(TUninit)) {
          m_data->deps->add(
            AnalysisDeps::Class { cinfo.name },
            AnalysisDeps::Type::PropInitVals
          );
        } else if (prop.attrs & AttrSystemInitialValue) {
          ty |= initialTy;
        }

        ITRACE(6, "found {}:${} {}\n", cinfo.cls->name, prop.name, show(ty));
        addType(std::move(ty));
        break;
      }
      return;
    },
    [&] {
      if (!type.has_value()) {
        if (!itype.has_value()) itype.emplace(TCell);
      } else if (!itype.has_value()) {
        itype = std::move(type);
      } else {
        *itype &= *type;
      }
      type.reset();
    },
    [&] { addType(TCell); }
  );
  if (!full) return TCell;

  if (itype.has_value()) {
    assertx(!type.has_value());
    ITRACE(4, "union -> {}\n", show(*itype));
    return *itype;
  }
  if (type.has_value()) {
    ITRACE(4, "union -> {}\n", show(*type));
    return *type;
  }

  // Unlike in the static case, we cannot assert TBottom if we don't
  // find anything. Properties can be set dynamically, so even if we
  // don't find a declaration, it could still exist.
  return TCell;
}

Slot AnalysisIndex::lookup_iface_vtable_slot(const php::Class& c) const {
  assertx(mode() == Mode::Final);
  return folly::get_default(m_data->ifaceSlotMap, c.name, kInvalidSlot);
}

Index::ReturnType AnalysisIndex::lookup_return_type(MethodsInfo* methods,
                                                    res::Func rfunc) const {
  using R = Index::ReturnType;

  auto const fromFInfo = [&] (const FuncInfo2& finfo) {
    m_data->deps->add(*finfo.func, AnalysisDeps::Type::RetType);
    return return_type_for_func(*finfo.func);
  };

  auto const meth = [&] (const FuncInfo2& finfo) {
    if (methods) {
      if (auto ret = methods->lookupReturnType(*finfo.func)) {
        return R{ unctx(std::move(ret->t)), ret->effectFree };
      }
    }
    return fromFInfo(finfo);
  };

  return match<R>(
    rfunc.val,
    [&] (res::Func::FuncName f) {
      m_data->deps->add(
        AnalysisDeps::Func { f.name },
        AnalysisDeps::Type::RetType
      );
      return R{ TInitCell, false };
    },
    [&] (res::Func::MethodName m) {
      // If we know the name of the class, we can register a
      // dependency on it. If not, nothing we can do.
      if (m.cls) m_data->deps->add(AnalysisDeps::Class { m.cls });
      return R{ TInitCell, false };
    },
    [&] (res::Func::Fun)                -> R { always_assert(false); },
    [&] (res::Func::Method)             -> R { always_assert(false); },
    [&] (res::Func::MethodFamily)       -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing)    -> R { always_assert(false); },
    [&] (res::Func::MissingFunc)        { return R{ TBottom, false }; },
    [&] (res::Func::MissingMethod)      { return R{ TBottom, false }; },
    [&] (const res::Func::Isect&)       -> R { always_assert(false); },
    [&] (res::Func::Fun2 f)             { return fromFInfo(*f.finfo); },
    [&] (res::Func::Method2 m)          { return meth(*m.finfo); },
    [&] (res::Func::MethodFamily2)      -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing2 m) { return meth(*m.finfo); },
    [&] (const res::Func::Isect2&)      -> R { always_assert(false); }
  );
}

Index::ReturnType
AnalysisIndex::lookup_return_type(MethodsInfo* methods,
                                  const CompactVector<Type>& args,
                                  const Type& context,
                                  res::Func rfunc) const {
  using R = Index::ReturnType;

  auto ty = lookup_return_type(methods, rfunc);

  auto const contextual = [&] (const FuncInfo2& finfo) {
    return context_sensitive_return_type(
      *m_data,
      { finfo.func, args, context },
      std::move(ty)
    );
  };

  return match<R>(
    rfunc.val,
    [&] (res::Func::FuncName)           { return ty; },
    [&] (res::Func::MethodName)         { return ty; },
    [&] (res::Func::Fun)                -> R { always_assert(false); },
    [&] (res::Func::Method)             -> R { always_assert(false); },
    [&] (res::Func::MethodFamily)       -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing)    -> R { always_assert(false); },
    [&] (res::Func::MissingFunc)        { return R{ TBottom, false }; },
    [&] (res::Func::MissingMethod)      { return R{ TBottom, false }; },
    [&] (const res::Func::Isect&)       -> R { always_assert(false); },
    [&] (res::Func::Fun2 f)             { return contextual(*f.finfo); },
    [&] (res::Func::Method2 m)          { return contextual(*m.finfo); },
    [&] (res::Func::MethodFamily2)      -> R { always_assert(false); },
    [&] (res::Func::MethodOrMissing2 m) { return contextual(*m.finfo); },
    [&] (const res::Func::Isect2&)      -> R { always_assert(false); }
  );
}

Index::ReturnType
AnalysisIndex::lookup_foldable_return_type(const CallContext& calleeCtx) const {
  constexpr size_t maxNestingLevel = 2;

  using R = Index::ReturnType;

  auto const& func = *calleeCtx.callee;

  if (m_data->mode == Mode::Constants) {
    ITRACE_MOD(
      Trace::hhbbc, 4,
      "Skipping inline interp of {} because analyzing constants\n",
      func_fullname(func)
    );
    return R{ TInitCell, false };
  }

  auto const ctxType =
    adjust_closure_context(AnalysisIndexAdaptor { *this }, calleeCtx);

  // Don't fold functions when staticness mismatches
  if (!func.isClosureBody) {
    if ((func.attrs & AttrStatic) && ctxType.couldBe(TObj)) {
      return R{ TInitCell, false };
    }
    if (!(func.attrs & AttrStatic) && ctxType.couldBe(TCls)) {
      return R{ TInitCell, false };
    }
  }

  auto const& finfo = func_info(*m_data, func);
  // No need to call unserialize_type here. If it's a scalar, there's
  // nothing to unserialize anyways.
  if (finfo.inferred.effectFree && is_scalar(finfo.inferred.returnTy)) {
    return R{ finfo.inferred.returnTy, finfo.inferred.effectFree };
  }

  auto const& caller = first_func_context(*m_data);

  using T = AnalysisDeps::Type;
  m_data->deps->add(func, T::ScalarRetType | T::Bytecode);
  if (m_data->foldableInterpNestingLevel > maxNestingLevel) {
    return R{ TInitCell, false };
  }

  if (!func.rawBlocks) {
    ITRACE_MOD(
      Trace::hhbbc, 4,
      "Skipping inline interp of {} because bytecode not present\n",
      func_fullname(func)
    );
    return R{ TInitCell, false };
  }

  auto contextualRet = [&] () -> Optional<Type> {
    ++m_data->foldableInterpNestingLevel;
    SCOPE_EXIT { --m_data->foldableInterpNestingLevel; };

    auto const wf = php::WideFunc::cns(&func);
    auto const fa = analyze_func_inline(
      AnalysisIndexAdaptor { *this },
      AnalysisContext {
        func.unit,
        wf,
        func.cls,
        &context_for_deps(*m_data)
      },
      ctxType,
      calleeCtx.args,
      nullptr,
      CollectionOpts::EffectFreeOnly
    );
    if (!fa.effectFree) return std::nullopt;
    return fa.inferredReturn;
  }();

  if (!contextualRet) {
    ITRACE_MOD(
      Trace::hhbbc, 4,
      "Foldable inline analysis failed due to possible side-effects\n"
    );
    return R{ TInitCell, false };
  }

  auto const insensitive = return_type_for_func(func).t;

  ITRACE_MOD(
    Trace::hhbbc, 4,
    "Foldable return type: {}, context insensitive type: {}\n",
    show(*contextualRet),
    show(insensitive)
  );

  auto const error_context = [&] {
    using namespace folly::gen;
    return folly::sformat(
      "{} calling {} (context: {}, args: {})",
      show(caller),
      func_fullname(func),
      show(calleeCtx.context),
      from(calleeCtx.args)
        | map([] (const Type& t) { return show(t); })
        | unsplit<std::string>(",")
    );
  };

  // The context sensitive type could be a subtype of the insensitive
  // type if the analysis took advantage of the known arguments. On
  // the other-hand, it could be a supertype of the insensitive type
  // if we didn't have the same dependencies present as when the
  // insensitive type was produced. So, we cannot make any assumptions
  // about the two's relationship (except that they must have a
  // non-empty intersection).
  always_assert_flog(
    contextualRet->is(BBottom) || contextualRet->couldBe(insensitive),
    "Context sensitive return type for {} is {} "
    "which is not compatible with context insensitive "
    "return type {}\n",
    error_context(),
    show(*contextualRet),
    show(insensitive)
  );
  *contextualRet &= insensitive;
  if (!is_scalar(*contextualRet)) return R{ TInitCell, false };
  return R{ *contextualRet, true };
}

std::pair<Index::ReturnType, size_t>
AnalysisIndex::lookup_return_type_raw(const php::Func& f) const {
  auto const& finfo = func_info(*m_data, f);
  return std::make_pair(
    Index::ReturnType{
      unserialize_type(finfo.inferred.returnTy),
      finfo.inferred.effectFree
    },
    finfo.inferred.returnRefinements
  );
}

CompactVector<Type>
AnalysisIndex::lookup_closure_use_vars(const php::Func& func) const {
  assertx(func.isClosureBody);
  assertx(func.cls);
  assertx(is_closure(*func.cls));

  auto const numUseVars = closure_num_use_vars(&func);
  if (!numUseVars) return {};
  m_data->deps->add(
    AnalysisDeps::Class{ func.cls->name },
    AnalysisDeps::Type::UseVars
  );
  auto const cinfo = func.cls->cinfo;
  if (!cinfo) return CompactVector<Type>{numUseVars, TCell};
  auto const& inferred = cinfo->inferred.useVars;

  CompactVector<Type> out;
  out.reserve(numUseVars);
  for (size_t i = 0; i < numUseVars; ++i) {
    out.emplace_back(unserialize_type(inferred.get_default(i, TCell)));
  }

  if (auto const rinfo = retained_for_context(*m_data)) {
    auto better = false;
    if (auto const info = rinfo->get(*cinfo)) {
      auto const& retained = info->useVars;
      for (size_t i = 0; i < numUseVars; ++i) {
        out[i] &= unserialize_type(retained.get_default(i, TCell));
        auto const old = unserialize_type(inferred.get_default(i, TCell));
        if (out[i].strictSubtypeOf(old)) better = true;
      }
    }

    if (should_retain(*cinfo, better, *m_data)) {
      ITRACE_MOD(
        Trace::hhbbc, 4,
        "Retaining inferred closure use-vars type information about {}\n",
        func.cls
      );
      auto& info = rinfo->retain(*cinfo);
      info.useVars.clear();
      info.useVars.reserve(numUseVars);
      for (auto const& t : out) info.useVars.emplace_back(serialize_type(t));
    }
  }

  return out;
}

CompactVector<Type>
AnalysisIndex::lookup_closure_use_vars_raw(const php::Func& func) const {
  assertx(func.isClosureBody);
  assertx(func.cls);
  assertx(is_closure(*func.cls));

  auto const numUseVars = closure_num_use_vars(&func);
  if (!numUseVars) return {};
  auto const cinfo = func.cls->cinfo;
  if (!cinfo) return CompactVector<Type>{numUseVars, TCell};
  auto const& inferred = cinfo->inferred.useVars;

  CompactVector<Type> out;
  out.reserve(numUseVars);
  for (size_t i = 0; i < numUseVars; ++i) {
    out.emplace_back(unserialize_type(inferred.get_default(i, TCell)));
  }
  return out;
}

bool AnalysisIndex::func_depends_on_arg(const php::Func& func,
                                        size_t arg) const {
  m_data->deps->add(func, AnalysisDeps::Type::UnusedParams);
  auto const& finfo = func_info(*m_data, func);
  if (arg >= finfo.inferred.unusedParams.size()) return true;

  auto depends = !finfo.inferred.unusedParams.test(arg);
  if (auto const rinfo = retained_for_context(*m_data)) {
    auto better = false;
    if (auto const info = rinfo->get(finfo)) {
      if (depends && info->unusedParams.test(arg)) {
        depends = false;
        better = true;
      }
    }

    if (should_retain(finfo, better, *m_data)) {
      ITRACE_MOD(
        Trace::hhbbc, 4,
        "Retaining inferred param dependency information ({} on {}) about {}\n",
        depends ? "depends" : "does not depend",
        arg,
        func_fullname(func)
      );
      auto& info = rinfo->retain(finfo);
      info.unusedParams.set(arg, !depends);
    }
  }

  return depends;
}

/*
 * Given a DCls, return the most specific res::Func for that DCls. For
 * intersections, this will call process/general on every component of
 * the intersection and combine the results. process is called to
 * obtain a res::Func from a ClassInfo. If a ClassInfo isn't
 * available, general will be called instead.
 */
template <typename P, typename G>
res::Func AnalysisIndex::rfunc_from_dcls(const DCls& dcls,
                                         SString name,
                                         const P& process,
                                         const G& general) const {
  using Func = res::Func;
  using Class = res::Class;

  /*
   * Combine together multiple res::Funcs together. Since the DCls
   * represents a class which is a subtype of every ClassInfo in the
   * list, every res::Func we get is true.
   *
   * The relevant res::Func types in order from most general to more
   * specific are:
   *
   * MethodName -> FuncFamily -> MethodOrMissing -> Method -> Missing
   *
   * Since every res::Func in the intersection is true, we take the
   * res::Func which is most specific. Two different res::Funcs cannot
   * be contradict. For example, we shouldn't get a Method and a
   * Missing since one implies there's no func and the other implies
   * one specific func. Or two different res::Funcs shouldn't resolve
   * to two different methods.
   */
  auto missing = TriBool::Maybe;
  Func::Isect2 isect;
  const php::Func* singleMethod = nullptr;

  auto const onFunc = [&] (Func func) {
    match(
      func.val,
      [&] (Func::MethodName)      {},
      [&] (Func::Method)          { always_assert(false); },
      [&] (Func::MethodFamily)    { always_assert(false); },
      [&] (Func::MethodOrMissing) { always_assert(false); },
      [&] (Func::MissingMethod) {
        assertx(missing != TriBool::No);
        singleMethod = nullptr;
        isect.families.clear();
        missing = TriBool::Yes;
      },
      [&] (Func::FuncName)        { always_assert(false); },
      [&] (Func::Fun)             { always_assert(false); },
      [&] (Func::Fun2)            { always_assert(false); },
      [&] (Func::Method2 m) {
        if (singleMethod) {
          assertx(missing != TriBool::Yes);
          assertx(isect.families.empty());
          if (singleMethod != m.finfo->func) {
            singleMethod = nullptr;
            missing = TriBool::Yes;
          } else {
            missing = TriBool::No;
          }
        } else if (missing != TriBool::Yes) {
          singleMethod = m.finfo->func;
          isect.families.clear();
          missing = TriBool::No;
        }
      },
      [&] (Func::MethodFamily2)   { always_assert(false); },
      [&] (Func::MethodOrMissing2 m) {
        if (singleMethod) {
          assertx(missing != TriBool::Yes);
          assertx(isect.families.empty());
          if (singleMethod != m.finfo->func) {
            singleMethod = nullptr;
            missing = TriBool::Yes;
          }
        } else if (missing != TriBool::Yes) {
          singleMethod = m.finfo->func;
          isect.families.clear();
        }
      },
      [&] (Func::MissingFunc)     { always_assert(false); },
      [&] (const Func::Isect&)    { always_assert(false); },
      [&] (const Func::Isect2&)   { always_assert(false); }
    );
  };

  auto const onClass = [&] (Class cls, bool isExact) {
    auto const g = cls.graph();
    g.ensureCInfo();

    if (auto const cinfo = g.cinfo2()) {
      onFunc(process(cinfo, isExact, dcls.containsNonRegular()));
    } else {
      // The class doesn't have a ClassInfo present, so we cannot call
      // process. We can, however, look at any parents that do have a
      // ClassInfo. This won't result in as good results, but it
      // preserves monotonicity.
      g.ensure();
      onFunc(general(dcls.containsNonRegular()));
      if (g.isMissing()) return;

      if (!dcls.containsNonRegular() && !g.mightBeRegular()) {
        g.ensureWithChildren(true);
        if (g.hasCompleteChildren()) {
          if (isExact) {
            onFunc(
              Func { Func::MissingMethod { dcls.smallestCls().name(), name } }
            );
            return;
          }

          auto const common = g.commonParentsOfRegSubs();
          if (common.empty()) {
            onFunc(
              Func { Func::MissingMethod { dcls.smallestCls().name(), name } }
            );
            return;
          }

          for (auto const p : common) {
            p.ensureCInfo();
            if (auto const cinfo = p.cinfo2()) {
              onFunc(process(cinfo, false, false));
            }
          }
          return;
        }
      }

      g.walkParents(
        [&] (ClassGraph p) {
          p.ensureCInfo();
          if (auto const cinfo = p.cinfo2()) {
            onFunc(process(cinfo, false, dcls.containsNonRegular()));
            return false;
          }
          return true;
        }
      );
    }
  };

  if (dcls.isExact() || dcls.isSub()) {
    // If this isn't an intersection, there's only one class to
    // process and we're done.
    onClass(dcls.cls(), dcls.isExact());
  } else if (dcls.isIsect()) {
    for (auto const c : dcls.isect()) onClass(c, false);
  } else {
    assertx(dcls.isIsectAndExact());
    auto const [e, i] = dcls.isectAndExact();
    onClass(e, true);
    for (auto const c : *i) onClass(c, false);
  }

  // If we got a method, that always wins. Again, every res::Func is
  // true, and method is more specific than a FuncFamily, so it is
  // preferred.
  if (singleMethod) {
    assertx(missing != TriBool::Yes);
    // If missing is Maybe, then *every* resolution was to a
    // MethodName or MethodOrMissing, so include that fact here by
    // using MethodOrMissing.
    if (missing == TriBool::Maybe) {
      return Func {
        Func::MethodOrMissing2 { &func_info(*m_data, *singleMethod) }
      };
    }
    return Func { Func::Method2 { &func_info(*m_data, *singleMethod) } };
  }
  // We only got unresolved classes. If missing is TriBool::Yes, the
  // function doesn't exist. Otherwise be pessimistic.
  if (isect.families.empty()) {
    if (missing == TriBool::Yes) {
      return Func { Func::MissingMethod { dcls.smallestCls().name(), name } };
    }
    assertx(missing == TriBool::Maybe);
    return general(dcls.containsNonRegular());
  }
  // Isect case. Isects always might contain missing funcs.
  assertx(missing == TriBool::Maybe);

  // We could add a FuncFamily multiple times, so remove duplicates.
  std::sort(begin(isect.families), end(isect.families));
  isect.families.erase(
    std::unique(begin(isect.families), end(isect.families)),
    end(isect.families)
  );
  // If everything simplifies down to a single FuncFamily, just use
  // that.
  if (isect.families.size() == 1) {
    return Func {
      Func::MethodFamily2 { isect.families[0], isect.regularOnly }
    };
  }
  return Func { std::move(isect) };
}

res::Func AnalysisIndex::resolve_method(const Type& thisType,
                                        SString name) const {
  assertx(thisType.subtypeOf(BCls) || thisType.subtypeOf(BObj));

  using Func = res::Func;

  auto const general = [&] (SString maybeCls, bool) {
    assertx(name != s_construct.get());
    return Func { Func::MethodName { maybeCls, name } };
  };

  auto const process = [&] (ClassInfo2* cinfo,
                            bool isExact,
                            bool includeNonRegular) {
    assertx(name != s_construct.get());

    // In some cases we can resolve a method exactly if the name
    // matches a private method declared on the context.
    auto const priv = [&] () -> const php::Func* {
      auto const& ctx = current_context(*m_data);
      if (!ctx.cls || !ctx.cls->cinfo) return nullptr;
      m_data->deps->add(AnalysisDeps::Class { ctx.cls->name });

      auto const meth = folly::get_ptr(ctx.cls->cinfo->methods, name);
      if (!meth || !(meth->attrs & AttrPrivate) || !meth->topLevel()) {
        return nullptr;
      }
      m_data->deps->add(meth->meth());

      auto const check = [&] {
        if (ctx.cls == cinfo->cls) return true;

        auto const ctxType =
          subCls(res::Class::get(*ctx.cls->cinfo), includeNonRegular);
        auto const cinfoType = isExact
          ? clsExact(res::Class::get(*cinfo), includeNonRegular)
          : subCls(res::Class::get(*cinfo), includeNonRegular);
        if (!cinfoType.subtypeOf(ctxType)) return false;
        if (ctxType.subtypeOf(cinfoType))  return true;
        return false;
      }();
      if (!check) return nullptr;
      return func_from_meth_ref(*m_data, meth->meth());
    }();
    if (priv) return Func { Func::Method2 { &func_info(*m_data, *priv) } };

    auto const meth = folly::get_ptr(cinfo->methods, name);
    if (!meth) {
      // We don't store metadata for special methods, so be pessimistic
      // (the lack of a method entry does not mean the call might fail
      // at runtme).
      if (is_special_method_name(name)) {
        return Func { Func::MethodName { cinfo->name, name } };
      }
      // We're only considering this class, not it's subclasses. Since
      // it doesn't exist here, the resolution will always fail.
      if (isExact) {
        return Func { Func::MissingMethod { cinfo->name, name } };
      }
      // The method isn't present on this class, but it might be in the
      // subclasses. In most cases try a general lookup to get a
      // slightly better type than nothing.
      if (includeNonRegular ||
          !(cinfo->cls->attrs & (AttrInterface|AttrAbstract))) {
        return general(cinfo->name, includeNonRegular);
      }

      // A special case is if we're only considering regular classes,
      // and this is an interface or abstract class. For those, we
      // "expand" the method families table to include any methods
      // defined in *all* regular subclasses. This is needed to
      // preserve monotonicity. Check this now.
      auto const entry = folly::get_ptr(cinfo->methodFamilies, name);
      // If no entry, treat it pessimistically like the rest of the
      // cases.
      if (!entry) return general(cinfo->name, false);
      // We found an entry. This cannot be empty (remember the method
      // is guaranteed to exist on *all* regular subclasses), and must
      // be complete (for the same reason). Use it.
      assertx(!entry->m_regularIncomplete);
      return match<Func>(
        entry->m_meths,
        [&] (const FuncFamilyEntry::BothFF&) {
          // Be conservative for now
          return general(cinfo->name, false);
        },
        [&] (const FuncFamilyEntry::FFAndSingle& e) {
          m_data->deps->add(e.m_regular);
          auto const func = func_from_meth_ref(*m_data, e.m_regular);
          if (!func) return general(cinfo->name, false);
          return Func { Func::Method2 { &func_info(*m_data, *func) } };
        },
        [&] (const FuncFamilyEntry::FFAndNone&) -> Func {
          always_assert(false);
        },
        [&] (const FuncFamilyEntry::BothSingle& e) {
          m_data->deps->add(e.m_all);
          auto const func = func_from_meth_ref(*m_data, e.m_all);
          if (!func) return general(cinfo->name, false);
          return Func { Func::Method2 { &func_info(*m_data, *func) } };
        },
        [&] (const FuncFamilyEntry::SingleAndNone&) -> Func {
          always_assert(false);
        },
        [&] (const FuncFamilyEntry::None&) -> Func {
          always_assert(false);
        }
      );
    }

    // We found a method declared on this class.

    m_data->deps->add(meth->meth());
    auto const func = func_from_meth_ref(*m_data, meth->meth());
    if (!func) return general(cinfo->name, includeNonRegular);

    // We don't store method family information about special methods
    // and they have special inheritance semantics.
    if (is_special_method_name(name)) {
      // If we know the class exactly, we can use ftarget.
      if (isExact) {
        return Func { Func::Method2 { &func_info(*m_data, *func) } };
      }
      // The method isn't overwritten, but they don't inherit, so it
      // could be missing.
      if (meth->attrs & AttrNoOverride) {
        return Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } };
      }
      // Otherwise be pessimistic.
      return Func { Func::MethodName { cinfo->name, name } };
    }

    // Private method handling: Private methods have special lookup
    // rules. If we're in the context of a particular class, and that
    // class defines a private method, an instance of the class will
    // always call that private method (even if overridden) in that
    // context.
    assertx(cinfo->cls);
    auto const& ctx = current_context(*m_data);
    if (ctx.cls == cinfo->cls) {
      // The context matches the current class. If we've looked up a
      // private method (defined on this class), then that's what
      // we'll call.
      if ((meth->attrs & AttrPrivate) && meth->topLevel()) {
        return Func { Func::Method2 { &func_info(*m_data, *func) } };
      }
    } else if ((meth->attrs & AttrPrivate) || meth->hasPrivateAncestor()) {
      // Otherwise the context doesn't match the current class. If the
      // looked up method is private, or has a private ancestor,
      // there's a chance we'll call that method (or
      // ancestor). Otherwise there's no private method in the
      // inheritance tree we'll call.
      auto conservative = false;
      auto const ancestor = [&] () -> const php::Func* {
        if (!ctx.cls) return nullptr;
        m_data->deps->add(AnalysisDeps::Class { ctx.cls->name });

        // Look up the ClassInfo corresponding to the context.
        auto const ctxCInfo = ctx.cls->cinfo;
        if (!ctxCInfo) return nullptr;
        // Is this context a parent of our class?
        if (!cinfo->classGraph.isChildOf(ctxCInfo->classGraph)) {
          return nullptr;
        }
        // It is. See if it defines a private method.
        auto const ctxMeth = folly::get_ptr(ctxCInfo->methods, name);
        if (!ctxMeth) return nullptr;
        // If it defines a private method, use it.
        if ((ctxMeth->attrs & AttrPrivate) && ctxMeth->topLevel()) {
          m_data->deps->add(ctxMeth->meth());
          auto const ctxFunc = func_from_meth_ref(*m_data, ctxMeth->meth());
          if (!ctxFunc) conservative = true;
          return ctxFunc;
        }
        // Otherwise do normal lookup.
        return nullptr;
      }();
      if (ancestor) {
        return Func { Func::Method2 { &func_info(*m_data, *ancestor) } };
      } else if (conservative) {
        return Func { Func::MethodName { cinfo->name, name } };
      }
    }

    // If we're only including regular subclasses, and this class
    // itself isn't regular, the result may not necessarily include
    // func.
    if (!includeNonRegular && !is_regular_class(*cinfo->cls)) {
      // We're not including this base class. If we're exactly this
      // class, there's no method at all. It will always be missing.
      if (isExact) {
        return Func { Func::MissingMethod { cinfo->name, name } };
      }
      if (meth->noOverrideRegular()) {
        // The method isn't overridden in a subclass, but we can't use
        // the base class either. This leaves two cases. Either the
        // method isn't overridden because there are no regular
        // subclasses (in which case there's no resolution at all), or
        // because there's regular subclasses, but they use the same
        // method (in which case the result is just func).
        if (!cinfo->classGraph.mightHaveRegularSubclass()) {
          return Func { Func::MissingMethod { cinfo->name, name } };
        }
        return Func { Func::Method2 { &func_info(*m_data, *func) } };
      }
      // Otherwise, fall through into the general case.
    } else if (isExact ||
               meth->attrs & AttrNoOverride ||
               (!includeNonRegular && meth->noOverrideRegular())) {
      // Either we want all classes, or the base class is regular. If
      // the method isn't overridden we know it must be just func (the
      // override bits include it being missing in a subclass, so we
      // know it cannot be missing either).
      return Func { Func::Method2 { &func_info(*m_data, *func) } };
    }

    // Look up the entry in the normal method family table and use
    // whatever is there.
    auto const entry = folly::get_ptr(cinfo->methodFamilies, name);
    // Since this method isn't AttrNoOverride, We should always have
    // an entry for it.
    always_assert(entry != nullptr);

    return match<Func>(
      entry->m_meths,
      [&] (const FuncFamilyEntry::BothFF&) {
        // Be conservative for now
        return general(cinfo->name, includeNonRegular);
      },
      [&] (const FuncFamilyEntry::FFAndSingle& e) {
        if (includeNonRegular) return general(cinfo->name, true);
        m_data->deps->add(e.m_regular);
        auto const func = func_from_meth_ref(*m_data, e.m_regular);
        if (!func) return general(cinfo->name, false);
        return entry->m_regularIncomplete
          ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
          : Func { Func::Method2 { &func_info(*m_data, *func) } };
      },
      [&] (const FuncFamilyEntry::FFAndNone&) {
        assertx(includeNonRegular);
        return general(cinfo->name, true);
      },
      [&] (const FuncFamilyEntry::BothSingle& e) {
        m_data->deps->add(e.m_all);
        auto const func = func_from_meth_ref(*m_data, e.m_all);
        if (!func) return general(cinfo->name, includeNonRegular);
        return
          ((includeNonRegular && entry->m_allIncomplete) ||
           (!includeNonRegular && entry->m_regularIncomplete))
          ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
          : Func { Func::Method2 { &func_info(*m_data, *func) } };
      },
      [&] (const FuncFamilyEntry::SingleAndNone& e) {
        assertx(includeNonRegular);
        m_data->deps->add(e.m_all);
        auto const func = func_from_meth_ref(*m_data, e.m_all);
        if (!func) return general(cinfo->name, true);
        return entry->m_allIncomplete
          ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
          : Func { Func::Method2 { &func_info(*m_data, *func) } };
      },
      [&] (const FuncFamilyEntry::None&) -> Func {
        always_assert(false);
      }
    );
  };

  auto const isClass = thisType.subtypeOf(BCls);
  if (name == s_construct.get()) {
    if (isClass) {
      return Func { Func::MethodName { nullptr, s_construct.get() } };
    }
    return resolve_ctor(thisType);
  }

  if (m_data->mode == Mode::Constants) return general(nullptr, true);

  if (isClass) {
    if (!is_specialized_cls(thisType)) return general(nullptr, true);
  } else if (!is_specialized_obj(thisType)) {
    return general(nullptr, false);
  }

  auto const& dcls = isClass ? dcls_of(thisType) : dobj_of(thisType);
  if (dcls.isCtx()) {
    auto const& ctx = current_context(*m_data);
    assertx(ctx.cls);
    if (ctx.cls->cinfo) {
      m_data->deps->add(AnalysisDeps::Class { ctx.cls->name });
      auto const meth = folly::get_ptr(ctx.cls->cinfo->methods, name);
      if (meth && (meth->attrs & AttrPrivate) && meth->topLevel()) {
        if (auto const func = func_from_meth_ref(*m_data, meth->meth())) {
          m_data->deps->add(meth->meth());
          return Func { Func::Method2 { &func_info(*m_data, *func) } };
        }
      }
    }
  }

  return rfunc_from_dcls(
    dcls,
    name,
    process,
    [&] (bool i) { return general(dcls.smallestCls().name(), i); }
  );
}

res::Func AnalysisIndex::resolve_ctor(const Type& obj) const {
  assertx(obj.subtypeOf(BObj));

  using Func = res::Func;

  if (m_data->mode == Mode::Constants) {
    return Func { Func::MethodName { nullptr, s_construct.get() } };
  }

  // Can't say anything useful if we don't know the object type.
  if (!is_specialized_obj(obj)) {
    return Func { Func::MethodName { nullptr, s_construct.get() } };
  }

  auto const& dcls = dobj_of(obj);
  return rfunc_from_dcls(
    dcls,
    s_construct.get(),
    [&] (ClassInfo2* cinfo, bool isExact, bool includeNonRegular) {
      // We're dealing with an object here, which never uses
      // non-regular classes.
      assertx(!includeNonRegular);

      // See if this class has a ctor.
      auto const meth = folly::get_ptr(cinfo->methods, s_construct.get());
      if (!meth) {
        // There's no ctor on this class. This doesn't mean the ctor
        // won't exist at runtime, it might get the default ctor, so
        // we have to be conservative.
        return Func { Func::MethodName { cinfo->name, s_construct.get() } };
      }
      m_data->deps->add(meth->meth());

      // We have a ctor, but it might be overridden in a subclass.
      assertx(!(meth->attrs & AttrStatic));
      auto const func = func_from_meth_ref(*m_data, meth->meth());
      if (!func) {
        // Relevant function doesn't exist on the AnalysisIndex. Be
        // conservative.
        return Func { Func::MethodName { cinfo->name, s_construct.get() } };
      }
      assertx(!(func->attrs & AttrStatic));

      // If this class is known exactly, or we know nothing overrides
      // this ctor, we know this ctor is precisely it.
      if (isExact || meth->noOverrideRegular()) {
        // If this class isn't regular, and doesn't have any regular
        // subclasses (or if it's exact), this resolution will always
        // fail.
        if (!is_regular_class(*cinfo->cls) &&
            (isExact || !cinfo->classGraph.mightHaveRegularSubclass())) {
          return Func {
            Func::MissingMethod { cinfo->name, s_construct.get() }
          };
        }
        return Func { Func::Method2 { &func_info(*m_data, *func) } };
      }

      // Look up the entry in the normal method family table and use
      // whatever is there.
      auto const entry =
        folly::get_ptr(cinfo->methodFamilies, s_construct.get());
      // Since this method isn't AttrNoOverride, We should always have
      // an entry for it.
      always_assert(entry != nullptr);

      return match<Func>(
        entry->m_meths,
        [&] (const FuncFamilyEntry::BothFF&) {
          // Be conservative for now
          return Func {
            Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
          };
        },
        [&] (const FuncFamilyEntry::FFAndSingle& e) {
          if (includeNonRegular) {
            return Func {
              Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
            };
          }
          m_data->deps->add(e.m_regular);
          auto const func = func_from_meth_ref(*m_data, e.m_regular);
          if (!func) {
            return Func {
              Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
            };
          }
          return entry->m_regularIncomplete
            ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
            : Func { Func::Method2 { &func_info(*m_data, *func) } };
        },
        [&] (const FuncFamilyEntry::FFAndNone&) {
          assertx(includeNonRegular);
          return Func {
            Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
          };
        },
        [&] (const FuncFamilyEntry::BothSingle& e) {
          m_data->deps->add(e.m_all);
          auto const func = func_from_meth_ref(*m_data, e.m_all);
          if (!func) {
            return Func {
              Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
            };
          }
          return
            ((includeNonRegular && entry->m_allIncomplete) ||
             (!includeNonRegular && entry->m_regularIncomplete))
            ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
            : Func { Func::Method2 { &func_info(*m_data, *func) } };
        },
        [&] (const FuncFamilyEntry::SingleAndNone& e) {
          assertx(includeNonRegular);
          m_data->deps->add(e.m_all);
          auto const func = func_from_meth_ref(*m_data, e.m_all);
          if (!func) {
            return Func {
              Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
            };
          }
          return entry->m_allIncomplete
            ? Func { Func::MethodOrMissing2 { &func_info(*m_data, *func) } }
            : Func { Func::Method2 { &func_info(*m_data, *func) } };
        },
        [&] (const FuncFamilyEntry::None&) -> Func {
          always_assert(false);
        }
      );
    },
    [&] (bool includeNonRegular) {
      assertx(!includeNonRegular);
      return Func {
        Func::MethodName { dcls.smallestCls().name(), s_construct.get() }
      };
    }
  );
}

std::pair<const php::TypeAlias*, bool>
AnalysisIndex::lookup_type_alias(SString name) const {
  m_data->deps->add(AnalysisDeps::Class { name });
  if (m_data->classes.contains(name)) return std::make_pair(nullptr, false);
  if (auto const ta = folly::get_ptr(m_data->typeAliases, name)) {
    return std::make_pair(ta->first, true);
  }
  return std::make_pair(nullptr, !m_data->badClasses.contains(name));
}

Index::ClassOrTypeAlias
AnalysisIndex::lookup_class_or_type_alias(SString n) const {
  n = normalizeNS(n);
  m_data->deps->add(AnalysisDeps::Class { n });
  if (auto const cls = folly::get_default(m_data->classes, n)) {
    return Index::ClassOrTypeAlias{cls, nullptr, true};
  }
  if (auto const ta = folly::get_ptr(m_data->typeAliases, n)) {
    return Index::ClassOrTypeAlias{nullptr, ta->first, true};
  }
  return Index::ClassOrTypeAlias{
    nullptr,
    nullptr,
    !m_data->badClasses.contains(n)
  };
}

PropMergeResult AnalysisIndex::merge_static_type(
    Context ctx,
    PublicSPropMutations& publicMutations,
    PropertiesInfo& privateProps,
    const Type& cls,
    const Type& name,
    const Type& val,
    bool checkUB,
    bool ignoreConst,
    bool mustBeReadOnly) const {
  ITRACE(
    4, "merge_static_type: {} {}::${} {}\n",
    show(ctx), show(cls), show(name), show(val)
  );
  Trace::Indent _;

  assertx(val.subtypeOf(BInitCell));

  using R = PropMergeResult;

  // In some cases we might try to merge Bottom if we're in
  // unreachable code. This won't affect anything, so just skip out
  // early.
  if (val.subtypeOf(BBottom)) return R{ TBottom, TriBool::No };

  // Try to turn the given property name into a static string
  auto const sname = [&] () -> SString {
    // Non-string names are treated conservatively here. The caller
    // should be checking for these and doing the right thing.
    if (!is_specialized_string(name)) return nullptr;
    return sval_of(name);
  }();

  // The case where we don't know `cls':
  auto const unknownCls = [&] {
    if (!sname) {
      // Very bad case. We don't know `cls' or the property name. This
      // mutation can be affecting anything, so merge it into all
      // properties (this drops type information for public
      // properties).
      ITRACE(4, "unknown class and prop. merging everything\n");
      privateProps.mergeInAllPrivateStatics(
        AnalysisIndexAdaptor { *this },
        unctx(val),
        ignoreConst,
        mustBeReadOnly
      );
    } else {
      // Otherwise we don't know `cls', but do know the property
      // name. We'll store this mutation separately and union it in to
      // any lookup with the same name.
      ITRACE(4, "unknown class. merging all props with name {}\n", sname);

      // Assume that it could possibly affect any private property
      // with the same name.
      privateProps.mergeInPrivateStatic(
        AnalysisIndexAdaptor { *this },
        sname,
        unctx(val),
        ignoreConst,
        mustBeReadOnly
      );
    }

    // To be conservative, say we might throw and be conservative
    // about conversions.
    return PropMergeResult{
      loosen_likeness(val),
      TriBool::Maybe
    };
  };

  // If we don't find a property, then the mutation will definitely
  // fail.
  auto const notFound = [&] {
    return PropMergeResult{
      TBottom,
      TriBool::Yes
    };
  };

  // check if we can determine the class.
  if (!is_specialized_cls(cls)) return unknownCls();

  if (ctx.cls && !ctx.cls->cinfo) return unknownCls();
  auto const ctxCls = ctx.cls ? ctx.cls->cinfo : nullptr;

  auto const initMightRaise =
    class_init_might_raise(*m_data, dcls_of(cls));

  // Perform the actual merge for a given property, returning the
  // effects of that merge.
  auto const merge = [&] (const php::Prop& prop, const ClassInfo2& cinfo) {
    // First calculate the effects of the type-constraint.
    auto effects = prop_tc_effects(
      AnalysisIndexAdaptor { *this },
      *cinfo.cls,
      prop,
      val,
      checkUB
    );
    // No point in merging if the type-constraint will always fail.
    if (effects.throws == TriBool::Yes) {
      ITRACE(
        6, "tc would throw on {}::${} with {}, skipping\n",
        cinfo.cls->name, prop.name, show(val)
      );
      return effects;
    } else if (effects.throws == TriBool::No) {
      if (initMightRaise) effects.throws = TriBool::Maybe;
    }

    assertx(!effects.adjusted.subtypeOf(BBottom));

    ITRACE(
      6, "merging {} into {}::${}\n",
      show(effects), cinfo.cls->name, prop.name
    );

    switch (prop.attrs & (AttrPublic|AttrProtected|AttrPrivate)) {
      case AttrPublic:
      case AttrProtected:
        // If the property is internal, accessing it may throw
        // TODO(T131951529): we can do better by checking modules here
        if ((prop.attrs & AttrInternal) && effects.throws == TriBool::No) {
          ITRACE(6, "{}::${} is internal, "
                 "being pessimistic with regards to throwing\n",
                 cinfo.cls->name, prop.name);
          return PropMergeResult{
            effects.adjusted,
            TriBool::Maybe
          };
        }
        return effects;
      case AttrPrivate: {
        assertx(ctxCls == &cinfo);
        privateProps.mergeInPrivateStaticPreAdjusted(
          prop.name,
          unctx(effects.adjusted)
        );
        return effects;
      }
    }
    always_assert(false);
  };

  Optional<R> result;
  auto const addResult = [&] (R r) {
    if (!result.has_value()) {
      result.emplace(std::move(r));
    } else {
      *result |= r;
      ITRACE(4, "  -> {}\n", show(*result));
    }
  };
  Optional<R> iresult;

  auto const full = visit_prop_decls(
    dcls_of(cls),
    sname,
    [&] (const ClassInfo2& cinfo) {
      if (sname) {
        for (auto const& prop : cinfo.cls->properties) {
          if (prop.name != sname) continue;
          // We found a property with the right name, but its
          // inaccessible from this context (or not even static). This
          // mutation will fail, so we don't need to modify the type.
          if (!(prop.attrs & AttrStatic) ||
              !static_is_accessible(ctxCls, &cinfo, prop)) {
            ITRACE(
              6, "{}::${} found but inaccessible\n",
              cinfo.cls->name, sname
            );
            addResult(notFound());
            return;
          }
          // Mutations to AttrConst properties will fail as well, unless
          // it we want to override that behavior.
          if (!ignoreConst && (prop.attrs & AttrIsConst)) {
            ITRACE(
              6, "{}:${} found but const\n",
              cinfo.cls->name, sname
            );
            addResult(notFound());
            return;
          }
          if (mustBeReadOnly && !(prop.attrs & AttrIsReadonly)) {
            ITRACE(
              6, "{}:${} found but is mutable and must be readonly\n",
              cinfo.cls->name, sname
            );
            addResult(notFound());
            return;
          }

          auto r = merge(prop, cinfo);
          ITRACE(6, "including {}:${} {}\n", cinfo.cls->name, prop.name, show(r));
          addResult(std::move(r));
          return;
        }

        addResult(notFound());
        return;
      }

      addResult(notFound());
      for (auto const& prop : cinfo.cls->properties) {
        if (!(prop.attrs & AttrStatic) ||
            !static_is_accessible(ctxCls, &cinfo, prop)) {
          ITRACE(
            6, "skipping inaccessible {}::${}\n",
            cinfo.cls->name, prop.name
          );
          continue;
        }

        auto r = merge(prop, cinfo);
        ITRACE(6, "including {}:${} {}\n", cinfo.cls->name, prop.name, show(r));
        addResult(std::move(r));
      }
    },
    [&] {
      if (!result.has_value()) {
        if (!iresult.has_value()) {
          iresult.emplace(notFound());
        } else {
          *iresult &= notFound();
        }
      } else if (!iresult.has_value()) {
        iresult = std::move(result);
      } else {
        *iresult &= *result;
      }
      result.reset();
    },
    [&] { addResult(notFound()); }
  );

  if (!full) return unknownCls();

  if (iresult.has_value()) {
    assertx(!result.has_value());
    ITRACE(4, "union -> {}\n", show(*iresult));
    return *iresult;
  }
  if (result.has_value()) {
    ITRACE(4, "union -> {}\n", show(*result));
    return *result;
  }

  ITRACE(6, "nothing found\n");
  return notFound();
}

void AnalysisIndex::refine_constants(const FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  auto const& func = *fa.ctx.func;
  if (func.cls) return;

  auto const name = Constant::nameFromFuncName(func.name);
  if (!name) return;

  auto const cnsPtr = folly::get_ptr(m_data->constants, name);
  always_assert_flog(
    cnsPtr,
    "Attempting to refine constant {} "
    "which we don't have meta-data for",
    name
  );
  auto const cns = cnsPtr->first;
  auto const val = tv(fa.inferredReturn);
  if (!val) {
    always_assert_flog(
      type(cns->val) == KindOfUninit,
      "Constant value invariant violated in {}.\n"
      "    Value went from {} to {}",
      name,
      show(from_cell(cns->val)),
      show(fa.inferredReturn)
    );
    return;
  }

  if (type(cns->val) != KindOfUninit) {
    always_assert_flog(
      equal(from_cell(cns->val), fa.inferredReturn),
      "Constant value invariant violated in {}.\n"
      "    Value went from {} to {}",
      name,
      show(from_cell(cns->val)),
      show(fa.inferredReturn)
    );
  } else {
    always_assert_flog(
      !m_data->frozen,
      "Attempting to refine constant {} to {} when index is frozen",
      name,
      show(fa.inferredReturn)
    );
    cns->val = *val;
    m_data->deps->update(*cns);
  }
}

void AnalysisIndex::refine_class_constants(const FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  auto const resolved = fa.resolvedInitializers.left();
  if (!resolved || resolved->empty()) return;

  assertx(fa.ctx.func->cls);
  auto& constants = fa.ctx.func->cls->constants;

  for (auto const& c : *resolved) {
    assertx(c.first < constants.size());
    auto& cns = constants[c.first];
    assertx(cns.kind == ConstModifierFlags::Kind::Value);
    always_assert(cns.val.has_value());
    always_assert(type(*cns.val) == KindOfUninit);

    auto cinfo = fa.ctx.func->cls->cinfo;
    if (auto const val = tv(c.second.type)) {
      assertx(type(*val) != KindOfUninit);
      always_assert_flog(
        !m_data->frozen,
        "Attempting to refine class constant {}::{} to {} "
        "when index is frozen",
        fa.ctx.func->cls->name,
        cns.name,
        show(c.second.type)
      );
      cns.val = *val;
      if (cinfo) cinfo->inferred.clsConstantInfo.erase(cns.name);
      m_data->deps->update(
        cns,
        ConstIndex { fa.ctx.func->cls->name, c.first }
      );
    } else if (cinfo) {
      auto old = folly::get_default(
        cinfo->inferred.clsConstantInfo,
        cns.name,
        ClsConstInfo{ TInitCell, 0 }
      );
      old.type = unserialize_type(std::move(old.type));

      if (c.second.type.strictlyMoreRefined(old.type)) {
        always_assert(c.second.refinements > old.refinements);
        always_assert_flog(
          !m_data->frozen,
          "Attempting to refine class constant {}::{} to {} "
          "when index is frozen",
          cinfo->name,
          cns.name,
          show(c.second.type)
        );
        cinfo->inferred.clsConstantInfo.insert_or_assign(
          cns.name,
          ClsConstInfo {
            serialize_type(c.second.type),
            c.second.refinements
          }
        );
        m_data->deps->update(cns, ConstIndex { cinfo->name, c.first });
      } else {
        always_assert_flog(
          c.second.type.moreRefined(old.type),
          "Class constant type invariant violated for {}::{}\n"
          "    {} is not at least as refined as {}\n",
          fa.ctx.func->cls->name,
          cns.name,
          show(c.second.type),
          show(old.type)
        );
      }
    }
  }
}

void AnalysisIndex::refine_return_info(const FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  auto const& func = *fa.ctx.func;
  auto& finfo = func_info(*m_data, func);

  auto changes = AnalysisDeps::Type::None;

  auto const oldReturnTy = unserialize_type(finfo.inferred.returnTy);
  if (fa.inferredReturn.strictlyMoreRefined(oldReturnTy)) {
    if (finfo.inferred.returnRefinements < options.returnTypeRefineLimit) {
      finfo.inferred.returnTy = serialize_type(fa.inferredReturn);
      finfo.inferred.returnRefinements += fa.localReturnRefinements + 1;
      if (finfo.inferred.returnRefinements > options.returnTypeRefineLimit) {
        FTRACE(
          1, "maxed out return type refinements at {} {}\n",
          func.unit, func_fullname(func)
        );
      }
      changes |= AnalysisDeps::Type::RetType;
      if (is_scalar(finfo.inferred.returnTy)) {
        changes |= AnalysisDeps::Type::ScalarRetType;
      }
    } else {
      FTRACE(
        1, "maxed out return type refinements at {} {}\n",
        func.unit, func_fullname(func)
      );
    }
  } else {
    always_assert_flog(
      fa.inferredReturn.moreRefined(oldReturnTy),
      "Index return type invariant violated in {} {}.\n"
      "   {} is not at least as refined as {}\n",
      func.unit, func_fullname(func),
      show(fa.inferredReturn),
      show(oldReturnTy)
    );
  }

  always_assert_flog(
    !finfo.inferred.effectFree || fa.effectFree,
    "Index effect-free invariant violated in {} {}.\n"
    "    Went from true to false\n",
    func.unit,
    func_fullname(func)
  );

  if (finfo.inferred.effectFree != fa.effectFree) {
    finfo.inferred.effectFree = fa.effectFree;
    changes |= AnalysisDeps::Type::RetType;
  }

  auto const unusedParams = ~fa.usedParams;
  if (finfo.inferred.unusedParams != unusedParams) {
    always_assert_flog(
      (finfo.inferred.unusedParams | unusedParams) == unusedParams,
      "Index unused params decreased in {} {}.\n",
      func.unit, func_fullname(func)
    );
    finfo.inferred.unusedParams = unusedParams;
    changes |= AnalysisDeps::Type::UnusedParams;
  }

  if (finfo.inferred.retParam == NoLocalId && fa.retParam != NoLocalId) {
    // This is just a heuristic; it doesn't mean that the value passed
    // in was returned, but that the value of the parameter at the
    // point of the RetC was returned. We use it to make (heuristic)
    // decisions about whether to do inline interps, so we only allow
    // it to change once. (otherwise later passes might not do the
    // inline interp, and get worse results, which breaks
    // monotonicity).
    finfo.inferred.retParam = fa.retParam;
    changes |= AnalysisDeps::Type::RetParam;
  }

  always_assert_flog(
    !m_data->frozen || changes == AnalysisDeps::Type::None,
    "Attempting to refine return info for {} {} ({}) "
    "when index is frozen",
    func.unit,
    func_fullname(func),
    show(changes)
  );

  if (changes & AnalysisDeps::Type::RetType) {
    if (auto const name = Constant::nameFromFuncName(func.name)) {
      auto const cns = folly::get_ptr(m_data->constants, name);
      always_assert_flog(
        cns,
        "Attempting to update constant {} type, but constant is not present!",
        name
      );
      m_data->deps->update(*cns->first);
    }
  }

  if (changes != AnalysisDeps::Type::None &&
      fa.reanalyzeOnUpdate) {
    ITRACE(2, "Updated return info for {} in a way that requires re-analysis\n",
           func_fullname(*fa.ctx.func));
    m_data->worklist.schedule(fc_from_context(fa.ctx, *m_data));
  }

  m_data->deps->update(func, changes);
}

void AnalysisIndex::refine_closure_use_vars(const FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  for (auto const& [cls, vars] : fa.closureUseTypes) {
    assertx(is_closure(*cls));

    if constexpr (debug) {
      for (size_t i = 0; i < vars.size(); ++i) {
        always_assert_flog(
          equal(vars[i], unctx(vars[i])),
          "Closure {} cannot have a used var with a context dependent type",
          cls->name
        );
      }
    }

    if (!cls->cinfo) continue;

    auto& current = cls->cinfo->inferred.useVars;
    current.reserve(vars.size());

    auto changed = false;
    for (size_t i = 0, size = vars.size(); i < size; ++i) {
      auto const old = unserialize_type(current.get_default(i, TCell));
      if (vars[i].strictSubtypeOf(old)) {
        current.ensure(i, TCell) = serialize_type(vars[i]);
        changed = true;
      } else {
        always_assert_flog(
          vars[i].moreRefined(old),
          "Index closure use-var invariant violated for {}.\n"
          "   {} is not at least as refined as {}\n",
          cls->name,
          show(vars[i]),
          show(old)
        );
      }
    }

    always_assert_flog(
      !m_data->frozen || !changed,
      "Attempting to refine closure use-var info for {} "
      "when index is frozen",
      cls->name
    );

    if (changed) {
      m_data->deps->update(*cls, AnalysisDeps::Type::UseVars);
    }
  }
}

void AnalysisIndex::refine_private_propstate(const php::Class& cls,
                                             const PropState& from,
                                             PropState& to) const {
  assertx(!from.empty());
  auto const DEBUG_ONLY wasEmpty = to.empty();

  for (auto const& prop : cls.properties) {
    auto const newElem = folly::get_ptr(from, prop.name);
    if (!newElem) continue;

    if (!to.contains(prop.name)) {
      assertx(wasEmpty);
      to.emplace(
        prop.name,
        PropStateElem{
          TCell,
          newElem->tc,
          newElem->attrs,
          true
        }
      );
    }

    assertx(to.contains(prop.name));
    auto& oldElem = to[prop.name];
    assertx(oldElem.tc == newElem->tc);

    auto const oldT = unserialize_type(oldElem.ty);
    auto newT = unserialize_type(newElem->ty);

    auto changed = false;

    if (newT.strictlyMoreRefined(oldT)) {
      oldElem.ty = serialize_type(std::move(newT));
      changed = true;
    } else {
      always_assert_flog(
        newT.moreRefined(oldT),
        "Property refinement failed on {}::${} -- {} was not a subtype of {}\n",
        cls.name,
        prop.name,
        show(newT),
        show(oldT)
      );
    }

    if (newElem->everModified) {
      always_assert_flog(
        oldElem.everModified,
        "Property refinement failed on {}::${} -- "
        "ever-modified flag went from false to true\n",
        cls.name, prop.name
      );
    } else if (oldElem.everModified) {
      oldElem.everModified = false;
      changed = true;
    }

    if (changed) {
      always_assert_flog(
        !m_data->frozen,
        "Attempting to update property {}::${} when index is frozen",
        cls.name, prop.name
      );
      m_data->deps->update(cls, prop);
    }
  }
}

void AnalysisIndex::refine_private_props(const ClassAnalysis& ca) {
  assertx(m_data->mode != Mode::Final);
  assertx(ca.ctx.cls);

  if (!ca.ctx.cls->cinfo) return;
  if (ca.privateProperties.empty()) return;
  assertx(!is_used_trait(*ca.ctx.cls));

  refine_private_propstate(
    *ca.ctx.cls,
    ca.privateProperties,
    ca.ctx.cls->cinfo->inferred.privateProps
  );
}

void AnalysisIndex::refine_private_statics(const ClassAnalysis& ca) {
  assertx(m_data->mode != Mode::Final);
  assertx(ca.ctx.cls);

  if (!ca.ctx.cls->cinfo) return;
  if (ca.privateStatics.empty()) return;
  assertx(!is_used_trait(*ca.ctx.cls));

  // We can't store context dependent types in private statics since
  // they could be accessed using different contexts.
  //
  // I don't think this is needed anymore.
  auto cleaned = ca.privateStatics;
  for (auto& [name, elem] : cleaned) {
    elem.ty = unctx(unserialize_type(std::move(elem.ty)));
  }

  refine_private_propstate(
    *ca.ctx.cls,
    cleaned,
    ca.ctx.cls->cinfo->inferred.privateStatics
  );
}

void AnalysisIndex::update_prop_initial_values(const FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  auto const resolved = fa.resolvedInitializers.right();
  if (!resolved || resolved->empty()) return;

  assertx(fa.ctx.func->cls);
  auto& props = const_cast<php::Class*>(fa.ctx.func->cls)->properties;

  auto changed = false;
  auto changedInitialVal = false;
  for (auto const& [idx, info] : *resolved) {
    assertx(idx < props.size());
    auto& prop = props[idx];

    if (info.satisfies) {
      if (!(prop.attrs & AttrInitialSatisfiesTC)) {
        always_assert_flog(
          !m_data->frozen,
          "Attempting to update AttrInitialSatisfiesTC for {}::{} "
          "when index is frozen",
          fa.ctx.func->cls->name,
          prop.name
        );
        attribute_setter(prop.attrs, true, AttrInitialSatisfiesTC);
        changed = true;
      }
    } else {
      always_assert_flog(
        !(prop.attrs & AttrInitialSatisfiesTC),
        "AttrInitialSatisfiesTC invariant violated for {}::{}\n"
        "  Went from true to false",
        fa.ctx.func->cls->name, prop.name
      );
    }

    always_assert_flog(
      IMPLIES(!(prop.attrs & AttrDeepInit), !info.deepInit),
      "AttrDeepInit invariant violated for {}::{}\n"
      "  Went from false to true",
      fa.ctx.func->cls->name, prop.name
    );
    if (bool(prop.attrs & AttrDeepInit) != info.deepInit) {
      always_assert_flog(
        !m_data->frozen,
        "Attempting to update AttrDeepInit for {}::{} "
        "when index is frozen",
        fa.ctx.func->cls->name,
        prop.name
      );
      attribute_setter(prop.attrs, info.deepInit, AttrDeepInit);
    }

    if (type(info.val) != KindOfUninit) {
      always_assert_flog(
        !m_data->frozen,
        "Attempting to update property initial value for {}::{} "
        "to {} when index is frozen",
        fa.ctx.func->cls->name,
        prop.name,
        show(from_cell(info.val))
      );
      if (type(prop.val) == KindOfUninit) {
        prop.val = info.val;
        changedInitialVal = true;
      } else {
        always_assert_flog(
          equal(from_cell(prop.val), from_cell(info.val)),
          "Property initial value invariant violated for {}::{}\n"
          "  Value went from {} to {}",
          fa.ctx.func->cls->name, prop.name,
          show(from_cell(prop.val)), show(from_cell(info.val))
        );
      }
    } else {
      always_assert_flog(
        type(prop.val) == KindOfUninit,
        "Property initial value invariant violated for {}::{}\n"
        " Value went from {} to not set",
        fa.ctx.func->cls->name, prop.name,
        show(from_cell(prop.val))
      );
    }
  }

  if (changedInitialVal) {
    ITRACE(
      2, "Updated property init info for {} in a way that requires re-analysis\n",
      fa.ctx.func->cls->name
    );
    m_data->worklist.schedule(fc_from_context(fa.ctx, *m_data));
    m_data->deps->update(
      *fa.ctx.func->cls,
      AnalysisDeps::Type::PropInitVals
    );
  }

  if (!changed) return;

  auto const cinfo = fa.ctx.func->cls->cinfo;
  if (!cinfo) return;

  assertx(cinfo->hasBadInitialPropValues);
  auto const noBad = std::all_of(
    begin(props), end(props),
    [] (const php::Prop& prop) {
      return bool(prop.attrs & AttrInitialSatisfiesTC);
    }
  );

  if (noBad) {
    cinfo->hasBadInitialPropValues = false;
    m_data->deps->update(
      *cinfo->cls,
      AnalysisDeps::Type::ClassInitMightRaise
    );
  }
}

void AnalysisIndex::update_type_consts(const ClassAnalysis& analysis) {
  assertx(m_data->mode != Mode::Final);

  if (analysis.resolvedTypeConsts.empty()) return;

  always_assert_flog(
    !m_data->frozen,
    "Attempting to update type constants for {} when index is frozen",
    analysis.ctx.cls->name
  );

  auto const cls = const_cast<php::Class*>(analysis.ctx.cls);
  auto const cinfo = cls->cinfo;
  if (!cinfo) return;

  for (auto const& update : analysis.resolvedTypeConsts) {
    auto const srcCls = folly::get_default(m_data->classes, update.from.cls);
    assertx(srcCls);
    assertx(update.from.idx < srcCls->constants.size());

    auto& newCns = [&] () -> php::Const& {
      auto& srcCns = srcCls->constants[update.from.idx];
      if (srcCls == cls) {
        assertx(!srcCns.resolvedTypeStructure ||
                srcCns.resolvedTypeStructure == update.resolved);
        return srcCns;
      }
      cinfo->clsConstants[srcCns.name] =
        ClassInfo2::ConstIndexAndKind {
         ConstIndex { cinfo->name, (ConstIndex::Idx)cls->constants.size() },
         srcCns.kind
      };
      cls->constants.emplace_back(srcCns);
      return cls->constants.back();
    }();

    newCns.resolvedTypeStructure = update.resolved;
    newCns.contextInsensitive = update.contextInsensitive;
    newCns.invariance = update.invariance;
    newCns.resolvedLocally = true;
  }
}

void AnalysisIndex::update_bytecode(FuncAnalysisResult& fa) {
  assertx(m_data->mode != Mode::Final);

  auto func = php::WideFunc::mut(const_cast<php::Func*>(fa.ctx.func));
  auto const update = HHBBC::update_bytecode(func, std::move(fa.blockUpdates));
  if (update == UpdateBCResult::None) return;

  always_assert_flog(
    !m_data->frozen,
    "Attempting to update bytecode for {} when index is frozen",
    func_fullname(*fa.ctx.func)
  );

  if (fa.reanalyzeOnUpdate ||
      update == UpdateBCResult::ChangedAnalyze ||
      fa.ctx.func->name == s_86cinit.get()) {
    ITRACE(2, "Updated bytecode for {} in a way that requires re-analysis\n",
           func_fullname(*fa.ctx.func));
    m_data->worklist.schedule(fc_from_context(fa.ctx, *m_data));
  }

  m_data->deps->update(*fa.ctx.func, AnalysisDeps::Type::Bytecode);
}

void AnalysisIndex::update_type_aliases(const UnitAnalysis& ua) {
  assertx(m_data->mode != Mode::Final);
  assertx(ua.ctx.unit);
  if (ua.resolvedTypeAliases.empty()) return;

  always_assert_flog(
    !m_data->frozen,
    "Attempting to update type-aliases for unit {} when index is frozen\n",
    ua.ctx.unit
  );

  auto const& unit = lookup_unit(ua.ctx.unit);
  for (auto const& update : ua.resolvedTypeAliases) {
    assertx(update.idx < unit.typeAliases.size());
    auto& ta = *unit.typeAliases[update.idx];
    ta.resolvedTypeStructure = update.resolved;
    ta.resolvedLocally = true;
  }
}

// Finish using the AnalysisIndex and calculate the output to be
// returned back from the job.
AnalysisIndex::Output AnalysisIndex::finish() {
  assertx(m_data->mode != Mode::Final);
  assertx(m_data->frozen);

  Variadic<AnalysisIndexBundle> bundles;
  AnalysisOutput::Meta meta;

  // Remove any 86cinits that are now unneeded.
  meta.removedFuncs = strip_unneeded_constant_inits(*m_data);

  auto const moveNewAuxs = [&] (AuxClassGraphs& auxs) {
    if (m_data->mode != Mode::Constants) {
      auxs.noChildren = std::move(auxs.newNoChildren);
      auxs.withChildren = std::move(auxs.newWithChildren);
    } else {
      // When analyzing constants, we may not be processing all of the
      // class' methods, so it's not safe to drop the previous
      // noChildren and withChildren sets.
      auxs.noChildren.insert(
        begin(auxs.newNoChildren),
        end(auxs.newNoChildren)
      );
      auxs.withChildren.insert(
        begin(auxs.newWithChildren),
        end(auxs.newWithChildren)
      );
    }
  };

  TSStringSet keepClasses;
  for (auto const [n, c] : m_data->classes) {
    if (!m_data->toReport.contains(c)) continue;

    keepClasses.emplace(n);
    for (auto const& clo : c->closures) {
      keepClasses.emplace(clo->name);
    }

    mark_fixed_class_constants(*c, *m_data);
    always_assert(
      meta.cnsBases.emplace(n, record_cns_bases(*c, *m_data)).second
    );

    if (debug && is_closure(*c)) {
      auto const d = m_data->deps->get(c);
      always_assert(!d || d->empty());
    }
    meta.classDeps.emplace(n, m_data->deps->take(c));
    for (auto const& clo : c->closures) {
      assertx(m_data->deps->take(clo.get()).empty());
      mark_fixed_class_constants(*clo, *m_data);
    }
    if (auto cinfo = folly::get_default(m_data->cinfos, n)) {
      moveNewAuxs(cinfo->auxClassGraphs);
      if (debug) {
        always_assert(IMPLIES(is_closure(*c), cinfo->retained.empty()));
        for (auto const& clo : cinfo->closures) {
          always_assert(clo->retained.empty());
        }
      }
      cinfo->retained.flip();
    }
  }

  FSStringSet keepFuncs;
  for (auto const [n, f] : m_data->funcs) {
    if (!m_data->toReport.contains(f)) continue;
    keepFuncs.emplace(n);
    meta.funcDeps.emplace(n, m_data->deps->take(f));

    auto const closures = folly::get_ptr(m_data->funcToClosures, f);
    if (closures) {
      for (auto const& clo : *closures) keepClasses.emplace(clo->name);
    }

    auto const finfo = folly::get_default(m_data->finfos, n);
    always_assert(finfo);
    if (finfo->auxClassGraphs) moveNewAuxs(*finfo->auxClassGraphs);
    finfo->retained.flip();
  }

  SStringSet keepUnits;
  for (auto const [n, u] : m_data->units) {
    if (!m_data->toReport.contains(u)) continue;
    keepUnits.emplace(n);
    meta.unitDeps.emplace(n, m_data->deps->take(u));
    mark_fixed_unit(*u, m_data->deps->getChanges());
  }

  SStringSet keepConstants;
  for (auto const& [_, p] : m_data->constants) {
    if (!m_data->toReport.contains(p.second) &&
        !meta.removedFuncs.contains(Constant::funcNameFromName(p.first->name))) {
      continue;
    }
    keepConstants.emplace(p.first->name);
  }

  for (auto& b : m_data->reportBundles) {
    for (auto& bc : b->classBytecode) {
      auto cls = folly::get_default(m_data->classes, bc->cls);
      always_assert(cls);

      size_t idx = 0;
      for (auto& meth : cls->methods) {
        assertx(idx < bc->methodBCs.size());
        auto& methBC = bc->methodBCs[idx++];
        always_assert(methBC.name == meth->name);
        methBC.bc = std::move(meth->rawBlocks);
      }
      for (auto& clo : cls->closures) {
        assertx(idx < bc->methodBCs.size());
        auto& methBC = bc->methodBCs[idx++];
        assertx(clo->methods.size() == 1);
        always_assert(methBC.name == clo->methods[0]->name);
        methBC.bc = std::move(clo->methods[0]->rawBlocks);
      }
      assertx(idx == bc->methodBCs.size());
    }

    for (auto& bc : b->funcBytecode) {
      auto func = folly::get_default(m_data->funcs, bc->name);
      always_assert(func);
      bc->bc = std::move(func->rawBlocks);
    }
  }

  bundles.vals.reserve(m_data->reportBundles.size());
  for (auto& b : m_data->reportBundles) {
    AnalysisIndexBundle aib;
    aib.ptr = decltype(aib.ptr){b.release()};
    bundles.vals.emplace_back(std::move(aib));
  }

  meta.changed = std::move(m_data->deps->getChanges());
  meta.changed.filter(keepClasses, keepFuncs, keepUnits, keepConstants);

  return std::make_tuple(std::move(bundles), std::move(meta));
}

//////////////////////////////////////////////////////////////////////

PublicSPropMutations::PublicSPropMutations(bool enabled) : m_enabled{enabled} {}

PublicSPropMutations::Data& PublicSPropMutations::get() {
  if (!m_data) m_data = std::make_unique<Data>();
  return *m_data;
}

void PublicSPropMutations::mergeKnown(const ClassInfo* ci,
                                      const php::Prop& prop,
                                      const Type& val) {
  if (!m_enabled) return;
  ITRACE(4, "PublicSPropMutations::mergeKnown: {} {} {}\n",
         ci->cls->name->data(), prop.name, show(val));

  auto const res = get().m_known.emplace(
    KnownKey { const_cast<ClassInfo*>(ci), prop.name }, val
  );
  if (!res.second) res.first->second |= val;
}

void PublicSPropMutations::mergeUnknownClass(SString prop, const Type& val) {
  if (!m_enabled) return;
  ITRACE(4, "PublicSPropMutations::mergeUnknownClass: {} {}\n",
         prop, show(val));

  auto const res = get().m_unknown.emplace(prop, val);
  if (!res.second) res.first->second |= val;
}

void PublicSPropMutations::mergeUnknown(Context ctx) {
  if (!m_enabled) return;
  ITRACE(4, "PublicSPropMutations::mergeUnknown\n");

  /*
   * We have a case here where we know neither the class nor the static
   * property name.  This means we have to pessimize public static property
   * types for the entire program.
   *
   * We could limit it to pessimizing them by merging the `val' type, but
   * instead we just throw everything away---this optimization is not
   * expected to be particularly useful on programs that contain any
   * instances of this situation.
   */
  std::fprintf(
    stderr,
    "NOTE: had to mark everything unknown for public static "
    "property types due to dynamic code.  -fanalyze-public-statics "
    "will not help for this program.\n"
    "NOTE: The offending code occured in this context: %s\n",
    show(ctx).c_str()
  );
  get().m_nothing_known = true;
}

//////////////////////////////////////////////////////////////////////

bool AnalysisIndexAdaptor::frozen() const {
  return index.frozen();
}

void AnalysisIndexAdaptor::push_context(const Context& ctx) const {
  index.push_context(ctx);
}

void AnalysisIndexAdaptor::pop_context() const {
  index.pop_context();
}

bool AnalysisIndexAdaptor::set_in_type_cns(bool b) const {
  return index.set_in_type_cns(b);
}

const php::Unit* AnalysisIndexAdaptor::lookup_func_unit(const php::Func& func) const {
  return &index.lookup_func_unit(func);
}
const php::Unit*
AnalysisIndexAdaptor::lookup_func_original_unit(const php::Func& func) const {
  return &index.lookup_func_original_unit(func);
}
const php::Unit* AnalysisIndexAdaptor::lookup_class_unit(const php::Class& cls) const {
  return &index.lookup_class_unit(cls);
}
const php::Class* AnalysisIndexAdaptor::lookup_const_class(const php::Const& cns) const {
  return index.lookup_const_class(cns);
}
const php::Class* AnalysisIndexAdaptor::lookup_closure_context(const php::Class& cls) const {
  return &index.lookup_closure_context(cls);
}
const php::Class* AnalysisIndexAdaptor::lookup_class(SString c) const {
  return index.lookup_class(c);
}

CompactVector<const php::Func*>
AnalysisIndexAdaptor::lookup_extra_methods(const php::Class& c) const {
  return index.lookup_extra_methods(c);
}

void AnalysisIndexAdaptor::for_each_unit_func(
  const php::Unit& u,
  std::function<void(const php::Func&)> f
) const {
  index.for_each_unit_func(u, std::move(f));
}
void AnalysisIndexAdaptor::for_each_unit_func_mutable(
  php::Unit& u,
  std::function<void(php::Func&)> f
) {
  index.for_each_unit_func_mutable(u, std::move(f));
}
void AnalysisIndexAdaptor::for_each_unit_class(
  const php::Unit& u,
  std::function<void(const php::Class&)> f
) const {
  index.for_each_unit_class(u, std::move(f));
}
void AnalysisIndexAdaptor::for_each_unit_class_mutable(
  php::Unit& u,
  std::function<void(php::Class&)> f
) {
  index.for_each_unit_class_mutable(u, std::move(f));
}

Optional<res::Class> AnalysisIndexAdaptor::resolve_class(SString n) const {
  return index.resolve_class(n);
}
Optional<res::Class>
AnalysisIndexAdaptor::resolve_class(const php::Class& c) const {
  return index.resolve_class(c);
}
std::pair<const php::TypeAlias*, bool>
AnalysisIndexAdaptor::lookup_type_alias(SString n) const {
  return index.lookup_type_alias(n);
}
Index::ClassOrTypeAlias
AnalysisIndexAdaptor::lookup_class_or_type_alias(SString n) const {
  return index.lookup_class_or_type_alias(n);
}

res::Func AnalysisIndexAdaptor::resolve_func_or_method(const php::Func& f) const {
  return index.resolve_func_or_method(f);
}
res::Func AnalysisIndexAdaptor::resolve_func(SString f) const {
  return index.resolve_func(f);
}
res::Func AnalysisIndexAdaptor::resolve_method(Context,
                                               const Type& t,
                                               SString n) const {
  return index.resolve_method(t, n);
}
res::Func AnalysisIndexAdaptor::resolve_ctor(const Type& obj) const {
  return index.resolve_ctor(obj);
}

std::vector<std::pair<SString, ClsConstInfo>>
AnalysisIndexAdaptor::lookup_class_constants(const php::Class& cls) const {
  return index.lookup_class_constants(cls);
}

ClsConstLookupResult
AnalysisIndexAdaptor::lookup_class_constant(Context,
                                            const Type& cls,
                                            const Type& name) const {
  return index.lookup_class_constant(cls, name);
}

ClsTypeConstLookupResult
AnalysisIndexAdaptor::lookup_class_type_constant(
  const Type& cls,
  const Type& name,
  const Index::ClsTypeConstLookupResolver& resolver
) const {
  return index.lookup_class_type_constant(cls, name, resolver);
}

ClsTypeConstLookupResult
AnalysisIndexAdaptor::lookup_class_type_constant(const php::Class& ctx,
                                                 SString n,
                                                 ConstIndex idx) const {
  return index.lookup_class_type_constant(ctx, n, idx);
}

std::vector<std::pair<SString, ConstIndex>>
AnalysisIndexAdaptor::lookup_flattened_class_type_constants(
  const php::Class& cls
) const {
  return index.lookup_flattened_class_type_constants(cls);
}

Type AnalysisIndexAdaptor::lookup_constant(Context, SString n) const {
  return index.lookup_constant(n);
}
bool AnalysisIndexAdaptor::func_depends_on_arg(const php::Func* f,
                                               size_t arg) const {
  return index.func_depends_on_arg(*f, arg);
}
Index::ReturnType
AnalysisIndexAdaptor::lookup_foldable_return_type(Context,
                                                  const CallContext& callee) const {
  return index.lookup_foldable_return_type(callee);
}
Index::ReturnType AnalysisIndexAdaptor::lookup_return_type(Context,
                                                           MethodsInfo* methods,
                                                           res::Func func,
                                                           Dep) const {
  return index.lookup_return_type(methods, func);
}
Index::ReturnType AnalysisIndexAdaptor::lookup_return_type(Context,
                                                           MethodsInfo* methods,
                                                           const CompactVector<Type>& args,
                                                           const Type& context,
                                                           res::Func func,
                                                           Dep) const {
  return index.lookup_return_type(methods, args, context, func);
}

std::pair<Index::ReturnType, size_t>
AnalysisIndexAdaptor::lookup_return_type_raw(const php::Func* f) const {
  return index.lookup_return_type_raw(*f);
}
CompactVector<Type>
AnalysisIndexAdaptor::lookup_closure_use_vars(const php::Func& f) const {
  return index.lookup_closure_use_vars(f);
}
CompactVector<Type>
AnalysisIndexAdaptor::lookup_closure_use_vars_raw(const php::Func& f) const {
  return index.lookup_closure_use_vars_raw(f);
}

PropState AnalysisIndexAdaptor::lookup_private_props(const php::Class* cls,
                                                     bool) const {
  return index.lookup_private_props(*cls);
}
PropState AnalysisIndexAdaptor::lookup_private_statics(const php::Class* cls,
                                                       bool) const {
  return index.lookup_private_statics(*cls);
}
PropState AnalysisIndexAdaptor::lookup_public_statics(const php::Class* cls) const {
  return index.lookup_public_statics(*cls);
}

PropLookupResult AnalysisIndexAdaptor::lookup_static(Context ctx,
                                                     const PropertiesInfo& p,
                                                     const Type& c,
                                                     const Type& n) const {
  return index.lookup_static(ctx, p, c, n);
}

Type AnalysisIndexAdaptor::lookup_public_prop(const Type& obj,
                                              const Type& name) const {
  return index.lookup_public_prop(obj, name);
}

PropMergeResult
AnalysisIndexAdaptor::merge_static_type(Context ctx,
                                        PublicSPropMutations& publicMutations,
                                        PropertiesInfo& privateProps,
                                        const Type& cls,
                                        const Type& name,
                                        const Type& val,
                                        bool checkUB,
                                        bool ignoreConst,
                                        bool mustBeReadOnly) const {
  return index.merge_static_type(
    ctx,
    publicMutations,
    privateProps,
    cls,
    name,
    val,
    checkUB,
    ignoreConst,
    mustBeReadOnly
  );
}

Slot AnalysisIndexAdaptor::lookup_iface_vtable_slot(const php::Class* c) const {
  return index.lookup_iface_vtable_slot(*c);
}

bool AnalysisIndexAdaptor::tracking_public_sprops() const {
  return index.tracking_public_sprops();
}

//////////////////////////////////////////////////////////////////////

template <typename T>
void AnalysisIndexParam<T>::Deleter::operator()(T* t) const {
  delete t;
}

template <typename T>
void AnalysisIndexParam<T>::serde(BlobEncoder& sd) const {
  sd(ptr);
}

template <typename T>
void AnalysisIndexParam<T>::serde(BlobDecoder& sd) {
  sd(ptr);
}

template struct AnalysisIndexParam<ClassBundle>;

//////////////////////////////////////////////////////////////////////

}

//////////////////////////////////////////////////////////////////////

}
